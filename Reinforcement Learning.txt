  Reinforcement Learning is learning to act through trial and error with no models, no labels, no demonstrations, or any human-provided supervision signal other than scalar reward.
  Reinforcement Learning problem is as hard as any problem of computer science, since any task with a computable description can be formulated in reinforcement learning framework.


  * applications
  * overview
  * theory
  * deep reinforcement learning
  * value-based methods
  * policy-based methods
  * model-based methods
  * bandits
  * benchmarks
  * frameworks
  * interesting papers
    - applications
    - exploration
    - model-based methods
    - value-based methods
    - policy-based methods
    - guided policy search
    - inverse reinforcement learning


  selected papers and books - https://dropbox.com/sh/zc5qxqksgqmxs0a/AAA4C1y_6Y0-3dm3gPuQhb_va




  "Reinforcement Learning is a general-purpose framework for decision-making:
   - Is for an agent with the capacity to act
   - Each action influences the agent's future state
   - Success is measured by a scalar reward signal
   - Goal: select actions to maximise future reward"

  "Deep Learning is a general-purpose framework for representation learning:
   - Given an objective
   - Learn representation that is required to achieve objective
   - Directly from raw inputs
   - Using minimal domain knowledge"

  "We seek a single agent which can solve any human-level task:
   - Reinforcement Learning defines the objective
   - Deep Learning gives the mechanism
   - Reinforcement Learning + Deep Learning = general intelligence"

  relations with other fields - https://goo.gl/XlgPJu




[applications]

  play games: Atari, poker, Go, ...
  control physical systems: manipulate, walk, drive, fly, ...
  interact with users: recommend, retain customers, personalise, optimise user experience, ...
  solve logistical problems: scheduling, bandwidth allocation, elevator control, cognitive radio, power optimisation, ...
  learn sequential algorithms: attention, memory, conditional computation, activations, ...


  "Why Tool AIs Want to Be Agent AIs" - http://www.gwern.net/Tool%20AI
	"The logical extension of these neural networks all the way down papers is that an actor like Google/Baidu/Facebook/MS could effectively turn neural networks into a black box: a user/developer uploads through an API a dataset of input/output pairs of a specified type and a monetary loss function, and a top-level neural network running on a large GPU cluster starts autonomously optimizing over architectures & hyperparameters for the neural network design which balances GPU cost and the monetary loss, interleaved with further optimization over the thousands of previous submitted tasks, sharing its learning across all of the datasets/loss functions/architectures/hyperparameters, and the original user simply submits future data through the API for processing by the best neural network so far."


  - industry
	Google (recommender system, Deep Q-Learning) - https://deepmind.com/blog/deep-reinforcement-learning/
		"We have also built a massively distributed deep RL system, known as Gorila, that utilises the Google Cloud platform to speed up training time by an order of magnitude; this system has been applied to recommender systems within Google."

	Google (datacenter cooling, Deep Q-Learning) - https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/

	Microsoft Multiworld Testing Decision Service (personalized news on MSN, contextual bandits) - https://mwtds.azurewebsites.net + http://research.microsoft.com/en-us/projects/mwt/ + http://hunch.net/?p=4464948

	(Demis Hassabis) "In our view reinforcement learning is going to be as big as deep learning in the next two or three years. The further we go with this, the more we feel our thesis is correct, and I think we’re changing the entire field." "Technology could surface in virtual assistants or improve recommendation systems, which are crucial to products such as YouTube (similar systems also power some of Google’s advertising products)." [https://www.technologyreview.com/s/601139/how-google-plans-to-solve-artificial-intelligence/]

	Google's patent on Deep Q-Network [http://google.com/patents/US20150100530]:
	"Further applications of the techniques we describe, which are merely given by way of example, include: robot control (such as bipedal or quadrupedal walking or running, navigation, grasping, and other control skills); vehicle control (autonomous vehicle control, steering control, airborne vehicle control such as helicopter or plane control, autonomous mobile robot control); machine control; control of wired or wireless communication systems; control of laboratory or industrial equipment; control of real or virtual resources (such as memory management, inventory management and the like); drug discovery (where the controlled action is, say, the definition of DNA sequence of a drug and the states are defined by states of a living entity to which the drug is applied); application to a system in which the state of or output from the system is defined by words (text and/or audio and/or image), such as a system employing natural language; application to a trading system such as a stock market (although the actions taken may have little effect on such a system, very small effects can be sufficient to achieve useful overall rewards); and others."

  - Go
	"Mastering the Game of Go with Deep Neural Networks and Tree Search" - https://vk.com/doc-44016343_437229031?hash=9c999a03ee82850948&dl=56ce06e325d42fbc72
	"Google AlphaGo is a historical tour of AI ideas: 70s (Alpha-Beta), 80s/90s (RL & self-play), 00's (Monte-Carlo), 10's (deep neural networks)."
	history of ideas by Sutton, Szepesvari, Bowling, Hayward, Muller - http://youtube.com/watch?v=UMm0XaCFTJQ

	https://youtu.be/i3lEG6aRGm8?t=16m (Hassabis)
	https://youtube.com/watch?v=4D5yGiYe8p4 (Silver)
	https://youtu.be/LX8Knl0g0LE?t=4m41s (Huang)
	https://youtu.be/yCALyQRN3hw?t=15m18s + https://youtu.be/qUAmTYHEyM8?t=15m31s + https://youtu.be/mzpW10DPHeQ?t=10m22s (Silver, Maddison)

	https://xcorr.net/2016/02/03/5-easy-pieces-how-deepmind-mastered-go/
	https://github.com/Rochester-NRT/RocAlphaGo/wiki

	https://deepmind.com/research/alphago/alphago-games-english/

	https://gogameguru.com/alphago-defeats-lee-sedol-game-1/ + http://deeplearningskysthelimit.blogspot.ru/2016/04/part-6-review-of-game-1-lee-sedol.html + https://youtube.com/watch?v=bIQxOsRAXCo
	https://gogameguru.com/alphago-races-ahead-2-0-lee-sedol/ + http://deeplearningskysthelimit.blogspot.ru/2016/04/part-7-review-of-game-2-alphagos-new.html + https://youtube.com/watch?v=1aMt7ulL6EI
	https://gogameguru.com/alphago-shows-true-strength-3rd-victory-lee-sedol/ + http://deeplearningskysthelimit.blogspot.ru/2016/04/part-8-review-of-game-3-lee-sedols.html + https://youtube.com/watch?v=6hROM_bxZ9E
	https://gogameguru.com/lee-sedol-defeats-alphago-masterful-comeback-game-4/ + http://deeplearningskysthelimit.blogspot.ru/2016/04/part-9-review-of-game-4-lee-sedols.html + https://youtube.com/watch?v=G5gJ-pVo1gs
	https://gogameguru.com/alphago-defeats-lee-sedol-4-1/ + http://deeplearningskysthelimit.blogspot.ru/2016/05/part-10-review-of-game-5-alphago.html + https://youtube.com/watch?v=QxHdPdRcMhw

  - Doom
	http://vizdoom.cs.put.edu.pl
	https://youtube.com/watch?v=Qv4esGWOg7w
	https://youtube.com/watch?v=947bSUtuSQ0
	https://youtube.com/watch?v=tDRdgpkleXI

  - Chess
	"Giraffe: Using Deep Reinforcement Learning to Play Chess" - http://arxiv.org/abs/1509.01549

  - Poker
	"DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker" - http://arxiv.org/abs/1701.01724
	"Deep Reinforcement Learning from Self-Play in Imperfect-Information Games" - http://arxiv.org/abs/1603.01121
	"Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games Using Convolutional Networks" - http://colinraffel.com/publications/aaai2016poker.pdf

  - Bridge
	"Automatic Bridge Bidding Using Deep Reinforcement Learning" - http://arxiv.org/abs/1607.03290

  - 2048
	https://arxiv.org/abs/1604.05085
	https://github.com/georgwiese/2048-rl
	https://github.com/wonjunyoon/2048_deepql_torch

  - robotics
	"Making Robots Learn" by Pieter Abbeel -
		https://youtu.be/xe-z4i3l-iQ?t=30m35s
		http://on-demand.gputechconf.com/gtc/2016/video/S6812.html
		http://youtube.com/watch?v=xMHjkZBvnfU
	"Deep Robotic Learning" by Sergey Levine -
		http://videolectures.net/iclr2016_levine_deep_learning/
		https://youtube.com/watch?v=f41JXf-ojrM
		https://youtube.com/watch?v=EtMyH_--vnU
		https://video.seas.harvard.edu/media/ME+Sergey+Levine+2015+-04-01/1_gqqp9r3o/23375211
	"Learning Deep Visuomotor Policies" by Trevor Darrell -
		https://youtu.be/xgUZTvAZfDo?t=28m20s

	MuJoCo physics simulator - http://mujoco.org/gallery.html
	V-REP - http://coppeliarobotics.com
	Gazebo - http://gazebosim.org
	Unreal Engine (Torch integration) - https://github.com/facebook/UETorch

  - OpenAI Universe
	https://universe.openai.com
	https://openai.com/blog/universe/




[overview]

  introduction by David Silver - https://youtube.com/watch?v=2pWv7GOvuf0

  introduction -
	http://kvfrans.com/reinforcement-learning-basics/
	http://kvfrans.com/markov-processes-in-reinforcement-learning/
	http://kvfrans.com/planning-policy-evaluation-policy-iteration-value-iteration/
	http://kvfrans.com/model-free-prediction-and-control/
	http://kvfrans.com/the-policy-gradient/
	http://kvfrans.com/making-use-of-the-model/

  introduction by Shakir Mohamed -
	"Learning in Brains and Machines: Temporal Differences" - http://blog.shakirm.com/2016/02/learning-in-brains-and-machines-1/
	"Synergistic and Modular Action" - http://blog.shakirm.com/2016/07/learning-in-brains-and-machines-3-synergistic-and-modular-action/

  introduction by Michael Littman - http://readcube.com/articles/10.1038%2Fnature14540


  overview by David Silver
	http://techtalks.tv/talks/deep-reinforcement-learning/62360/
	http://videolectures.net/rldm2015_silver_reinforcement_learning/
	http://youtube.com/watch?v=qLaDWKd61Ig
	http://youtube.com/watch?v=3hWn5vMnpiM

  overview by Peter Abbeel
	http://youtube.com/watch?v=evq4p1zhS7Q
	http://research.microsoft.com/apps/video/default.aspx?id=260045


  tutorial by Rich Sutton
	https://youtube.com/watch?v=Fsh1qMTg1xI
	https://gridworld.wordpress.com/2015/12/08/nips-2015-rl-tutorial/

  tutorial by Emma Brunskill
	https://youtube.com/watch?v=fIKkhoI1kF4 + https://youtube.com/watch?v=8hK0NnG_DhY


  course by David Silver
	http://youtube.com/playlist?list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa
	http://machinelearningtalks.com/tag/rl-course/
	http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html

  course by Michael Littman
	https://udacity.com/course/reinforcement-learning--ud600


  "MDP Cheatsheet Reference" by John Schulman - http://rll.berkeley.edu/deeprlcourse/docs/mdp-cheatsheet.pdf
  course notes by Dustin Tran - http://dustintran.com/notes/cs282r.pdf
  course slides by Rich Sutton - http://incompleteideas.net/sutton/609%20dropbox/slides%20(pdf%20and%20keynote)/


  Rich Sutton and Andrew Barto - "Reinforcement Learning: An Introduction" (second edition, draft) - https://dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf
  Rich Sutton and Andrew Barto - "Reinforcement Learning: An Introduction" (first edition) - http://webdocs.cs.ualberta.ca/~sutton/book/ebook/the-book.html
  Csaba Szepesvari - "Algorithms for Reinforcement Learning" - http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf


  exercises with solutions - http://wildml.com/2016/10/learning-reinforcement-learning/

  collection of resources - https://github.com/aikorea/awesome-rl




[theory]

  differences between reinforcement learning and other learning paradigms  [https://youtube.com/watch?v=2pWv7GOvuf0&t=9m37s]:
  - there is no supervisor, only a reward signal
  - feedback is delayed, not instantaneous
  - time really matters (sequential, not i.i.d. data)
  - agent's actions affect subsequent data it receives

  differences between reinforcement learning and supervised learning  [https://youtube.com/watch?v=8jQIKgTzQd4&t=50m28s]:
  - no full access to analytic representation of loss function being optimized - value has to be queried by interaction with environment
  - interacting with stateful environment (unknown, nonlinear, stochastic, arbitrarily complex) - next input depends on previous actions

  characteristics:
  - can learn any function
  - inherently handles uncertainty
      uncertainty in actions (the world)
      uncertainty in observations (sensors)
  - directly maximise criteria we care about (instead of loss function on samples)
  - copes with delayed feedback
      temporal credit assignment problem

  challenges:
  - stability (non-stationary, fleeting nature of time and online data)
  - credit assigment (delayed rewards and consequences)
  - exploration vs exploitation (need for trial and error)
  - using learned model of environment

  open problems:
  - adaptive methods which work under large number of conditions
  - addressing exploration problem in large MDPs
  - large-scale empirical evaluations
  - learning and acting under partial information
  - modular and hierarchical learning over multiple time scales
  - sample efficiency
  - improving existing value-function and policy search methods
  - algorithms that work well with large or continuous action spaces
  - transfer learning
  - lifelong learning
  - efficient sample-based planning (e.g., based on Monte-Carlo tree search)
  - multiagent or distributed learning
  - learning from demostrations


  dimensions for classification of methods - https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node105.html

  - prediction vs control
  - MDPs vs bandits (one state, non-sequential)
  - on-policy vs off-policy
  - bootstrapping vs Monte Carlo
  - value-based vs policy-based
  - model-based vs model-free


  value-based reinforcement learning
        estimate the optimal value function Q*(s,a) (expected total reward from state s and action a under policy π)
        this is the maximum value achievable under any policy

  policy-based reinforcement learning
        search directly for the optimal policy (behaviour function selecting actions given states) achieving maximum expected reward
	often simpler to represent and learn good policies than good state value or action value functions (such as for robot grasping an object)
	state value function doesn't prescribe actions (dynamics model becomes necessary)
	action value function requires to solve maximization problem over actions (challenge for continuous / high-dimensional action spaces)
	focus on discriminating between several actions instead of estimating values for every state-action
	true objectives of expected cost is optimized (vs a surrogate like Bellman error)
	suboptimal values does not necessarily give suboptimal actions in every state (but optimal values do)
	easier generalization to continuous action spaces


  model-based reinforcement learning
        build prediction model for next state and reward after action
        define objective function measuring goodness of model
	e.g. number of bits to reconstruct next state
        plan (e.g. by lookahead) using model
	allows reasoning about task-independent aspects of environment
	allows for transfer learning across domains and faster learning

	"The reinforcement learning problem can be decomposed into two parallel types of inference: estimating the parameters of a model for the underlying process, and determining behavior which maximizes return under the estimated model."
	"Two fundamentally different classes of model based reinforcement learning methods.
	Simulation based techniques involve learning some kind of forward model of the environment from which future samples can be generated. Given access to such models, planning can be performed directly using search. Although these methods demonstrate quite impressive performance on small domains possessing complicated dynamics, scaling them to large state or observation spaces has proven challenging. The main difficulty that arises when using learnt forward models is that the modeling errors tend to compound when reasoning over long time horizons.
	In contrast, another family of techniques, referred to in the literature as planning as inference, attempt to side-step the issue of needing to perform accurate simulations by reducing the planning task to one of probabilistic inference within a generative model of the system. The experimental results to date have been somewhat inconclusive, making it far from clear whether the transformed problem is any easier to solve in practice."
	reduce complex real-world problem to tractable structured Markov Decision Process (~Dynamic Bayesian Network, Neural Network, Memory) automatically by learning relevant features


  abstractions for states and actions
	"Combining State and Temporal Abstractions" - https://youtube.com/watch?v=iLSUByYY6so (Konidaris)
	"Towards Representations for Efficient Reinforcement Learning" - https://youtube.com/watch?v=Pk3E5zqhl9k (Brunskill)

	hierarchical reinforcement learning
		http://blog.shakirm.com/2016/07/learning-in-brains-and-machines-3-synergistic-and-modular-action/

		- simplifies dimensionality of the action spaces over which we need to reason
		- enables quick planning and execution of low-level actions (such as robot movements)
		- provides a simple mechanism that connects plans and intentions to commands at the level of execution
		- supports rapid learning and generalisation (that humans are capable of)

		introduction to options framework by Doina Precup - http://videolectures.net/deeplearning2016_precup_advanced_lr/
		"Advances in option construction: The option-critic architecture" - https://youtube.com/watch?v=8r_EoYnPjGk (Bacon)


  off-policy learning
  - evaluate target policy to compute control while following another policy
  - learn from observing humans or other agents
  - re-use experience generated from old policies
  - learn about optimal policy while following exploratory policy
  - learn about multiple policies (options, waypoints) while following one policy


  intrinsic motivation as additional reward function
  - uncertainty motivation - maximize prediction error/surprise of observations
  - information gain - minimize uncertainty in model of environment
  - empowerment - maximize mutual information between actions and future state

  forms of supervision other than reward function
   - demonstrated behavior -> imitation, inferring intention
   - self-supervision, prediction -> model-based control
   - auxiliary objectives
	additional sensing modalities
	learning related tasks
	task-relevant properties of the world


  inverse reinforcement learning
	"The objective is to infer the underlying reward structure guiding an agent’s behaviour based on observations and a model of the environment. This may be done either to learn the reward structure for modelling purposes (reward learning) or to provide a framework to allow the agent to imitate a demonstrator’s specific behaviour (apprenticeship learning)."

	introduction by Pieter Abbeel - http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (part 2, 20:40) + http://www.cs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf
	"Apprenticeship Learning and Reinforcement Learning with Application to Robotic Control" by Pieter Abbeel - http://ai.stanford.edu/~pabbeel/thesis/thesis.pdf




[deep reinforcement learning]

  "Deep Reinforcement Learning" by John Schulman - http://rll.berkeley.edu/deeprlcourse/docs/2015.08.26.Lecture01Intro.pdf

  "Brief Review of Deep Reinforcement Learning" by Junlong Liu - https://plus.google.com/photos/103321132484611285919/albums/6164519479726594273

  Deep Reinforcement Learning tutorial from OpenAI - https://gym.openai.com/docs/rl


  introduction by Nando de Freitas
	http://youtube.com/watch?v=kUiR0RLmGCo (Deep Policy Search)
	http://youtube.com/watch?v=dV80NAlEins (Deep Q-Learning)
	http://youtube.com/watch?v=HUmEbUkeQHg

  course by David Silver
	http://youtube.com/watch?v=UoPei5o4fps (Deep Q-Learning)
	http://youtube.com/watch?v=KHZVXao4qXs (Deep Policy Search)

  course by John Schulman and Pieter Abbeel
	https://youtube.com/watch?v=PtAIh9KSnjo
	https://youtube.com/watch?v=aUrX-rP_ss4
	https://youtube.com/watch?v=oPGVsoBonLM
	https://youtube.com/watch?v=rO7Dx8pSJQw
	https://youtube.com/watch?v=gb5Q2XL5c8A
	http://goo.gl/5wsgbJ

  course by Sergey Levine, John Schulman and Chelsea Finn
	https://youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX
	http://rll.berkeley.edu/deeprlcourse/


  tutorial by Pieter Abbeel
	http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Deep Policy Search)

  tutorial by Jan Peters and Gerhard Neumann
	http://videolectures.net/icml2015_neumann_peters_policy_search/ + http://icml.cc/2015/tutorials/PolicySearch.pdf

  seminars on latest developments in deep reinforcement learning
	"Latest Developments in Deep Reinforcement Learning" by Pavlov and Seleznev (in russian) - https://youtube.com/watch?v=mrgJ53TIcQc


  "The Nuts and Bolts of Deep RL Research" by John Schulman - http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf


  "Abstraction in RL" workshop at ICML 2016 - http://rlabstraction2016.wix.com/icml
  "Deep Reinforcement Learning: Frontiers and Challenges" workshop at IJCAI 2016 - https://sites.google.com/site/deeprlijcai16/
  "Deep Reinforcement Learning" workshop at NIPS 2015 - http://rll.berkeley.edu/deeprlworkshop/ +
	https://gridworld.wordpress.com/2015/12/13/nips-2015-deep-rl-workshop/
  "Novel Trends and Applications in RL" workshop at NIPS 2014 - https://tcrl14.wordpress.com/videos/


  what's the right core model-free algorithm is not clear
  - policy gradients (score function vs reparametrization, natural vs not natural) vs Q-learning vs derivative-free optimization
  - desired properties: scalable, sample-efficient, robust, learning from off-policy data
	policy gradient methods scalable, not sample-efficient, not robust, no off-policy
	trust region methods less scalable, more sample-efficient, more robust, no off-policy
	Q-Learning scalable in state space, little more sample-efficient, not robust, no off-policy




[value-based methods]

  http://videolectures.net/rldm2015_silver_reinforcement_learning/ (David Silver)
  http://youtu.be/qLaDWKd61Ig?t=9m16s (David Silver)
  http://youtube.com/watch?v=dV80NAlEins (Nando de Freitas)
  http://youtube.com/watch?v=HUmEbUkeQHg (Nando de Freitas)
  http://youtube.com/watch?v=mrgJ53TIcQc (Mikhail Pavlov, in russian)


  Deep Q-Network:
  - use deep network to represent value function/policy/model
  - optimise value function/policy/model end-to-end using SGD

  Prior to DQN, it was generally believed that learning value functions using large, non-linear function approximators was difficult and unstable. DQN is able to learn value functions using such function approximators in a stable and robust way due to two innovations:
  - The network is trained off-policy with samples from a replay buffer to minimize correlations between samples.
  - The network is trained with a target Q network to give consistent targets during temporal difference backups.

  stability issues with deep RL:
  - naive Q-learning oscillates or diverges with neural nets
      data is sequential
      successive samples are correlated, non-iid
  - policy changes rapidly with slight changes to Q-values
      policy may oscillate
      distribution of data can swing from one extreme to another
  - scale of rewards and Q-values is unknown
      naive Q-learning gradients can be large
      unstable when backpropagated

  DQN provides a stable solution to deep value-based RL:
  - use experience replay
      break correlations in data, bring us back to iid setting
      learn from all past policies
      using off-policy Q-learning
  - freeze target Q-network
      avoid oscillations
      break correlations between Q-network and target
  - clip rewards or normalize network adaptively to sensible range
      robust gradients

  deterministic actor-critic:
  - use two networks for actor and critic
  - actor is policy p(s, u) with parameters u
  - critic is value function Q(s, a, w)
  - critic provides loss function for actor
  - gradient backpropagates from critic into actor


  open research problems:
  - hierarchy
  - exploration
  - memory
  - state construction
  - partial observability

  limitations of DQN (Andrej Karpathy):
  "- Most crucially, the exploration used is random. You button mash random things and hope to receive a reward at some point or you're completely lost. If anything at any point requires a precise sequence of actions to get a reward, exponentially more training time is necessary.
  - Experience replay that performs the model updates is performed uniformly at random, instead of some kind of importance sampling.
  - A discrete set of actions is assumed. Any real-valued output (e.g. torque on a join) is a non-obvious problem in the current model. [addressed by Deterministic Actor-Critic]
  - There is no transfer learning between games. The algorithm always starts from scratch. This is very much unlike what humans do in their own problem solving.
  - The agent's policy is reactive. It's as if you always forgot what you did 1 second ago. You keep repeatedly "waking up" to the world and get 1 second to decide what to do.
  - Q Learning is model-free, meaning that the agent builds no internal model of the world/reward dynamics. Unlike us, it doesn't know what will happen to the world if it perfoms some action. This also means that it does not have any capacity to plan anything. [not yet successful attempts to address]
    Of these, the biggest and most insurmountable problem is the first one: Random exploration of actions. As humans we have complex intuitions and an internal model of the dynamics of the world. This allows us to plan out actions that are very likely to yield a reward, without flailing our arms around greedily, hoping to get rewards at random at some point. Games like Starcraft will significantly challenge an algorithm like this. You could expect that the model would develop super-human micro, but have difficulties with the overall strategy. For example, performing an air drop to enemy base would be impossible with the current model: You'd have to plan it out over many actions: "load the marines into the ship, fly the ship in stealth around the map, drop it at the precise location of enemy base". Hence, DQN is best at games that provide immediate rewards, and where you can afford to "live in the moment" without much planning. Shooting things in space invaders is a good example."


  implementations:
   - https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner + https://sites.google.com/a/deepmind.com/dqn/
   - https://github.com/carpedm20/deep-rl-tensorflow
   - https://github.com/Kaixhin/Atari (persistent advantage learning, dueling network architecture, Double Q-learning)
   - https://github.com/tambetm/simple_dqn + https://youtu.be/KkIf0Ok5GCE + https://youtu.be/0ZlgrQS3krg
   - https://github.com/tambetm/gymexperiments
   - https://github.com/spragunr/deep_q_rl + http://sodeepdude.blogspot.ru/2015/03/deepminds-atari-paper-replicated.html
   - https://github.com/ugo-nama-kun/DQN-chainer + http://youtube.com/watch?v=N813o-Xb6S8
   - https://github.com/muupan/dqn-in-the-caffe
   - https://github.com/brian473/neural_rl
   - https://github.com/kristjankorjus/Replicating-DeepMind
   - https://github.com/asrivat1/DeepLearningVideoGames
   - https://github.com/Jabberwockyll/deep_rl_ale
   - https://github.com/VinF/deer
   - https://github.com/sherjilozair/dqn
   - https://github.com/gliese581gg/DQN_tensorflow
   - https://github.com/devsisters/DQN-tensorflow
   - https://github.com/osh/kerlym
   - https://github.com/ludc/rltorch/blob/master/torch/policies/DeepQPolicy.lua
   - http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html




[policy-based methods]

  http://karpathy.github.io/2016/05/31/rl/


  methods:
   - derivative free methods
	Cross Entropy Method, Finite Differences, Fixed Random Seed
   - likelihood ratio (LR) policy gradient
	natural gradient / trust regions (TRPO)
	actor-critic (GAE, A3C)
   - path derivative (PD)
	DPG, DDPG, SVG
   - stochastic computation graphs (generalizes LR and PD)
   - guided policy search (reduction to supervised learning)

  "Benchmarking Deep Reinforcement Learning for Continuous Control" by Duan, Chen, Houthooft, Schulman, Abbeel -
	http://arxiv.org/abs/1604.06778
	http://techtalks.tv/talks/benchmarking-deep-reinforcement-learning-for-continuous-control/62380/



  * Cross Entropy Method (no gradient estimation)
  	"If your policy has a small number of parameters (say 20), and sometimes even if it has a moderate number (say 2000), you might be better off using the Cross-Entropy Method than any of the fancy methods. It works like this: (1) Sample n sets of parameters from some prior that allows for closed-form updating, e.g. a multivariate Gaussian. (2) For each parameter set, compute a noisy score by running your policy on the environment you care about. (3) Take the top 20% percentile (say) of sampled parameter sets. Fit a Gaussian distribution to this set, then go to (1) and repeat using this as the new prior."

  	http://aaai.org/Papers/ICML/2003/ICML03-068.pdf


  "Most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well. PG is preferred because it is end-to-end: there’s an explicit policy and a principled approach that directly optimizes the expected reward."

  "Policy Gradient works well only in settings where there are a few discrete choices so that one is not hopelessly sampling through huge search spaces."

  optimization issues:
   - inefficient use of data, large number of samples required
  	each experience is only used to compute one gradient (on-policy)
  	given a batch of trajectories what's the most we can do with it?
   - hard to choose reasonable stepsize that works for the whole optimization
  	we have a gradient estimate, no objective for line search
  	statistics of data (observations and rewards) change during learning

  "For both reinforcement learning and variational inference, there are two widely known ways of optimizing a policy (or variational distribution) based on sampled sequences of actions and outcomes: There’s (a) the likelihood-ratio estimator, which updates the policy such that action sequences that lead to higher scores happen more often and that doesn’t need gradients, and (b) the pathwise estimator, which adjusts individual actions such that the policy results in a higher score and that needs gradients. While pathwise methods may be more sample-efficient, they work less generally due to high bias and don’t scale up as well to very high-dimensional problems."


  * REINFORCE / Likelihood Ratio Policy Gradient (likelihood-ratio estimation)
  	https://theneural.wordpress.com/2011/09/13/the-useless-beauty-of-reinforce/ (Ilya Sutskever)
  	https://youtube.com/watch?v=oPGVsoBonLM + https://youtube.com/watch?v=rO7Dx8pSJQw + http://goo.gl/5wsgbJ (John Schulman)
  	http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Pieter Abbeel, 0:16:40)

  	score function gradient estimator
  		∇θE x~θ[f(x)] = ∇θ ∫dx p(x|θ)f(x) = ∫dx ∇θp(x|θ)f(x) = ∫dx p(x|θ)∇θp(x|θ)/p(x|θ)*f(x) = ∫dx p(x|θ)∇θlog p(x|θ)f(x) = E x~θ[f(x)∇θlog p(x|θ)]
  		unbiased gradient estimator: sample xi ~ p(x|θ) and compute gi=f(xi)∇θlog p(xi|θ)
  		valid even if f(x) is discontinuous and unknown, or if sample space containing x is discrete

  		E x~θ[f(x)] = E x~θold[p(x|θ)/p(x|θold)*f(x)]  (alternative derivation using importance sampling)
  		∇θE x~θ[f(x)] = E x~θold[∇θp(x|θ)/p(x|θold)*f(x)]
  		∇θE x~θ[f(x)]|θ=θold = E x~θold[∇θp(x|θ)|θ=θold / p(x|θold)*f(x)] = E x~θold[∇θlog p(x|θ)|θ=θold*f(x)]

  		baseline: if ∀x f(x)>=0 then for any xi gradient estimator tries to push up it's density but it should push up density only for better-than-average xi
  		∇θE x~θ[f(x)] = ∇θE x~θ[f(x) - b] = E x~θ[∇θ log p(x|θ)(f(x) - b)] where optimal b is always E[f(x)]

  	∇θ Σx pθ(x)r(x) = Σx pθ(x)∇θlog(pθ(x))r(x)  (score function gradient estimator)
  	Σx pθ(x)∇θlog(pθ(x)) = Σx pθ(x)∇θpθ(x)/pθ(x) = ∇θ Σx pθ(x) = ∇θ[1] = 0 =>
  	∇θ Σx pθ(x)r(x) = Σx pθ(x)∇θlog(pθ(x))(r(x)-ravg)  (baseline)

  	τ=(s0,a0,r0,s1,a1,r1,...,sT)
  	∇θEτ[R(τ)]=Eτ[∇θ log p(τ|θ)R(τ)]
  	p(τ|θ) = μ(s0) Πt=0..T-1 [π(at|st,θ)P(s+1,rt|st,at)]
  	∇θ log p(τ|θ) = ∇θ Σt=0..T-1 log π(at|st,θ)
  	∇θEτ[R(τ)] = Eτ[(Σt=0..T-1 rt)(∇θ Σt=0..T-1 log π(at|st,θ))]

  	∇θEτ[rt] = E[rt*(Σt'=0..t ∇θ log π(at'|st',θ))]  (single reward depends only on previous actions)
  	∇θE[R(τ)] = E[Σt=0..T-1 rt*(Σt'=0..T-1 ∇θ log π(at'|st',θ))]  (summing rewards over t)
  	∇θE[R(τ)] = E[Σt'=0..T-1 (∇θ log π(at'|st',θ))(Σt=t'..T-1 rt)]
  	  (due to Markov property st summarizes all previous effects thus considering only future rewards for action)

  	Ex[∇θ log π(at'|st',θ)] = 0 => ∇θE[R(τ)] = (Σt'=0..T-1 ∇θ log π(at'|st',θ))(Σt=t'..T-1 rt - b(st))  (baseline)
  	Actor-Critic: use value function as suboptimal baseline to further isolate effect of action at the cost of bias

  	implementations:
  	 - https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5 (Karpathy) + http://karpathy.github.io/2016/05/31/rl/
  	 - https://github.com/kvfrans/openai-cartpole/blob/master/cartpole-policygradient.py + http://kvfrans.com/simple-algoritms-for-solving-cartpole/
  	 - http://rl-gym-doc.s3-website-us-west-2.amazonaws.com/mlss/pg-startercode.py (Schulman)
  	 - https://github.com/osh/kerlym/blob/master/kerlym/pg.py
  	 - https://github.com/ludc/rltorch/blob/master/torch/policies/PolicyGradient.lua
  	 - https://github.com/ludc/rltorch/blob/master/torch/policies/RecurrentPolicyGradient.lua
  	 - https://github.com/rllab/rllab/blob/master/rllab/algos/vpg.py (Rocky Duan)


  * Trust Region Policy Optimization (improvement on Likelihood Ratio Policy Gradient)
  	second order policy gradient algorithm that is highly effective on both continuous and discrete control problems

  	"As you iteratively improve your policy, it’s important to avoid parameter updates that change your policy too much, as enforced by constraining the KL divergence between the distributions predicted by the old and the new policy on a batch of data to be less than some constant δ. This δ (in the unit of nats) is better than a fixed step size, since the meaning of the step size changes depending on what the rewards and problem structure look like at different points in training. This is called Trust Region Policy Optimization (or, in a first-order variant, Proximal Policy Optimization) and it matters more as we do more experience replay. Instead of conjugate gradients the simplest instantiation of this idea could be implemented by doing a line search and checking the KL along the way."

  	https://arxiv.org/abs/1502.05477

  	https://youtu.be/xe-z4i3l-iQ?t=30m35s (Abbeel)
  	http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, 0:27:10)
  	https://youtube.com/watch?v=gb5Q2XL5c8A (Schulman)

  	want to optimize η(θ) = η(θold) + Er~π[Aπ(s0,a0)+Aπ(s1,a1)+...]
  	collected data with policy parameters θold, now want to do update
  	can't use η(θ) = η(θold) + Es0..∞[Ea0..∞[Aπ(s0,a0)+Aπ(s1,a1)+...]] because state distribution has complicated dependence on π(θ)
  	derived local approximation Lθold(θ) = Es0..∞[Σt=1..T-1 Ea~θ[π(at|st,θ)/π(at|st,θold)*Aθ(st,at)]] which matches η(θ) to first order around θold
  	∇θ Lθold(θ) = Es0..∞[Σt=1..T-1 Ea~θ[∇θπ(at|st,θ)/π(at|st,θold)*Aθ(st,at)]] = Es0..∞[Σt=1..T-1 Ea~θ[∇θlog π(at|st,θ) Aθ(st,at)]] = ∇θη(θ)|θ=θold
  	optimizing KL penalized local approximation gives guaranteed improvement to η: η(θ) >= Lθold(θ) - C * max s[Dkl(π(.|θold,s) || π(.|θ,s))]

  	implementations:
  	 - https://github.com/joschu/modular_rl
  	 - https://github.com/rll/deeprlhw2/blob/master/ppo.py + https://github.com/rll/deeprlhw2
  	 - https://github.com/wojzaremba/trpo + https://github.com/wojzaremba/trpo_rnn
  	 - https://github.com/rllab/rllab/blob/master/rllab/algos/trpo.py
  	 - https://github.com/kvfrans/parallel-trpo


  * Actor-Critic (improvement on Likelihood Ratio Policy Gradient)
  	critic provides loss function for actor

  	http://videolectures.net/rldm2015_silver_reinforcement_learning/ (Silver, 1:07:23)
  	https://youtu.be/qLaDWKd61Ig?t=38m58s (Silver)
  	https://youtu.be/mrgJ53TIcQc?t=1h3m2s (Seleznev, in russian)
  	https://youtu.be/rO7Dx8pSJQw?t=50m (Schulman)
  	http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, 0:49:45)

  	http://www.rage.net/~greg/2016-07-05-ActorCritic-with-OpenAI-Gym.html

  	cumulative discounted reward
  		J(πθ) = ∫S ρπ(s) ∫A πθ(s,a)r(s,a)da = E s~ρπ,a~πθ[r(s,a)]
  		J(μθ) = ∫S ρμ(s)r(s,μθ(s))ds = E s~ρμ[r(s,μθ(s))]  (deterministic policy)
  		ρπ(s') = ∫S (Σt=1..∞ γ^(t-1)p1(s)p(s->s',t,π))ds

  	stochastic policy gradient theorem (Sutton, 1999)
  		∇θJ(πθ) = ∫S ρπ(s) ∫A ∇θπθ(a|s)Qπ(s,a)dads = E s~ρπ,a~πθ[∇θlogπθ(a|s)Qπ(s,a)]


  	"In advantage learning one throws away information that is not needed for coming up with a good policy. The argument is that throwing away information allows you to focus your resources on learning what is important.
  	As an example consider Tetris when you gain a unit reward for every time step you survive. Arguably the optimal value function takes on large values when the screen is near empty, while it takes on small values when the screen is near full. The range of differences can be enormous (from millions to zero). However, for optimal decision making how long you survive does not matter. What matters is the small differences in how the screen is filled up because this is what determines where to put the individual pieces. If you learn an action value function and your algorithm focuses on something like the mean square error, i.e., getting the magnitudes right, it is very plausible that most resources of the learning algorithm will be spent on capturing how big the values are, while little resource will be spent on capturing the value differences between the actions. This is what advantage learning can fix. The fix comes because advantage learning does not need to wait until the value magnitudes are properly captured before it can start learning the value differences.
  	As can be seen from this example, advantage learning is expected to make a bigger difference where the span of optimal values is orders of magnitudes larger than action-value differences."


  	Generalized Advantage Estimation
  		http://arxiv.org/abs/1506.02438

  		https://youtu.be/xe-z4i3l-iQ?t=30m35s (Abbeel)
  		https://youtu.be/rO7Dx8pSJQw?t=40m20s (Schulman)

  		implementations:
  		- https://github.com/joschu/modular_rl
  		- https://github.com/rll/deeprlhw2/blob/master/ppo.py + https://github.com/rll/deeprlhw2

  	Asynchronous Advantage Actor-Critic (A3C)
  		learns only a state value function V(st) rather than an action value function Q(st, at) and thus cannot pass back gradients of the value with respect to the action to the actor
  		approximates the action value with the rewards from several steps of experience and passes the TD error to the actor
  		exploits the multithreading capabilities of standard CPUs and executes many instances of the agent in parallel using shared model
  		alternative to experience replay since parallelisation also diversifies and decorrelates the data

  		https://arxiv.org/abs/1602.01783

  		https://youtube.com/watch?v=9sx1_u2qVhQ (Mnih)
  		http://techtalks.tv/talks/asynchronous-methods-for-deep-reinforcement-learning/62475/ (Mnih)

  		implementations:
  		 - https://github.com/yandexdataschool/AgentNet/blob/master/agentnet/learning/a2c_n_step.py
  		 - https://github.com/Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning
  		 - https://github.com/miyosuda/async_deep_reinforce
  		 - https://github.com/muupan/async-rl
  		 - https://github.com/coreylynch/async-rl
  		 - https://github.com/carpedm20/deep-rl-tensorflow/blob/master/agents/async.py
  		 - https://github.com/danijar/mindpark/blob/master/mindpark/algorithm/a3c.py


  * Path Derivative Policy Gradient (pathwise estimation)
  	http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, 01:02:04)

  	"Gradient Estimation Using Stochastic Computation Graphs" by Schulman, Heess, Weber, Abbeel - http://arxiv.org/abs/1506.05254


  	deterministic policy gradient theorem (Silver, 2014)
  		∇θJ(μθ) = ∫S ρμ(s)∇θμθ(s)∇aQμ(s,a)|a=μθ(s)ds = E s~ρμ[∇θμθ(s)∇aQμ(s,a)|a=μθ(s)]

  	Deep Deterministic Policy Gradient (DDPG)
  		provides a continuous analogue to DQN, exploiting the differentiability of the Q-network
  		instead of requiring samples from a stochastic policy and encouraging the ones that get higher scores, the approach uses a deterministic policy and gets the gradient information directly from a second network (called a critic) that models the score function
  		determinism of the policy allows the policy to be more efficiently (lower sample complexity) and easily optimized with respect to the expected reward function due to the action no longer being a random variable which must be integrated over in the expectation
  		can be much more efficient in settings with very high-dimensional actions where sampling actions provides poor coverage of state-action space

  		http://jmlr.org/proceedings/papers/v32/silver14.html
  		http://arxiv.org/abs/1509.02971

  		http://videolectures.net/rldm2015_silver_reinforcement_learning/ (Silver, 1:07:23)
  		http://youtu.be/qLaDWKd61Ig?t=39m (Silver)
  		http://youtu.be/KHZVXao4qXs?t=52m58s (Silver)
  		https://youtu.be/mrgJ53TIcQc?t=1h3m2s (Seleznev, in russian)
  		https://youtu.be/rO7Dx8pSJQw?t=50m (Schulman)

  		in continuous multidimensional action space ∇aQπ(s,a) tells how to improve action
  		∇θEτ[R(τ)]=Eτ[∇θQπ(s0,a0) + ∇θQπ(s1,a1) + ...]

  		∇wL(w) = E s,a,r,s~D[(r+γQ(s',π(s'),w')-Q(s,a,w))∇wQ(s,a,w)]
  		∇uL(u) = E s,a,r,s~D[∇aQ(s,a,w)∇uπ(s,u)]
  		where w' is target Q-network to avoid oscillations

  		implementations:
  		 - https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
  		 - http://pemami4911.github.io/blog_posts/2016/08/21/ddpg-rl.html
  		 - https://github.com/rllab/rllab/blob/master/rllab/algos/ddpg.py
  		 - https://github.com/yandexdataschool/AgentNet/blob/master/agentnet/learning/dpg_n_step.py
  		 - https://github.com/iassael/torch-policy-gradient

  	Stochastic Value Gradient (SVG)
  		generalizes DPG to stochastic policies in a number of ways, giving a spectrum from model-based to model-free algorithms
  		while SVG(0) is a direct stochastic generalization of DPG, SVG(1) combines an actor, critic and dynamics model f
  		the actor is trained through a combination of gradients from the critic, model and reward simultaneously

  		reparametrization trick: E p(y|x)[g(y)]=∫g(f(x,ξ))ρ(ξ)dξ where y=f(x,ξ) and ξ~ρ(.) a fixed noise distribution

  		http://arxiv.org/abs/1510.09142

  		https://youtu.be/mrgJ53TIcQc?t=1h10m31s (Seleznev, in russian)
  		https://youtu.be/rO7Dx8pSJQw?t=50m (Schulman)


  * Guided Policy Search
	"Use modification of importance sampling to get policy gradient, where samples are obtained via trajectory optimization."

  	"Trains a policy for accomplishing a given task by guiding the learning with multiple guiding distributions.
  	Relies on learning an underlying dynamical model of the environment and then, at each iteration of the algorithm, using that model to gradually improve the policy."

  	"Suppose you want to train a neural net policy that can solve a fairly broad class of problems. Here’s one approach: (1) Sample 10 instances of the problem, and solve each of the instances using a problem-specific method, e.g. a method that fits and uses an instance-specific model. (2) Train the neural net to agree with all of the per-instance solutions. But if you’re going to do that, you might do even better by constraining the specific solutions and what the neural net policy would do to be close to each other from the start, fitting both simultaneously."

  	http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, part 2)
  	http://youtube.com/watch?v=EtMyH_--vnU (Levine)
  	https://video.seas.harvard.edu/media/ME+Sergey+Levine+2015+-04-01/1_gqqp9r3o/23375211 (Levine)
  	http://youtube.com/watch?v=xMHjkZBvnfU (Abbeel)

  	challenges:
  	 - much weaker supervision: cost for being in a state but current state is consequence of initial state, control, and noise (temporal credit assignment problem)
  	 - distribution over observed states determined by robot's own actions (need for exploration)
  
  	 - optimal trajectories are not sufficiently informative -> distribution over near-optimal trajectories
  	 - representational mismatch trajectory distribution vs neural net -> constrained guided policy search

  	implementations:
  	 - http://rll.berkeley.edu/gps/ + http://rll.berkeley.edu/gps/faq.html
  	 - https://github.com/nivwusquorum/guided-policy-search/




[model-based methods]

  planning (lookahead search for optimal actions) using transition model of environment p(r,s0|s,a)

  * deep reinforcement learning
	  challenges:
	   - compounding errors
		errors in the transition model compound over the trajectory
		by the end of a long trajectory rewards can be totally wrong
		no success yet in Atari
	   - deep networks of value/policy can plan implicitly
		each layer of network performs arbitrary computational step
		n-layer network can “lookahead” n steps
		are transition models required at all?

	"Deep AutoRegressive Networks" by Gregor et al. - https://youtu.be/-yX1SYeDHbg?t=49m25s + https://youtu.be/P78QYjWh5sM?t=20m50s
	"Deep Recurrent Q-Network" by Alexander Fritzler - https://youtube.com/watch?v=bE5DIJvZexc (in russian)
	"Deep Reinforcement Learning with Memory" by Sergey Bartunov - http://93.180.23.59/videos/video/2420/in/channel/1/ (in russian)


  * bayesian reinforcement learning
	BRL agents aim to maximise the expected collected rewards obtained when interacting with an unknown Markov Decision Process while using some prior knowledge.

	Belief-augmented MDP is an MDP obtained when considering augmented states made of the concatenation of the actual state and the posterior.

	Bayes-Adaptive Markov Decision Processes form a natural framework to deal with sequential decision-making problems when some of the information is hidden. In these problems, an agent navigates in an initially unknown environment and receives a numerical reward according to its actions. However, actions that yield the highest instant reward and actions that maximise the gathering of knowledge about the environment are often different. The BAMDP framework leads to a rigorous definition of an optimal solution to this learning problem, which is based on finding a policy that reaches an optimal balance between exploration and exploitation.

	introduction by Alexey Seleznev - https://youtube.com/watch?v=_dkaynuKUFE (in russian)




[bandits]

  "Removing the credit assignment problem from reinforcement learning yields the Contextual Bandit setting which is generically solvable in the same manner as common supervised learning problems."

  demo - http://iosband.github.io/2015/07/28/Beat-the-bandit.html

  introduction by David Silver
	http://youtube.com/watch?v=sGuiWX07sKw

  introduction by Ian Osband
	http://iosband.github.io/2015/07/19/Efficient-experimentation-and-multi-armed-bandits.html

  introduction by Jeremy Kun
	http://jeremykun.com/2013/10/28/optimism-in-the-face-of-uncertainty-the-ucb1-algorithm/
	http://jeremykun.com/2013/11/08/adversarial-bandits-and-the-exp3-algorithm/
	http://jeremykun.com/2013/12/09/bandits-and-stocks/

  overview and course by Csaba Szepesvari
	http://banditalgs.com/2016/09/04/bandits-a-new-beginning/
	http://banditalgs.com/2016/09/04/stochastic-bandits-warm-up/
	http://banditalgs.com/2016/09/14/first-steps-explore-then-commit/
	http://banditalgs.com/2016/09/18/the-upper-confidence-bound-algorithm/
	http://banditalgs.com/2016/09/22/optimality-concepts-and-information-theory/
	http://banditalgs.com/2016/09/28/more-information-theory-and-minimax-lower-bounds/
	http://banditalgs.com/2016/09/30/instance-dependent-lower-bounds/
	http://banditalgs.com/2016/10/01/adversarial-bandits/
	http://banditalgs.com/2016/10/14/high-probability-lower-bounds/
	http://banditalgs.com/2016/10/14/exp4/
	http://banditalgs.com/2016/10/19/stochastic-linear-bandits/
	http://banditalgs.com/2016/10/20/lower-bounds-for-stochastic-linear-bandits/
	http://banditalgs.com/2016/11/13/ellipsoidal-confidence-bounds-for-least-squares-estimators/
	http://banditalgs.com/2016/11/21/sparse-stochastic-linear-bandits/
	http://banditalgs.com/2016/11/24/adversarial-linear-bandits/

  tutorial by Csaba Szepesvari
	https://youtube.com/watch?v=VVcLnAoU9Gw
	https://youtube.com/watch?v=cknukHreMdI
	https://youtube.com/watch?v=ruIO79C2IQc

  "Bandit Theory" by Sebastien Bubeck - https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/ + https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/

  "Multiworld Testing Decision Service" (first general purpose reinforcement-based learning system) - http://hunch.net/?p=4464948 + http://research.microsoft.com/en-us/projects/mwt/

  papers - https://dropbox.com/sh/saxdz2c9ljr4ip2/AADS7U3FwHedmDe5whaRKqHPa




[benchmarks]

  - OpenAI Universe
	https://universe.openai.com
	https://openai.com/blog/universe/
  - OpenAI Gym
	https://gym.openai.com
	https://gym.openai.com/envs + https://gym.openai.com/envs#atari
	https://openai.com/blog/openai-gym-beta/
	http://arxiv.org/abs/1606.01540
  - DeepMind Lab
	https://github.com/deepmind/lab
	https://deepmind.com/blog/open-sourcing-deepmind-lab/
  - RLLab
	https://github.com/rllab/rllab
	"Benchmarking Deep Reinforcement Learning for Continuous Control" -
		http://arxiv.org/abs/1604.06778
		http://techtalks.tv/talks/benchmarking-deep-reinforcement-learning-for-continuous-control/62380/
  - Parallel Game Engine
	https://github.com/222464/PGE
  - Project Malmo (Minecraft)
	https://microsoft.com/en-us/research/project/project-malmo/
	https://youtube.com/watch?v=399qJUBRA0o




[frameworks]

   - https://github.com/5vision/deep-reinforcement-learning-networks

   - https://github.com/rllab/rllab (Berkeley)
	https://github.com/rllab/rllab/tree/master/rllab/algos
	https://rllab.readthedocs.org/en/latest/user/implement_env.html
	https://rllab.readthedocs.org/en/latest/user/implement_algo_basic.html
	https://rllab.readthedocs.org/en/latest/user/implement_algo_advanced.html

   - https://github.com/rll/deeprlhw2 (Berkeley RLL)
   - https://github.com/Kaixhin/rlenvs/
   - https://github.com/VinF/deer
   - https://github.com/yandexdataschool/AgentNet
   - https://github.com/matthiasplappert/keras-rl
   - https://github.com/ludc/rltorch

   - REINFORCEjs demos by Andrej Karpathy - http://cs.stanford.edu/people/karpathy/reinforcejs/index.html
	- dynamic programming for finite and small MDPs: tabular value functions, model of environment
	- tabular temporal difference learning (SARSA, Q-Learning, Eligibility Traces, Planning with priority sweeps): tabular value functions, no environment model, learning from experience
	- Deep Q-Learning (experience replay, TD error clamping for robustness): approximating action-value function with neural network, support for continuous spaces, discrete number of actions
	- policy gradients (REINFORCE, LSTMs): learning actor policy, learning value function, Determenistic Policy Gradients, continuous actions




[interesting quotes]

  (Juergen Schmidhuber) "Generally speaking, when it comes to Reinforcement Learning, it is indeed a good idea to train a recurrent neural network called M to become a predictive model of the world, and use M to train a separate controller network C which is supposed to generate reward-maximising action sequences. To my knowledge, the first such CM system with an RNN C and an RNN M dates back to 1990 (Schmidhuber). It builds on earlier work where C and M are feedforward NNs. M is used to compute a gradient for the parameters of C. So does this have anything to do with AGI? Yes, it does: Marcus Hutter’s mathematically optimal universal AIXI also has a predictive world model M, and a controller C that uses M to maximise expected reward. Ignoring limited storage size, RNNs are general computers just like your laptop. That is, AIXI’s M is related to the RNN-based M above in the sense that both consider a very general space of predictive programs. AIXI’s M, however, really looks at all those programs simultaneously, while the RNN-based M uses a limited local search method such as gradient descent in program space (also known as backprop through time) to find a single reasonable predictive program (an RNN weight matrix). AIXI’s C always picks the action that starts the action sequence that yields maximal predicted reward, given the current M, which in a Bayes-optimal way reflects all the observations so far. The RNN-based C, however, uses a local search method (backprop through time) to optimise its program or weight matrix, using gradients derived from M. So in a way, the old RNN-based CM system of 1990 may be viewed as a limited, downscaled, sub-optimal, but at least computationally feasible approximation of AIXI."

  (Yann LeCun) "Like many conceptual ideas about AI, AIXI is completely impractical. I think if it were true that P=NP or if we had no limitations on memory and computation, AI would be a piece of cake. We could just brute-force any problem. We could go "full Bayesian" on everything (no need for learning anymore. Everything becomes Bayesian marginalization). But the world is what it is."

  (Christian Szegedy) "The inroads of machine learning will transform all of information technologies. Most prominently, the way we program our computers will slowly shift from prescribing how to solve problems to just specifying them and let machines learn to cope with them. We could even have them distill their solution to formal procedures akin to our current programs. In order to truly get there, the most exciting developments will come from the synergy of currently disjoint areas: the marriage of formal, discrete methods and fuzzy, probabilistic approaches, like deep neural networks."

  (David Silver) "reinforcement learning + deep learning = AI"

  (Rich Sutton) "If you want to make a long-lasting impact in AI, focus on generality and most importantly scalability in your models and research, as Moore's law will eventually render all other research, models, and algorithms completely obsolete."

  () "Stuart Russel is interested in RL agents that don't receive rewards directly but instead have Bayesian priors over their reward function, which they update by observing their environment (for example, a domestic robot might 'learn' about its reward function based on feedback from its human owner). This should lead to exploration-exploitation tradeoffs where the agent acts, at first, to learn more about what its reward function really is (observing its owner's behavior without interference, or even explicitly asking questions), so that it can better optimize it. This is not part of the traditional RL framework, but it's mathematically well defined: you could try to build such an agent, right now, in an appropriate toy environment (gridworld, etc.). What sort of issues come up? What priors are appropriate? What sort of behavior emerges?"

  (Nando de Freitas) "RL is a useful learning strategy, and work by Peter Dayan and colleagues indicates that it may also play a role in how some animals behave. Is a scalar reward enough? Hmmm, I don't know. Certainly for most supervised learning - e.g. think ImageNet, there is a single scalar reward. Note that the reward happens at every time step - i.e. it is very informative for ImageNet. Most of what people dub as unsupervised learning can also be cast as reinforcement learning. RL is a very general and broad framework, with huge variation depending on whether the reward is rare, whether we have mathematical expressions for the reward function, whether actions are continuous or discrete, etc. etc. Don't think of RL as a single thing. I feel many criticisms of RL fail because of narrow thinking about RL."




selected papers/books - https://dropbox.com/sh/zc5qxqksgqmxs0a/AAA4C1y_6Y0-3dm3gPuQhb_va

reinforcement learning papers - https://github.com/aikorea/awesome-rl
deep reinforcement learning papers - https://github.com/junhyukoh/deep-reinforcement-learning-papers + https://github.com/muupan/deep-reinforcement-learning-papers


interesting papers (see below):
 * applications
 * exploration
 * state-action space and planning
 * deep reinforcement learning
 * deep policy search
 * inverse reinforcement learning




[interesting papers - applications]

Silver, Huang, Maddison, Guez, Sifre, Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman, Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel, Hassabis - "Mastering the Game of Go with Deep Neural Networks and Tree Search" [https://vk.com/doc-44016343_437229031?hash=9c999a03ee82850948&dl=56ce06e325d42fbc72] (Go)
	"The game of Go has long been viewed as the most challenging of classic games for artificial intelligence due to its enormous search space and the difficulty of evaluating board positions and moves. We introduce a new approach to computer Go that uses value networks to evaluate board positions and policy networks to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte-Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte-Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away."
	--
	"Google AlphaGo is a historical tour of AI ideas: 70s (Alpha-Beta), 80s/90s (RL & self-play), 00's (Monte-Carlo), 10's (deep neural networks)." [http://youtube.com/watch?v=UMm0XaCFTJQ]
	"The most important application of RL here is to learn a value function which aims to predict with which probability a certain position will lead to winning the game. The learned expert moves are already good, but the network that produces them did not learn with the objective to win the game, but only to minimize the differences to the teacher values in the training data set."
	(Raia Hadsell) "This points to one of the largest differences between human learning and modern machine learning. Deep networks, such as AlphaGo's policy and value nets, learn with lots of data and are generalists. They do not retain and refer back to individual examples, nor can they learn meaningfully from single examples. Moreover, if trained on data from a changing distribution, they will forget previous skills, quickly and catastrophically. Researchers at DeepMind and elsewhere are developing methods for continual learning, memory, and one-shot learning that scale to large deep networks."
	-- http://youtube.com/watch?v=4D5yGiYe8p4 (Silver)
	-- http://youtube.com/watch?v=LX8Knl0g0LE (Huang)
	-- http://youtube.com/watch?v=UMm0XaCFTJQ (Sutton, Szepesvari, Bowling, Hayward, Muller, history of ideas)
	-- https://github.com/Rochester-NRT/RocAlphaGo/wiki (overview)
	-- https://github.com/Rochester-NRT/AlphaGo/

Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, Riedmiller - "Playing Atari with Deep Reinforcement Learning" [http://www.cs.toronto.edu/~vmnih/docs/dqn.pdf] (Atari videogames)
	"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
	-- http://youtube.com/watch?v=EfGD2qveGdQ (demo)
	-- http://videolectures.net/rldm2015_silver_reinforcement_learning/ (demos, 57:56, 1:06:04, 1:15:15)
	-- http://youtu.be/XAbLn66iHcQ?t=1h41m21s + http://youtube.com/watch?v=0xo1Ldx3L5Q (3D racing demo)
	-- http://youtube.com/watch?v=nMR5mjCFZCw (3D labyrinth demo)
	-- http://youtube.com/watch?v=re6hkcTWVUY (Doom gameplay demo)
	-- https://youtube.com/watch?v=6jlaBD9LCnM + https://youtube.com/watch?v=6JT6_dRcKAw (blockworld demo)
	-- http://youtu.be/XAbLn66iHcQ?t=1h41m21s (demo)
	-- http://youtube.com/user/eldubro/videos (demos)
	-- http://youtube.com/watch?v=iqXKQf2BOSE (demo)
	-- http://youtube.com/watch?v=lge-dl2JUAM + https://youtube.com/watch?v=xN1d3qHMIEQ (interviews and demos)
	-- http://sodeepdude.blogspot.ru/2015/03/deepminds-atari-paper-replicated.html (demos)
	-- http://videolectures.net/nipsworkshops2013_mnih_atari/ (Volodymyr Mnih)
	-- http://youtube.com/watch?v=xzM7eI7caRk (Volodymyr Mnih)
	-- http://youtube.com/watch?v=dV80NAlEins (Nando de Freitas)
	-- http://youtube.com/watch?v=HUmEbUkeQHg (Nando de Freitas)
	-- http://youtube.com/watch?v=mrgJ53TIcQc (Pavlov, in russian)
	-- https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner + https://sites.google.com/a/deepmind.com/dqn/
	-- https://github.com/carpedm20/deep-rl-tensorflow
	-- https://github.com/Kaixhin/Atari
	-- https://github.com/tambetm/simple_dqn + http://youtu.be/KkIf0Ok5GCE + http://youtu.be/0ZlgrQS3krg
	-- https://github.com/spragunr/deep_q_rl + http://sodeepdude.blogspot.ru/2015/03/deepminds-atari-paper-replicated.html
	-- https://github.com/ugo-nama-kun/DQN-chainer + http://youtube.com/watch?v=N813o-Xb6S8
	-- https://github.com/muupan/dqn-in-the-caffe
	-- https://github.com/asrivat1/DeepLearningVideoGames
	-- https://github.com/Jabberwockyll/deep_rl_ale
	-- https://github.com/VinF/deer
	-- https://github.com/sherjilozair/dqn
	-- https://github.com/gliese581gg/DQN_tensorflow
	-- https://github.com/devsisters/DQN-tensorflow
	-- https://github.com/osh/kerlym
	-- https://github.com/ludc/rltorch/blob/master/torch/policies/DeepQPolicy.lua
	-- http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html

Lample, Chaplot - "Playing FPS Games with Deep Reinforcement Learning" [https://arxiv.org/abs/1609.05521]
	"Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios."
	"We introduced a method to augment a DRQN model with high-level game information, and modularized our architecture to incorporate independent networks responsible for different phases of the game. These methods lead to dramatic improvements over the standard DRQN model when applied to complicated tasks like a deathmatch. We showed that the proposed model is able to outperform built-in bots as well as human players and demonstrated the generalizability of our model to unknown maps."
	-- https://youtube.com/playlist?list=PLduGZax9wmiHg-XPFSgqGg8PEAV51q1FT (demo)

Lai - "Giraffe: Using Deep Reinforcement Learning to Play Chess" [http://arxiv.org/abs/1509.01549] (Chess)
	"This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter tuning on hand-crafted evaluation functions, Giraffe’s learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess. We also investigated the possibility of using probability thresholds instead of depth to shape search trees. Depth-based searches form the backbone of virtually all chess engines in existence today, and is an algorithm that has become well-established over the past half century. Preliminary comparisons between a basic implementation of probability-based search and a basic implementation of depth-based search showed that our new probability-based approach performs moderately better than the established approach. There are also evidences suggesting that many successful ad-hoc add-ons to depth-based searches are generalized by switching to a probability-based search. We believe the probability-based search to be a more fundamentally correct way to perform minimax. Finally, we designed another machine learning system to shape search trees within the probability-based search framework. Given any position, this system estimates the probability of each of the moves being the best move without looking ahead. The system is highly effective - the actual best move is within the top 3 ranked moves 70% of the time, out of an average of approximately 35 legal moves from each position. This also resulted in a significant increase in playing strength. With the move evaluator guiding a probability-based search using the learned evaluator, Giraffe plays at approximately the level of an FIDE International Master (top 2.2% of tournament chess players with an official rating)."
	"In this project, we investigated the use of deep reinforcement learning with automatic feature extraction in the game of chess. The results show that the learned system performs at least comparably to the best expert-designed counterparts in existence today, many of which have been fine tuned over the course of decades. The beauty of this approach is in its generality. While it was not explored in this project due to time constraint, it is likely that this approach can easily be ported to other zero-sum turn-based board games, and achieve state-of-art performance quickly, especially in games where there has not been decades of intense research into creating a strong AI player. In addition to the machine learning aspects of the project, we introduced and tested an alternative variant of the decades-old minimax algorithm, where we apply probability boundaries instead of depth boundaries to limit the search tree. We showed that this approach is at least comparable and quite possibly superior to the approach that has been in use for the past half century. We also showed that this formulation of minimax works especially well with our probability-based machine learning approach. Efficiency is always a major consideration when switching from an expert system to a machine learning approach, since expert systems are usually more efficient to evaluate than generic models. This is especially important for applications like a chess engine, where being able to search nodes quickly is strongly correlated with playing strength. Some earlier attempts at applying neural network to chess have been thwarted by large performance penalties. Giraffe’s optimized implementation of neural network, when combined with the much higher vector arithmetics throughput of modern processors and effective caching, allows it to search at a speed that is less than 1 order of magnitude slower than the best modern chess engines, thus making it quite competitive against many chess engines in gameplay without need for time handicap. With all our enhancements, Giraffe is able to play at the level of an FIDE International Master on a modern mainstream PC. While that is still a long way away from the top engines today that play at super-Grandmaster levels, it is able to defeat many lower-tier engines, most of which search an order of magnitude faster. One of the original goals of the project is to create a chess engine that is less reliant on brute-force than its contemporaries, and that goal has certainly been achieved. Unlike most chess engines in existence today, Giraffe derives its playing strength not from being able to see very far ahead, but from being able to evaluate tricky positions accurately, and understanding complicated positional concepts that are intuitive to humans, but have been elusive to chess engines for a long time. This is especially important in the opening and end game phases, where it plays exceptionally well."
	"It is clear that Giraffe’s evaluation function has at least comparable positional understanding compared to evaluation functions of top engines in the world, which is remarkable because their evaluation functions are all carefully hand-designed behemoths with hundreds of parameters that have been tuned both manually and automatically over several years, and many of them have been worked on by human grandmasters. The test suite likely under-estimates the positional understanding of Giraffe compared to other engines, because most of the themes tested by the test suite are generally well-understood concepts in computer chess that are implemented by many engines, and since the test suite is famous, it is likely that at least some of the engines have been tuned specifically against the test suite. Since Giraffe discovered all the evaluation features through self-play, it is likely that it knows about patterns that have not yet been studied by humans, and hence not included in the test suite. As far as we are aware, this is the first successful attempt at using machine learning to create a chess evaluation function from self-play, including automatic feature extraction (many previous attempts are weight-tuning for hand-designed features), starting from minimal hand-coded knowledge, and achieving comparable performance to state-of-the-art expert-designed evaluation functions."
	-- https://bitbucket.org/waterreaction/giraffe

Veness, Silver, Uther, Blair - "Bootstrapping from Game Tree Search" [http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications_files/bootstrapping.pdf] (Chess)
	"In this paper we introduce a new algorithm for updating the parameters of a heuristic evaluation function, by updating the heuristic towards the values computed by an alpha-beta search. Our algorithm differs from previous approaches to learning from search, such as Samuel’s checkers player and the TD-Leaf algorithm, in two key ways. First, we update all nodes in the search tree, rather than a single node. Second, we use the outcome of a deep search, instead of the outcome of a subsequent search, as the training signal for the evaluation function. We implemented our algorithm in a chess program Meep, using a linear heuristic function. After initialising its weight vector to small random values, Meep was able to learn high quality weights from self-play alone. When tested online against human opponents, Meep played at a master level, the best performance of any chess program with a heuristic learned entirely from self-play."
	-- http://videolectures.net/nips09_veness_bfg/

Heinrich, Silver - "Smooth UCT Search in Computer Poker" [http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/smooth_uct.pdf] (Poker)
	"Self-play Monte Carlo Tree Search has been successful in many perfect-information two-player games. Although these methods have been extended to imperfect-information games, so far they have not achieved the same level of practical success or theoretical convergence guarantees as competing methods. In this paper we introduce Smooth UCT, a variant of the established Upper Confidence Bounds Applied to Trees algorithm. Smooth UCT agents mix in their average policy during self-play and the resulting planning process resembles game-theoretic fictitious play. When applied to Kuhn and Leduc poker, Smooth UCT approached a Nash equilibrium, whereas UCT diverged. In addition, Smooth UCT outperformed UCT in Limit Texas Hold’em and won 3 silver medals in the 2014 Annual Computer Poker Competition."

Heinrich, Silver - "Deep Reinforcement Learning from Self-Play in Imperfect-Information Games" [http://arxiv.org/abs/1603.01121] (Poker)
	"Many real-world applications can be described as large-scale games of imperfect information. To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain. In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without any prior knowledge. Our method combines fictitious self-play with deep reinforcement learning. When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged. In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a competitive strategy that approached the performance of human experts and state-of-the-art methods."
	"We have introduced NFSP, the first end-to-end deep reinforcement learning approach to learning approximate Nash equilibria of imperfect-information games from self-play. NFSP addresses three problems. Firstly, NFSP agents learn without prior knowledge. Secondly, they do not rely on local search at runtime. Thirdly, they converge to approximate Nash equilibria in self-play. Our empirical results provide the following insights. The performance of fictitious play degrades gracefully with various approximation errors. NFSP converges reliably to approximate Nash equilibria in a small poker game, whereas DQN’s greedy and average strategies do not. NFSP learned a competitive strategy in a real-world scale imperfect-information game from scratch without using explicit prior knowledge. In this work, we focussed on imperfect-information two-player zero-sum games. Fictitious play, however, is also guaranteed to converge to Nash equilibria in cooperative, potential games. It is therefore conceivable that NFSP can be successfully applied to these games as well. Furthermore, recent developments in continuous-action reinforcement learning (Lillicrap et al., 2015) could enable NFSP to be applied to continuous-action games, which current game-theoretic methods cannot deal with directly."
	-- http://techtalks.tv/talks/deep-reinforcement-learning/62360/ (Silver, 1:05:00)

Yakovenko, Cao, Raffel, Fan - "Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games Using Convolutional Networks" [http://colinraffel.com/publications/aaai2016poker.pdf]
	"Poker is a family of card games that includes many variations. We hypothesize that most poker games can be solved as a pattern matching problem, and propose creating a strong poker playing system based on a unified poker representation. Our poker player learns through iterative self-play, and improves its understanding of the game by training on the results of its previous actions without sophisticated domain knowledge. We evaluate our system on three poker games: single player video poker, two-player Limit Texas Hold’em, and finally two-player 2-7 triple draw poker. We show that our model can quickly learn patterns in these very different poker games while it improves from zero knowledge to a competitive player against human experts. The contributions of this paper include: (1) a novel representation for poker games, extendable to different poker variations, (2) a Convolutional Neural Network (CNN) based learning model that can effectively learn the patterns in three different games, and (3) a self-trained system that significantly beats the heuristic-based program on which it is trained, and our system is competitive against human expert players."
	-- https://github.com/moscow25/deep_draw

Moravcik, Schmid, Burch, Lisy, Morrill, Bard, Davis, Waugh, Johanson, Bowling - "DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker" [http://arxiv.org/abs/1701.01724]
	"Artificial intelligence has seen a number of breakthroughs in recent years, with games often serving as significant milestones. A common feature of games with these successes is that they involve information symmetry among the players, where all players have identical information. This property of perfect information, though, is far more common in games than in real-world problems. Poker is the quintessential game of imperfect information, and it has been a longstanding challenge problem in artificial intelligence. In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition about arbitrary poker situations that is automatically learned from selfplay games using deep learning. In a study involving dozens of participants and 44,000 hands of poker, DeepStack becomes the first computer program to beat professional poker players in heads-up no-limit Texas hold’em. Furthermore, we show this approach dramatically reduces worst-case exploitability compared to the abstraction paradigm that has been favored for over a decade."
	"DeepStack is the first computer program to defeat professional poker players at heads-up nolimit Texas Hold’em, an imperfect information game with 10160 decision points. Notably it achieves this goal with almost no domain knowledge or training from expert human games. The implications go beyond just being a significant milestone for artificial intelligence. DeepStack is a paradigmatic shift in approximating solutions to large, sequential imperfect information games. Abstraction and offline computation of complete strategies has been the dominant approach for almost 20 years. DeepStack allows computation to be focused on specific situations that arise when making decisions and the use of automatically trained value functions. These are two of the core principles that have powered successes in perfect information games, albeit conceptually simpler to implement in those settings. As a result, for the first time the gap between the largest perfect and imperfect information games to have been mastered is mostly closed. As “real life consists of bluffing... deception... asking yourself what is the other man going to think”, DeepStack also has implications for seeing powerful AI applied more in settings that do not fit the perfect information assumption. The old paradigm for handling imperfect information has shown promise in applications like defending strategic resources and robust decision making as needed for medical treatment recommendations. The new paradigm will hopefully open up many more possibilities."

Yeh, Lin - "Automatic Bridge Bidding Using Deep Reinforcement Learning" [http://arxiv.org/abs/1607.03290]
	"Bridge is among the zero-sum games for which artificial intelligence has not yet outperformed expert human players. The main difficulty lies in the bidding phase of bridge, which requires cooperative decision making under partial information. Existing artificial intelligence systems for bridge bidding rely on and are thus restricted by human-designed bidding systems or features. In this work, we propose a pioneering bridge bidding system without the aid of human domain knowledge. The system is based on a novel deep reinforcement learning model, which extracts sophisticated features and learns to bid automatically based on raw card data. The model includes an upper-confidence-bound algorithm and additional techniques to achieve a balance between exploration and exploitation. Our experiments validate the promising performance of our proposed model. In particular, the model advances from having no knowledge about bidding to achieving superior performance when compared with a champion-winning computer bridge program that implements a human-designed bidding system. To the best of our knowledge, our proposed model is the first to tackle automatic bridge bidding from raw data without additional human knowledge."

Jaskowski - "Mastering 2048 with Delayed Temporal Coherence Learning Multi-State Weight Promotion, Redundant Encoding and Carousel Shaping" [https://arxiv.org/abs/1604.05085]
	"2048 is an engaging single-player, nondeterministic video puzzle game, which, thanks to the simple rules and hard-to-master gameplay, has gained massive popularity in recent years. As 2048 can be conveniently embedded into the discrete-state Markov decision processes framework, we treat it as a testbed for evaluating existing and new methods in reinforcement learning. With the aim to develop a strong 2048 playing program, we employ temporal difference learning with systematic n-tuple networks. We show that this basic method can be significantly improved with temporal coherence learning, multi-stage function approximator with weight promotion, carousel shaping and redundant encoding. In addition, we demonstrate how to take advantage of the characteristics of the n-tuple network, to improve the algorithmic effectiveness of the learning process by i) delaying the (decayed) update and applying lock-free optimistic parallelism to effortlessly make advantage of multiple CPU cores. This way, we were able to develop the best known 2048 playing program to date, which confirms the effectiveness of the introduced methods for discrete-state Markov decision problems."
	-- https://github.com/aszczepanski/2048

Narasimhan, Kulkarni, Barzilay - "Language Understanding for Text-based Games using Deep Reinforcement Learning" [http://arxiv.org/abs/1506.08941] (language understanding)
	"In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations."
	"In contrast to the above work, our model combines text interpretation and strategy learning in a single framework. As a result, textual analysis is guided by the received control feedback, and the learned strategy directly builds on the text interpretation."
	"We address the task of end-to-end learning of control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language variability makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. Our experiments demonstrate the importance of learning good representations of text in order to play these games well. Future directions include tackling high-level planning and strategy learning to improve the performance of intelligent agents."

Cuayahuitl, Keizer, Lemon - "Strategic Dialogue Management via Deep Reinforcement Learning" [http://arxiv.org/abs/1511.08099] (dialog management)
	"Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan - where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (‘bots’), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities."
	"The contribution of this paper is the first application of Deep Reinforcement Learning to optimising the behaviour of strategic conversational agents. Our learning agents are able to: (i) discover what trading negotiations to offer, (ii) discover when to accept, reject, or counteroffer; (iii) discover strategic behaviours based on constrained action sets - i.e. action selection from legal actions rather than from all of them; and (iv) learn highly competitive behaviour against different types of opponents. All of this is supported by a comprehensive evaluation of three DRL agents trained against three baselines (random, heuristic and supervised), which are analysed from a crossevaluation perspective. Our experimental results report that all DRL agents substantially outperform all the baseline agents. Our results are evidence to argue that DRL is a promising framework for training the behaviour of complex strategic interactive agents. Future work can for example carry out similar evaluations as above in other strategic environments, and can also extend the abilities of the agents with other strategic features and forms of learning. In addition, a comparison of different model architectures, training parameters and reward functions can be explored in future work. Last but not least, given that our learning agents trade at the semantic level, they can be extended with language understanding/generation abilities to communicate verbally."

Ranzato, Chopra, Auli, Zaremba - "Sequence Level Training with Recurrent Neural Networks" [http://arxiv.org/abs/1511.06732]
	"Many natural language processing applications use language models to generate text. These models are typically trained to predict the next word in a sequence, given the previous words and some context such as an image. However, at test time the model is expected to generate the entire sequence from scratch. This discrepancy makes generation brittle, as errors may accumulate along the way. We address this issue by proposing a novel sequence level training algorithm that directly optimizes the metric used at test time, such as BLEU or ROUGE. On three different tasks, our approach outperforms several strong baselines for greedy generation. The method is also competitive when these baselines employ beam search, while being several times faster."
	"A wide variety of applications rely on text generation, including machine translation, video/text summarization, question answering, among others. From a machine learning perspective, text generation is the problem of predicting a syntactically and semantically correct sequence of consecutive words given some context. For instance, given an image, generate an appropriate caption or given a sentence in English language, translate it into French. Popular choices for text generation models are language models based on n-grams, feed-forward neural networks and recurrent neural networks. These models when used as is to generate text suffer from two major drawbacks. First, they are trained to predict the next word given the previous ground truth words as input. However, at test time, the resulting models are used to generate an entire sequence by predicting one word at a time, and by feeding the generated word back as input at the next time step. This process is very brittle because the model was trained on a different distribution of inputs, namely, words drawn from the data distribution, as opposed to words drawn from the model distribution. As a result the errors made along the way will quickly accumulate. We refer to this discrepancy as exposure bias which occurs when a model is only exposed to the training data distribution, instead of its own predictions. Second, the loss function used to train these models is at the word level. A popular choice is the cross-entropy loss used to maximize the probability of the next correct word. However, the performance of these models is typically evaluated using discrete metrics. One such metric is called BLEU for instance, which measures the n-gram overlap between the model generation and the reference text. Training these models to directly optimize metrics like BLEU is hard because a) these are not differentiable, and b) combinatorial optimization is required to determine which sub-string maximizes them given some context."
	"This paper proposes a novel training algorithm which results in improved text generation compared to standard models. The algorithm addresses the two issues discussed above as follows. First, while training the generative model we avoid the exposure bias by using model predictions at training time. Second, we directly optimize for our final evaluation metric. We build on the REINFORCE algorithm to achieve the above two objectives. While sampling from the model during training is quite a natural step for the REINFORCE algorithm, optimizing directly for any test metric can also be achieved by it. REINFORCE side steps the issues associated with the discrete nature of the optimization by not requiring rewards (or losses) to be differentiable. While REINFORCE appears to be well suited to tackle the text generation problem, it suffers from a significant issue. The problem setting of text generation has a very large action space which makes it extremely difficult to learn with an initial random policy. Specifically, the search space for text generation is of size O(WT), where W is the number of words in the vocabulary (typically around 10^4 or more) and T is the length of the sentence (typically around 10-30). Towards that end, we introduce Mixed Incremental Cross-Entropy Reinforce. MIXER is an easy-to-implement recipe to make REINFORCE work well for text generation applications. It is based on two key ideas: incremental learning and the use of a hybrid loss function which combines both REINFORCE and cross-entropy. Both ingredients are essential to training with large action spaces. In MIXER, the model starts from the optimal policy given by cross-entropy training (as opposed to a random one), from which it then slowly deviates, in order to make use of its own predictions, as is done at test time."
	"Our results show that MIXER with a simple greedy search achieves much better accuracy compared to the baselines on Text Summarization, Machine Translation and Image Captioning tasks. In addition we show that MIXER with greedy search is even more accurate than the cross entropy model augmented with beam search at inference time as a post-processing step. This is particularly remarkable because MIXER with greedy search is at least 10 times faster than the cross entropy model with a beam of size 10. Lastly, we note that MIXER and beam search are complementary to each other and can be combined to further improve performance, although the extent of the improvement is task dependent."
	-- https://www.evernote.com/shard/s189/sh/ada01a82-70a9-48d4-985c-20492ab91e84/8da92be19e704996dc2b929473abed46 (Larochelle)
	-- https://github.com/facebookresearch/MIXER

Norouzi, Bengio, Chen, Jaitly, Schuster, Wu, Schuurmans - "Reward Augmented Maximum Likelihood for Neural Structured Prediction" [https://arxiv.org/abs/1609.00150]
	"A key problem in structured output prediction is direct optimization of the task reward function that matters for test evaluation. This paper presents a simple and computationally efficient approach to incorporate task reward into a maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in the vicinity of the optimal solution. We show that optimal regularized expected reward is achieved when the conditional distribution of the outputs given the inputs is proportional to their exponentiated (temperature adjusted) rewards. Based on this observation, we optimize conditional log-probability of edited outputs that are sampled proportionally to their scaled exponentiated reward. We apply this framework to optimize edit distance in the output label space. Experiments on speech recognition and machine translation for neural sequence to sequence models show notable improvements over a maximum likelihood baseline by using edit distance augmented maximum likelihood."
	"Neural sequence models use a maximum likelihood framework to maximize the conditional probability of the ground-truth outputs given corresponding inputs. These models do not explicitly consider the task reward during training, hoping that conditional log-likelihood would serve as a good surrogate for the task reward. Such methods make no distinction between alternative incorrect outputs: log-probability is only measured on the ground-truth input-output pairs, and all alternative outputs are equally penalized, whether near or far from the ground-truth target. We believe that one can improve upon maximum likelihood sequence models, if the difference in the rewards of alternative outputs is taken into account. A key property of ML training for locally normalized RNN models is that the objective function factorizes into individual loss terms, which could be efficiently optimized using stochastic gradient descend. In particular, ML training does not require any form of inference or sampling from the model during training, which leads to computationally efficient and easy to implementations."
	"Alternatively, one can use reinforcement learning algorithms, such as policy gradient, to optimize expected task reward during training. Even though expected task reward seems like a natural objective, direct policy optimization faces significant challenges: unlike ML, the gradient for a mini-batch of training examples is extremely noisy and has a high variance; gradients need to be estimated via sampling from the model, which is a non-stationary distribution; the reward is often sparse in a high-dimensional output space, which makes it difficult to find any high value predictions, preventing learning from getting off the ground; and, finally, maximizing reward does not explicitly consider the supervised labels, which seems inefficient. In fact, all previous attempts at direct policy optimization for structured output prediction has started by bootstrapping from a previously trained ML solution and they use several heuristics and tricks to make learning stable."
	"This paper presents a new approach to task reward optimization that combines the computational efficiency and simplicity of ML with the conceptual advantages of expected reward maximization. Our algorithm called reward augmented maximum likelihood simply adds a sampling step on top of the typical likelihood objective. Instead of optimizing conditional log-likelihood on training input-output pairs, given each training input, we first sample an output proportional to its exponentiated scaled reward. Then, we optimize log-likelihood on such auxiliary output samples given corresponding inputs. When the reward for an output is defined as its similarity to a ground-truth output, then the output sampling distribution is peaked at the ground-truth output, and its concentration is controlled by a temperature hyper-parameter."
	"Surprisingly, we find that the best performance is achieved with output sampling distributions that put a lot of the weight away from the ground-truth outputs. In fact, in our experiments, the training algorithm rarely sees the original unperturbed outputs. Our results give further evidence that models trained with imperfect outputs and their reward values can improve upon models that are only exposed to a single ground-truth output per input."
	"There are several critical differences between gradient estimators for RML loss (reward augmented maximum likelihood) and RL loss (regularized expected reward) that make SGD optimization of RML loss more desirable. First, for RML loss, one has to sample from a stationary distribution, the so called exponentiated payoff distribution, whereas for RL loss one has to sample from the model distribution as it is evolving. Not only sampling from the model could slow down training, but also one needs to employ several tricks to get a better estimate of the gradient of RL loss. Further, the reward is often sparse in a high-dimensional output space, which makes finding any reasonable predictions challenging, when RL loss is used to refine a randomly initialized model. Thus, smart model initialization is needed. By contrast, we initialize the models randomly and refine them using RML loss."
	--
	"This reads as another way to use a world model to mitigate the sample complexity of reinforcement learning (e.g., what if edit distance was just the initial model of the reward?)."
	--
	"Andrej Karpathy provided another perspective: We can also view the process of optimizing LRML as distilling the exponentiated payoff distribution q(y|y*;τ) into the model pθ(y|x). The objective reaches a maximum when these two distributions are equivalent. From this distillation view, the question is clear: how can we distill more complex objects into pθ? Concretely, this means we should develop more complex reward distributions q to use in this setup. We have seen two examples so far: the exponentiated payoff from the paper and the label smoothing example of the previous section. We could define q to be a complex pre-trained model or a mixture of experts, and use this training process to distill them into a single model pθ. We just need to make sure that we can efficiently sample from the q we select."
	--
	"Alec Radford mentioned that the data augmentation suggested in the paper sounds similar in spirit to virtual adversarial training, where the current model is encouraged to make robust predictions not only for the examples in the training set but also for inputs “nearby” those that exist in the training set. A high-level comparison:
	  - Adversarial training can be seen as data-augmentation in the input space X. The RML objective does data-augmentation in the output space Y.
	  - Adversarial training performs model-based data augmentation: the examples generated are those for which the current model is maximally vulnerable. RML training performs data-based augmentation: the examples generated have outputs that are “near” the ground-truth outputs. (Here 'near' is defined by the reward function.)"
	-- http://youtube.com/watch?v=agA-rc71Uec (Samy Bengio)
	-- http://drive.google.com/file/d/0B3Rdm_P3VbRDVUQ4SVBRYW82dU0 (Gauthier)
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1609.00150

Microsoft Research - "A Multiworld Testing Decision Service" [http://arxiv.org/abs/1606.03966]
	"Applications and systems are constantly faced with decisions to make, often using a policy to pick from a set of actions based on some contextual information. We create a service that uses machine learning to accomplish this goal. The service uses exploration, logging, and online learning to create a counterfactually sound system supporting a full data lifecycle. The system is general: it works for any discrete choices, with respect to any reward metric, and can work with many learning algorithms and feature representations. The service has a simple API, and was designed to be modular and reproducible to ease deployment and debugging, respectively. We demonstrate how these properties enable learning systems that are robust and safe. Our evaluation shows that the Decision Service makes decisions in real time and incorporates new data quickly into learned policies. A large-scale deployment for a personalized news website has been handling all traffic since Jan. 2016, resulting in a 25% relative lift in clicks. By making the Decision Service externally available, we hope to make optimal decision making available to all."
	"We have presented the Decision Service: a powerful tool to support the complete data lifecycle, which automates many of the burdensome tasks that data scientists face such as gathering the right data and deploying in an appropriate manner. Instead, a data scientist can focus on more core tasks such as finding the right features, representation, or signal to optimize against. The data lifecycle support also makes basic application of the Decision Service feasible without a data scientist. To assist in lowering the barrier to entry, we are exploring techniques based on expert learning and hyperparameter search that may further automate the process. Since the policy evaluation techniques can provide accurate predictions of online performance, such automations are guaranteed to be statistically sound. We are also focusing on making the decision service easy to deploy and use because we believe this is key to goal of democratizing machine learning for everyone. The Decision Service can also naturally be extended to a greater variety of problems, all of which can benefit from data lifecycle support. Plausible extensions might address advanced variants like reinforcement and active learning, and simpler ones like supervised learning."
	--
	"It is the first general purpose reinforcement-based learning system. Wouldn’t it be great if Reinforcement Learning algorithms could easily be used to solve all reinforcement learning problems? But there is a well-known problem: It’s very easy to create natural RL problems for which all standard RL algorithms (epsilon-greedy Q-learning, SARSA, etc) fail catastrophically. That’s a serious limitation which both inspires research and which I suspect many people need to learn the hard way. Removing the credit assignment problem from reinforcement learning yields the Contextual Bandit setting which we know is generically solvable in the same manner as common supervised learning problems."
	"Many people have tried to create online learning system that do not take into account the biasing effects of decisions. These fail near-universally. For example they might be very good at predicting what was shown (and hence clicked on) rather that what should be shown to generate the most interest."
	"We need a system that explores over appropriate choices with logging of features, actions, probabilities of actions, and outcomes. These must then be fed into an appropriate learning algorithm which trains a policy and then deploys the policy at the point of decision. The system enables a fully automatic causally sound learning loop for contextual control of a small number of actions. It is strongly scalable, for example a version of this is in use for personalized news on MSN."
	-- http://hunch.net/?p=4464948
	-- http://research.microsoft.com/en-us/projects/mwt/
	-- https://mwtds.azurewebsites.net

Peng, Berseth, van de Panne - "Terrain-Adaptive Locomotion Skills Using Deep Reinforcement Learning" [http://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/2016-TOG-deepRL.pdf]
	"Reinforcement learning offers a promising methodology for developing skills for simulated characters, but typically requires working with sparse hand-crafted features. Building on recent progress in deep reinforcement learning, we introduce a mixture of actor-critic experts approach that learns terrain-adaptive dynamic locomotion skills using high-dimensional state and terrain descriptions as input, and parameterized leaps or steps as output actions. MACE learns more quickly than a single actor-critic approach and results in actor-critic experts that exhibit specialization. Additional elements of our solution that contribute towards efficient learning include Boltzmann exploration and the use of initial actor biases to encourage specialization. Results are demonstrated for multiple planar characters and terrain classes."
	"We introduce a novel mixture of actor-critic experts architecture to enable accelerated learning. MACE develops n individual control policies and their associated value functions, which each then specialize in particular regimes of the overall motion. During final policy execution, the policy associated with the highest value function is executed, in a fashion analogous to Q-learning with discrete actions. We show the benefits of Boltzmann exploration and various algorithmic features for our problem domain."
	-- https://youtube.com/watch?v=KPfzRSBzNX4 + https://youtube.com/watch?v=A0BmHoujP9k (demo)
	-- https://youtube.com/watch?v=mazfn4dHPRM + https://youtube.com/watch?v=RTuSHI5FNzg (overview)
	-- https://github.com/xbpeng/DeepTerrainRL

Veness, Ng, Hutter, Uther, Silver - "A Monte-Carlo AIXI Approximation" [https://www.jair.org/media/3125/live-3125-5397-jair.pdf] (Ms. Pac-man)
	"This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. Our approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a new Monte-Carlo Tree Search algorithm along with an agent-specific extension to the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a variety of stochastic and partially observable domains. We conclude by proposing a number of directions for future research."
	"This paper presents the first computationally feasible general reinforcement learning agent that directly and scalably approximates the AIXI ideal. Although well established theoretically, it has previously been unclear whether the AIXI theory could inspire the design of practical agent algorithms. Our work answers this question in the affirmative: empirically, our approximation achieves strong performance and theoretically, we can characterise the range of environments in which our agent is expected to perform well. To develop our approximation, we introduced two new algorithms: ρUCT, a Monte-Carlo expectimax approximation technique that can be used with any online Bayesian approach to the general reinforcement learning problem and FAC-CTW, a generalisation of the powerful CTW algorithm to the agent setting. In addition, we highlighted a number of interesting research directions that could improve the performance of our current agent; in particular, model class expansion and the online learning of heuristic rollout policies for ρUCT."
	-- http://youtube.com/watch?v=yfsMHtmGDKE (Pac-Man demo)
	-- http://videolectures.net/nips09_veness_bfg/
	-- http://jveness.info/publications/veness_phd_thesis_final.pdf
	-- http://jveness.info/software/mcaixi_jair_2010.zip
	-- https://github.com/moridinamael/mc-aixi




[interesting papers - exploration]

Schmidhuber - "Driven by Compression Progress: A Simple Principle Explains Essential Aspects of Subjective Beauty, Novelty, Surprise, Interestingness, Attention, Curiosity, Creativity, Art, Science, Music, Jokes" [http://arxiv.org/abs/0812.4360]
	"I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, non-arbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and artificial systems."

Schmidhuber - "Formal Theory of Creativity, Fun, and Intrinsic Motivation" [http://people.idsia.ch/~juergen/ieeecreative.pdf]
	"The simple but general formal theory of fun & intrinsic motivation & creativity is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical but non-optimal implementations (1991, 1995, 1997-2002) are reviewed, as well as several recent variants by others (2005-). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation."
	"I have argued that a simple but general formal theory of creativity based on reward for creating or finding novel patterns allowing for data compression progress explains many essential aspects of intelligence including science, art, music, humor. Here I discuss what kind of general bias towards algorithmic regularities we insert into our robots by implementing the principle, why that bias is good, and how the approach greatly generalizes the field of active learning. I provide discrete and continuous time formulations for ongoing work on building an Artificial General Intelligence based on variants of the artificial creativity framework."
	"In the real world external rewards are rare. But unsupervised AGIs using additional intrinsic rewards as described in this paper will be motivated to learn many useful behaviors even in absence of external rewards, behaviors that lead to predictable or compressible results and thus reflect regularities in the environment, such as repeatable patterns in the world’s reactions to certain action sequences. Often a bias towards exploring previously unknown environmental regularities through artificial curiosity / creativity is a priori desirable because goal-directed learning may greatly profit from it, as behaviors leading to external reward may often be rather easy to compose from previously learnt curiosity-driven behaviors. It may be possible to formally quantify this bias towards novel patterns in form of a mixture-based prior, a weighted sum of probability distributions on sequences of actions and resulting inputs, and derive precise conditions for improved expected external reward intake. Intrinsic reward may be viewed as analogous to a regularizer in supervised learning, where the prior distribution on possible hypotheses greatly influences the most probable interpretation of the data in a Bayesian framework (for example, the well-known weight decay term of neural networks is a consequence of a Gaussian prior with zero mean for each weight). Following the introductory discussion, some of the AGIs based on the creativity principle will become scientists, artists, or comedians."
	-- http://idsia.ch/~juergen/creativity.html
	-- https://archive.org/details/Redwood_Center_2014_08_15_Jurgen_Schmidhuber
	-- https://vimeo.com/28759091
	-- http://videolectures.net/ecmlpkdd2010_schmidhuber_ftf/
	-- https://vimeo.com/7441291

Houthooft, Chen, Duan, Schulman, Turck, Abbeel - "VIME: Variational Information Maximizing Exploration" [http://arxiv.org/abs/1605.09674] # approximation of Artificial Curiosity and Creativity by Juergen Schmidhuber
	"Scalable and effective exploration remains a key challenge in reinforcement learning. While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent’s belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards."
	"We have proposed Variational Information Maximizing Exploration, a curiosity-driven exploration strategy for continuous control tasks. Variational inference is used to approximate the posterior distribution of a Bayesian neural network that represents the environment dynamics. Using information gain in this learned dynamics model as intrinsic rewards allows the agent to optimize for both external reward and intrinsic surprise simultaneously. Empirical results show that VIME performs significantly better than heuristic exploration methods across various continuous control tasks and algorithms. As future work, we would like to investigate measuring surprise in the value function and using the learned dynamics model for planning."
	"This paper proposes a curiosity-driven exploration strategy, making use of information gain about the agent’s internal belief of the dynamics model as a driving force. This principle can be traced back to the concepts of curiosity and surprise (Schmidhuber). Within this framework, agents are encouraged to take actions that result in states they deem surprising - i.e., states that cause large updates to the dynamics model distribution. We propose a practical implementation of measuring information gain using variational inference. Herein, the agent’s current understanding of the environment dynamics is represented by a Bayesian neural networks. We also show how this can be interpreted as measuring compression improvement, a proposed model of curiosity (Schmidhuber). In contrast to previous curiosity-based approaches, our model scales naturally to continuous state and action spaces. The presented approach is evaluated on a range of continuous control tasks, and multiple underlying RL algorithms. Experimental results show that VIME achieves significantly better performance than naïve exploration strategies."
	--
	"Authors train Bayesian neural networks to approximate posterior distributions over dynamics models given observed data, by maximizing a variational lower bound; they then use second-order approximations of the Bayesian surprise as intrinsic motivation."
	"r'(st,at,st+1) = r(st,at) + μ * Dkl[p(θ|ξt,at,st+1)||p(θ|ξt)]"
	-- https://goo.gl/fyxLvI (demo)
	-- https://youtu.be/WRFqzYWHsZA?t=18m38s (Abbeel)
	-- https://youtube.com/watch?v=sRIjxxjVrnY (Panin)
	-- https://github.com/openai/vime

Bellemare, Srinivasan, Ostrovski, Schaul, Saxton, Munos - "Unifying Count-Based Exploration and Intrinsic Motivation" [http://arxiv.org/abs/1606.01868]
	"We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge."
	"Many of hard RL problems share one thing in common: rewards are few and far between. In reinforcement learning, exploration is the process by which an agent comes to understand its environment and discover where the reward is. Most practical RL applications still rely on crude algorithms, like epsilon-greedy (once in awhile, choose a random action), because more theoretically-motivated approaches don't scale. But epsilon-greedy is quite data inefficient, and often can't even get off the ground. In this paper we show that it's possible to use simple density models (assigning probabilities to states) to "count" the number of times we've visited a particular state. We call the output of our algorithm a pseudo-count. Pseudo-counts give us a handle on uncertainty: how confident are we that we've explored this part of the game?"
	"Intrinsic motivation offers a different perspective on exploration. Intrinsic motivation algorithms typically use novelty signals - surrogates for extrinsic rewards - to drive curiosity within an agent, influenced by classic ideas from psychology. To sketch out some recurring themes, these novelty signals might be prediction error (Singh et al., 2004; Stadie et al., 2015), value error (Simsek and Barto, 2006), learning progress (Schmidhuber, 1991), or mutual information (Still and Precup, 2012; Mohamed and Rezende, 2015). The idea also finds roots in continual learning (Ring, 1997). In Thrun’s taxonomy, intrinsic motivation methods fall within the category of error-based exploration."
	"We provide what we believe is the first formal evidence that intrinsic motivation and count-based exploration are but two sides of the same coin. Our main result is to derive a pseudo-count from a sequential density model over the state space. We only make the weak requirement that such a model should be learning-positive: observing x should not immediately decrease its density. In particular, counts in the usual sense correspond to the pseudo-counts implied by the data’s empirical distribution. We expose a tight relationship between the pseudo-count, a variant of Schmidhuber’s compression progress which we call prediction gain, and Bayesian information gain."
	--
	"Authors derived pseudo-counts from Context Tree Switching density models over states and used those to form intrinsic rewards."
	"Although I'm a bit bothered by the assumption of the density model being "learning-positive", which seems central to their theoretical derivation of pseudo-counts: after you observe a state, your subjective probability of observing it again immediately should generally decrease unless you believe that the state is a fixed point attractor with high probability. I can see that in practice the assumption works well in their experimental setting since they use pixel-level factored models and, by the nature of the ATARI games they test on, most pixels don't change value from frame to frame, but in a more general setting, e.g. a side-scroller game or a 3D first-person game this assumption would not hold."
	-- https://youtube.com/watch?v=0yI2wJ6F8r0 (demo)
	-- https://youtu.be/qduxl-vKz1E?t=1h16m30s (Seleznev, in russian)
	-- https://youtube.com/watch?v=qKyOLNVpknQ (Pavlov, in russian)
	-- https://github.com/lake4790k/pseudo-count-atari

Singh, Barto, Chentanez - "Intrinsically Motivated Reinforcement Learning" [http://www-anw.cs.umass.edu/pubs/2004/singh_bc_NIPS04.pdf]
	"Psychologists call behavior intrinsically motivated when it is engaged in for its own sake rather than as a step toward solving a specific problem of clear practical value. But what we learn during intrinsically motivated behavior is essential for our development as competent autonomous entities able to efficiently solve a wide range of practical problems as they arise. In this paper we present initial results from a computational study of intrinsically motivated reinforcement learning aimed at allowing artificial agents to construct and extend hierarchies of reusable skills that are needed for competent autonomy."
	"Psychologists distinguish between extrinsic motivation, which means being moved to do something because of some specific rewarding outcome, and intrinsic motivation, which refers to being moved to do something because it is inherently enjoyable. Intrinsic motivation leads organisms to engage in exploration, play, and other behavior driven by curiosity in the absence of explicit reward. These activities favor the development of broad competence rather than being directed to more externally-directed goals. In contrast, machine learning algorithms are typically applied to single problems and so do not cope flexibly with new problems as they arise over extended periods of time. Although the acquisition of competence may not be driven by specific problems, this competence is routinely enlisted to solve many different specific problems over the agent’s lifetime. The skills making up general competence act as the “building blocks” out of which an agent can form solutions to new problems as they arise. Instead of facing each new challenge by trying to create a solution out of low-level primitives, it can focus on combining and adjusting its higher-level skills. In animals, this greatly increases the efficiency of learning to solve new problems, and our main objective is to achieve a similar efficiency in our machine learning algorithms and architectures. This paper presents an elaboration of the reinforcement learning framework that encompasses the autonomous development of skill hierarchies through intrinsically motivated reinforcement learning."

Still, Precup - "An Information-Theoretic Approach to Curiosity-Driven Reinforcement Learning" [http://www2.hawaii.edu/~sstill/StillPrecup2011.pdf]
	"We provide a fresh look at the problem of exploration in reinforcement learning, drawing on ideas from information theory. First, we show that Boltzmann-style exploration, one of the main exploration methods used in reinforcement learning, is optimal from an information-theoretic point of view. Second, we address the problem of curiosity-driven learning. We propose that, in addition to maximizing the expected return, a learner should chose a policy that maximizes the predictive power of its own behavior, measured by the information that the most recent state-action pair carries about the future. This makes the world “interesting” and exploitable. The general result has the form of Boltzmann-style exploration with a bonus that contains a novel exploration-exploitation trade-off that emerges from the proposed optimization principle. Importantly, this exploration-exploitation trade-off is also present when the “ temperature”-like parameter in the Boltzmann distribution tends to zero, i.e. when there is no exploration due to randomness. As a result, exploration emerges as a directed behavior that optimizes information gain, rather than being modeled solely as behavior randomization."
	"We showed that a soft policy similar to Boltzmann exploration optimally trades return and the coding cost (or complexity) of the policy. By postulating that an agent should, in addition to maximizing the expected return, also maximize its predictive power, at a fixed policy complexity, we derived a trade-off between exploration and exploitation that does not rely on randomness in the action policy, and thereby may be more adequate to model exploration than previous schemes."

Salge, Glackin, Polani - "Empowerment - An Introduction" [https://arxiv.org/abs/1310.1863]
	"Is it better for you to own a corkscrew or not? If asked, you as a human being would likely say “yes”, but more importantly, you are somehow able to make this decision. You are able to decide this, even if your current acute problems or task do not include opening a wine bottle. Similarly, it is also unlikely that you evaluated several possible trajectories your life could take and looked at them with and without a corkscrew, and then measured your survival or reproductive fitness in each. When you, as a human cognitive agent, made this decision, you were likely relying on a behavioural “proxy”, an internal motivation that abstracts the problem of evaluating a decision impact on your overall life, but evaluating it in regard to some simple fitness function. One example would be the idea of curiosity, urging you to act so that your experience new sensations and learn about the environment. On average, this should lead to better and richer models of the world, which give you a better chance of reaching your ultimate goals of survival and reproduction.
	But how about questions such as, would you rather be rich than poor, sick or healthy, imprisoned or free? While each options offers some interesting new experience, there seems to be a consensus that rich, healthy and free is a preferable choice. We think that all these examples, in addition to the question of tool ownership above, share a common element of preparedness. Everything else being equal it is preferable to be prepared, to keep ones options open or to be in a state where ones actions have the greatest influence on ones direct environment.
	The concept of Empowerment, in a nutshell, is an attempt at formalizing and quantifying these degrees of freedom (or options) that an organism or agent has as a proxy for “preparedness”; preparedness, in turn, is considered a proxy for prospective fitness via the hypothesis that preparedness would be a good indicator to distinguish promising from less promising regions in the prospective fitness landscape, without actually having to evaluate the full fitness landscape.
	Empowerment aims to reformulate the options or degrees of freedom that an agent has as the agent’s control over its environment; and not only of its control - to be reproducible, the agent needs to be aware of its control influence and sense it. Thus, empowerment is a measure of both the control an agent has over its environment, as well as its ability to sense this control. Note that this already hints at two different perspectives to evaluate the empowerment of an agent. From the agent perspective empowerment can be a tool for decision making, serving as a behavioural proxy for the agent. This empowerment value can be skewed by the quality of the agent world model, so it should be more accurately described as the agent’s approximation of its own empowerment, based on its world model. The actual empowerment depends both on the agent’s embodiment, and the world the agent is situated in. More precisely, there is a specific empowerment value for the current state of the world (the agent’s current empowerment), and there is an averaged value over all possible states of the environment, weighted by their probability (the agent’s average empowerment).
	Empowerment, as introduced by Klyubin et al. (2005), aims to formalize the combined notion of an agent controlling its environment and sensing this control in the language of information theory. The idea behind this is that this should provide us with a utility function that is inherently local, universal and task-independent.
	1. Local means that the knowledge of the local dynamics of the agent is enough to compute it, and that it is not necessary to know the whole system to determine one’s empowerment. Ideally, the information that the agent itself can acquire should be enough.
	2. Universal means that it should be possible to apply empowerment “universally” to every possible agent-world interaction. This is achieved by expressing it in the language of information theory and thus making it applicable for any system that can be probabilistically expressed.
	3. Task-independent means that empowerment is not evaluated in regard to a specific goal or external reward state. Instead, empowerment is determined by the agent’s embodiment in the world. In particular, apart from minor niche-dependent parameters, the empowerment formalism should have the very same structure in most situations.
	More concretely, the proposed formulation of empowerment defines it via the concept of potential information flow, or channel capacity, between an agent’s actuator state at earlier times and their sensor state at a later time. The idea behind this is that empowerment would quantify how much an agent can reliably and perceptibly influence the world."
	"The different scenarios presented here, and in the literature on empowerment in general, are highlighting an important aspect of the empowerment flavour of intrinsic motivation algorithms, namely its universality. The same principle that organizes a swarm of agents into a pattern can also swing the pendulum into an upright position, seek out a central location in a maze, be driven towards a manipulable object, or drive the evolution of sensors. The task-independent nature reflected in this list can be both a blessing and a curse. In many cases the resulting solution, such as swinging the pendulum into the upright position, is the goal implied by default by a human observer. However, if indeed a goal is desired that differs from this default, then empowerment will not be the best solution. At present, the question of how to integrate explicit non-default goals into empowerment is fully open."
	"Let us conclude with a remark regarding the biological empowerment hypotheses in general: the fact that the default behaviours produced by empowerment seem often to match what intuitive expectations concerning default behaviour seem to imply, there is some relevance in investigating whether some of these behaviours are indeed approximating default behaviours observed in nature. A number of arguments in favour of why empowerment maximizing or similar behaviour could be relevant in biology have been made in (Klyubin et al. 2008), of which in this review we mainly highlighted its role as a measure of sensorimotor efficiency and the advantages that an evolutionary process would confer to more informationally efficient perception-action configurations."

Mohamed, Rezende - "Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning" [http://arxiv.org/abs/1509.08731]
	"The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm - an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions."
	"We have developed a new approach for scalable estimation of the mutual information by exploiting recent advances in deep learning and variational inference. We focussed specifically on intrinsic motivation with a reward measure known as empowerment, which requires at its core the efficient computation of the mutual information. By using a variational lower bound on the mutual information, we developed a scalable model and efficient algorithm that expands the applicability of empowerment to high-dimensional problems, with the complexity of our approach being extremely favourable when compared to the complexity of the Blahut-Arimoto algorithm that is currently the standard. The overall system does not require a generative model of the environment to be built, learns using only interactions with the environment, and allows the agent to learn directly from visual information or in continuous state-action spaces. While we chose to develop the algorithm in terms of intrinsic motivation, the mutual information has wide applications in other domains, all which stand to benefit from a scalable algorithm that allows them to exploit the abundance of data and be applied to large-scale problems."
	--
	"Authors developed a scalable method of approximating empowerment, the mutual information between an agent’s actions and the future state of the environment, using variational methods."
	"This paper presents a variational approach to the maximisation of mutual information in the context of a reinforcement learning agent. Mutual information in this context can provide a learning signal to the agent that is "intrinsically motivated", because it relies solely on the agent's state/beliefs and does not require from the ("outside") user an explicit definition of rewards. Specifically, the learning objective, for a current state s, is the mutual information between the sequence of K actions a proposed by an exploration distribution w(a|s) and the final state s' of the agent after performing these actions. To understand what the properties of this objective, it is useful to consider the form of this mutual information as a difference of conditional entropies: I(a,s'|s) = H(a|s) - H(a|s',s) Where I(.|.) is the (conditional) mutual information and H(.|.) is the (conditional) entropy. This objective thus asks that the agent find an exploration distribution that explores as much as possible (i.e. has high H(a|s) entropy) but is such that these actions have predictable consequences (i.e. lead to predictable state s' so that H(a|s',s) is low). So one could think of the agent as trying to learn to have control of as much of the environment as possible, thus this objective has also been coined as "empowerment".
	"Interestingly, the framework allows to also learn the state representation s as a function of some "raw" representation x of states."
	-- https://youtube.com/watch?v=tMiiKXPirAQ + https://youtube.com/watch?v=LV5jYY-JFpE (demo)
	-- https://youtube.com/watch?v=qduxl-vKz1E + https://youtube.com/watch?v=DpQKpSAMauY (Kretov, in russian)
	-- https://www.evernote.com/shard/s189/sh/8c7ff9d9-c321-4e83-a802-58f55ebed9ac/bfc614113180a5f4624390df56e73889 (Larochelle)

Stadie, Levine, Abbeel - "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models" [http://arxiv.org/abs/1507.00814]
	"Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space. Hence, exploration in complex domains is often performed with simple epsilon-greedy methods. To achieve more efficient exploration, we develop a method for assigning exploration bonuses based on a concurrently learned model of the system dynamics. By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces. We demonstrate our approach on the task of learning to play Atari games from raw pixel inputs. In this domain, our method offers substantial improvements in exploration efficiency when compared with the standard epsilon greedy approach. As a result of our improved exploration strategy, we are able to achieve state-of-the-art results on several games that pose a major challenge for prior methods."
	"In the field of reinforcement learning, agents acting in unknown environments face the exploration versus exploitation tradeoff. Without adequate exploration, the agent might fail to discover effective control strategies, particularly in complex domains. Both PAC-MDP algorithms and Bayesian algorithms have managed this tradeoff by assigning exploration bonuses to novel states. In these methods, the novelty of a state-action pair is derived from the number of times an agent has visited that pair. While these approaches offer strong formal guarantees, their requirement of an enumerable representation of the agent’s environment renders them impractical for large-scale tasks. As such, exploration in large RL tasks is still most often performed using simple heuristics, such as the epsilon-greedy strategy, which can be inadequate in more complex settings. To achieve better exploration, we develop a method for assigning exploration bonuses based on a learned model of the system dynamics. Rather than requiring an a priori and enumerable representation of the agent’s environment, we instead propose to learn a state representation from observations, and then optimize a dynamics model concurrently with the policy. The misprediction error in our learned dynamics model is then used to assess the novelty of a given state; since novel states are expected to disagree more strongly with the model than those states that have been visited frequently in the past. These exploration bonuses are motivated by Bayesian exploration bonuses, in which state-action counts serve to capture the uncertainty in the belief space over a model’s transition matrices. Though it is intractable to construct such transition matrices for complex, partially observed tasks with high-dimensional observations such as image pixels, our method captures a similar notion of uncertainty via the misprediction error in the learned dynamics model over the observation space."
	"There are a number of directions for future work. Our method assumes unpredictability is a good indicator for the need for more exploration, but in highly stochastic environments there can be transitions that remain unpredictable even after sufficient exploration, and a distributional prediction (rather than a single next state prediction) would become important. An important question both in policy learning and model learning with complex, high-dimensional observations is the question of representation. In the case of our model, we learn a representation using an autoencoder trained on prior experience, in an unsupervised setting. More generally, one can imagine learning representations as part of the model learning process in a supervised way, or even sharing representations between the model and the policy. Furthermore, although our method learns a model of the dynamics, the reinforcement learning is performed in a model-free way. We make no attempt to incorporate the model into the policy update except via exploration bonuses. An interesting direction for future work is to incorporate model-based updates with the same type of predictive model."
	--
	"The work of aims to add an effective bonus through a variation of DQN. The resulting algorithm relies on a large number of hand-tuned parameters and is only suitable for application to deterministic problems."
	-- http://research.microsoft.com/apps/video/default.aspx?id=260045 (Abbeel, 12:30)

Osband, Blundell, Pritzel, van Roy - "Deep Exploration via Bootstrapped DQN" [http://arxiv.org/abs/1602.04621]
	"Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as Epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games."
	"One of the reasons deep RL algorithms learn so slowly is that they do not gather the right data to learn about the problem. These algorithms use dithering (taking random actions) to explore their environment - which can be exponentially less efficient that deep exploration which prioritizes potentially informative policies over multiple timesteps. There is a large literature on algorithms for deep exploration for statistically efficient reinforcement learning. The problem is that none of these algorithms are computationally tractable with deep learning. We present the first practical reinforcement learning algorithm that combines deep learning with deep exploration."
	"In this paper we present bootstrapped DQN as an algorithm for efficient reinforcement learning in complex environments. We demonstrate that the bootstrap can produce useful uncertainty estimates for deep neural networks. Bootstrapped DQN can leverage these uncertainty estimates for deep exploration even in difficult stochastic systems; it also produces several state of the art results in Atari 2600. Bootstrapped DQN is computationally tractable and also naturally scalable to massive parallel systems as per (Nair et al., 2015). We believe that, beyond our specific implementation, randomized value functions represent a promising alternative to dithering for exploration. Bootstrapped DQN practically combines efficient generalization with exploration for complex nonlinear value functions.
	"Our algorithm, bootstrapped DQN, modifies DQN to produce distribution over Q-values via the bootstrap. At the start of each episode, bootstrapped DQN samples a single Q-value function from its approximate posterior. The agent then follows the policy which is optimal for that sample for the duration of the episode. This is a natural extension of the Thompson sampling heuristic to RL that allows for temporally extended (or deep) exploration. Bootstrapped DQN exhibits deep exploration unlike the naive application of Thompson sampling to RL which resample every timestep."
	"By contrast, Epsilon-greedy strategies are almost indistinguishable for small values of Epsilon and totally ineffectual for larger values. Our heads explore a diverse range of policies, but still manage to each perform well individually."
	"Unlike vanilla DQN, bootstrapped DQN can know what it doesn’t know."
	-- http://youtube.com/watch?v=Zm2KoT82O_M + http://youtube.com/watch?v=0jvEcC5JvGY (demo)
	-- https://youtu.be/mrgJ53TIcQc?t=32m24s (Pavlov, in russian)
	-- https://github.com/Kaixhin/Atari
	-- https://github.com/iassael/torch-bootstrapped-dqn
	-- https://github.com/carpedm20/deep-rl-tensorflow

Nachum, Norouzi, Schuurmans - "Improving Policy Gradient by Exploring Under-appreciated Rewards" [https://arxiv.org/abs/1611.09321]
	"This paper presents a novel form of policy gradient for model-free reinforcement learning with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring small modifications to an implementation of the REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. Our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Our algorithm successfully solves a benchmark multi-digit addition task and generalizes to long sequences. This is, to our knowledge, the first time that a pure RL method has solved addition using only reward feedback."
	"Prominent approaches to improving exploration beyond epsilon-greedy in value-based or model-based RL have focused on reducing uncertainty by prioritizing exploration toward states and actions where the agent knows the least. This basic intuition underlies work on counter and recency methods, exploration methods based on uncertainty estimates of values, methods that prioritize learning environment dynamics, and methods that provide an intrinsic motivation or curiosity bonus for exploring unknown states. We relate the concepts of value and policy in RL and propose an exploration strategy based on the discrepancy between the two."
	"To confirm whether our method is able to find the correct algorithm for multi-digit addition, we investigate its generalization to longer input sequences than provided during training. We evaluate the trained models on inputs up to a length of 2000 digits, even though training sequences were at most 33 characters. For each length, we test the model on 100 randomly generated inputs, stopping when the accuracy falls below 100%. Out of the 60 models trained on addition with UREX, we find that 5 models generalize to numbers up to 2000 digits without any observed mistakes."

Osband, van Roy - "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?" [http://arxiv.org/abs/1607.00215]
	"Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an O(H√S√A√T) expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where H is the horizon, S is the number of states, A is the number of actions and T is the time elapsed. This improves upon the best previous bound of O(HS√A√T) for any reinforcement learning algorithm."
	"We consider a well-studied reinforcement learning problem in which an agent interacts with a Markov decision process with the aim of maximizing expected cumulative reward. Our focus is on the tabula rasa case, in which the agent has virtually no prior information about the MDP. As such, the agent is unable to generalize across state-action pairs and may have to gather data at each in order to learn an effective decision policy. Key to performance is how the agent balances between exploration to acquire information of long-term benefit and exploitation to maximize expected near-term rewards. In principle, dynamic programming can be applied to compute the so-called Bayes-optimal solution to this problem. However, this is computationally intractable for anything beyond the simplest of toy problems. As such, researchers have proposed and analyzed a number of heuristic reinforcement learning algorithms.
	The literature on efficient reinforcement learning offers statistical efficiency guarantees for computationally tractable algorithms. These provably efficient algorithms predominantly address the exploration-exploitation trade-off via optimism in the face of uncertainty (OFU): when at a state, the agent assigns to each action an optimistically biased while statistically plausible estimate of future value and selects the action with the greatest estimate. If a selected action is not near-optimal, the estimate must be overly optimistic, in which case the agent learns from the experience. Efficiency relative to less sophisticated exploration strategies stems from the fact that the agent avoids actions that neither yield high value nor informative data.
	An alternative approach, based on Thompson sampling, involves sampling a statistically plausibly set of action values and selecting the maximizing action. These values can be generated, for example, by sampling from the posterior distribution over MDPs and computing the state-action value function of the sampled MDP. This approach is called posterior sampling for reinforcement learning (PSRL). Computational results demonstrate that PSRL dramatically outperforms algorithms based on OFU. The primary aim of this paper is to provide insight into the extent of this performance boost and the phenomenon that drives it.
	We argue that applying OFU in a manner that competes with PSRL in terms of statistical efficiency would require intractable computation. As such, OFU-based algorithms presented in the literature sacrifice statistical efficiency to attain computational tractability. We will explain how these algorithms are statistically inefficient. We will also leverage this insight to produce an O(H√S√A√T) expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where H is the horizon, S is the number of states, A is the number of actions and T is the time elapsed. This improves upon the best previous bound of O(HS√A√T) for any reinforcement learning algorithm. We discuss why we believe PSRL satisfies a tighter O(√H√S√A√T), though we have not proved that. We present computational results chosen to enhance insight on how learning times scale with problem parameters. These empirical scalings match our theoretical predictions."
	"PSRL is orders of magnitude more statistically efficient than UCRL and S-times less computationally expensive. In the future, we believe that analysts will be able to formally specify an OFU approach to RL whose statistical efficiency matches PSRL. However, we argue that the resulting confidence sets which address both the coupling over H and S will result in a computationally intractable optimization problem. For this reason, computationally efficient approaches to OFU RL will sacrifice statistical efficiency; this is why posterior sampling is better than optimism for reinforcement learning."

Castronovo, Francois-Lavet, Fonteneau, Ernst, Couetoux - "Approximate Bayes Optimal Policy Search using Neural Networks" [http://orbi.ulg.ac.be/bitstream/2268/204410/1/ANN-BRL.pdf]
	"Bayesian Reinforcement Learning agents aim to maximise the expected collected rewards obtained when interacting with an unknown Markov Decision Process while using some prior knowledge. State-of-the-art BRL agents rely on frequent updates of the belief on the MDP, as new observations of the environment are made. This offers theoretical guarantees to converge to an optimum, but is computationally intractable, even on small-scale problems. In this paper, we present a method that circumvents this issue by training a parametric policy able to recommend an action directly from raw observations. Artificial Neural Networks (ANNs) are used to represent this policy, and are trained on the trajectories sampled from the prior. The trained model is then used online, and is able to act on the real MDP at a very low computational cost. Our new algorithm shows strong empirical performance, on a wide range of test problems, and is robust to inaccuracies of the prior distribution."
	"State-of-the-art Bayesian algorithms generally do not use offline training. Instead, they rely on Bayes updates and sampling techniques during the interaction, which may be too computationally expensive, even on very small MDPs. In order to reduce significantly this cost, we propose a new practical algorithm to solve BAMDPs: Artificial Neural Networks for Bayesian Reinforcement Learning. Our algorithm aims at finding an optimal policy, i.e. a mapping from observations to actions, which maximises the rewards in a certain environment. This policy is trained to act optimally on some MDPs sampled from the prior distribution, and then it is used in the test environment. By design, our approach does not use any Bayes update, and is thus computationally inexpensive during online interactions."
	"We developed ANN-BRL, an offline policy-search algorithm for addressing BAMDPs. As shown by our experiments, ANN-BRL obtained state-of-the-art performance on all benchmarks considered in this paper. In particular, on the most challenging benchmark 9, a score 4 times higher than the one measured for the second best algorithm has been observed. Moreover, ANN-BRL is able to make online decisions faster than most BRL algorithms. Our idea is to define a parametric policy as an ANN, and train it using backpropagation algorithm. This requires a training set made of observations-action pairs and in order to generate this dataset, several simulations have been performed on MDPs drawn from prior distribution. In theory, we should label each example with a Bayes optimal action. However, those are too expensive to compute for the whole dataset. Instead, we chose to use optimal actions under full observability hypothesis. Due to the modularity of our approach, a better labelling technique could easily be integrated in ANN-BRL, and may bring stronger empirical results. Moreover, two types of features have been considered for representing the current history: Q-values and transition counters. The use of Q-values allows to reach state-of-the-art performance on most benchmarks and outperfom all other algorithms on the most difficult one. On the contrary, computing a good policy from transition counters only is a difficult task to achieve, even for Artificial Neural Networks. Nevertheless, we found that the difference between this approach and state-of-the-art algorithms was much less noticeable when prior distribution differs from test distribution, which means that at least in some cases, it is possible to compute efficient policies without relying on online computationally expensive tools such as Q-values."




[interesting papers - model-based methods]

Schmidhuber - "On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models" [http://arxiv.org/abs/1511.09249]
	"This paper addresses the general problem of reinforcement learning in partially observable environments. In 2013, our large RL recurrent neural networks learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially “learning to think.” The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another."
	"Real brains seem to be learning a predictive model of their initially unknown environment, but are still far superior to present artificial systems in many ways. They seem to exploit the model in smarter ways, e.g., to plan action sequences in hierarchical fashion, or through other types of abstract reasoning, continually building on earlier acquired skills, becoming increasingly general problem solvers able to deal with a large number of diverse and complex task."
	"We introduced novel combinations of a RNNs-based reinforcement learning controller, C, and an RNN-based predictive world model, M. In a series of trials, an RNN controller C steers an agent interacting with an initially unknown, partially observable environment. The entire lifelong interaction history is stored, and used to train an RNN world model M, which learns to predict new inputs of C (including rewards) from histories of previous inputs and actions, using predictive coding to compress the history. Controller C may uses M to achieve its goals more efficiently, e.g., through cheap, “mental” M-based trials, as opposed to expensive trials in the real world. M is temporarily used as a surrogate for the environment: M and C form a coupled RNN where M’s outputs become inputs of C, whose outputs (actions) in turn become inputs of M. Now a gradient descent technique can be used to learn and plan ahead by training C in a series of M-simulated trials to produce output action sequences achieving desired input events, such as high real-valued reward signals (while the weights of M remain fixed). Given an RL problem, C may speed up its search for rewarding behavior by learning programs that address/query/exploit M’s program-encoded knowledge about predictable regularities, e.g., through extra connections from and to (a copy of) M. This may be much cheaper than learning reward-generating programs from scratch. C also may get intrinsic reward for creating experiments causing data with yet unknown regularities that improve M."
	"The most general CM systems implement principles of algorithmic as opposed to traditional information theory. M is actively exploited in arbitrary computable ways by C, whose program search space is typically much smaller, and which may learn to selectively probe and reuse M’s internal programs to plan and reason. The basic principles are not limited to RL, but apply to all kinds of active algorithmic transfer learning from one RNN to another. By combining gradient-based RNNs and RL RNNs, we create a qualitatively new type of self-improving, general purpose, connectionist control architecture. This RNNAI may continually build upon previously acquired problem solving procedures, some of them self-invented in a way that resembles a scientist’s search for novel data with unknown regularities, preferring still-unsolved but quickly learnable tasks over others."
	"Early CM systems did not yet use powerful RNNs such as LSTM. A more fundamental problem is that if the environment is too noisy, M will usually only learn to approximate the conditional expectations of predicted values, given parts of the history. In certain noisy environments, Monte Carlo Tree Sampling and similar techniques may be applied to M to plan successful future action sequences for C. All such methods, however, are about simulating possible futures time step by time step, without profiting from human-like hierarchical planning or abstract reasoning, which often ignores irrelevant details."
	"This approach is different from other, previous combinations of traditional RL and RNNs which use RNNs only as value function approximators that directly predict cumulative expected reward, instead of trying to predict all sensations time step by time step. The CM system in the present section separates the hard task of prediction in partially observable environments from the comparatively simple task of RL under the Markovian assumption that the current input to C (which is M’s state) contains all information relevant for achieving the goal."
	"Our RNN-based CM systems of the early 1990s could in principle plan ahead by performing numerous fast mental experiments on a predictive RNN world model, M, instead of time-consuming real experiments, extending earlier work on reactive systems without memory. However, this can work well only in (near-)deterministic environments, and, even there, M would have to simulate many entire alternative futures, time step by time step, to find an action sequence for C that maximizes reward. This method seems very different from the much smarter hierarchical planning methods of humans, who apparently can learn to identify and exploit a few relevant problem-specific abstractions of possible future events; reasoning abstractly, and efficiently ignoring irrelevant spatio-temporal details."
	"According to Algorithmic Information Theory, given some universal computer, U, whose programs are encoded as bit strings, the mutual information between two programs p and q is expressed as K(q | p), the length of the shortest program w that computes q, given p, ignoring an additive constant of O(1) depending on U (in practical applications the computation will be time-bounded). That is, if p is a solution to problem P, and q is a fast (say, linear time) solution to problem Q, and if K(q | p) is small, and w is both fast and much shorter than q, then asymptotically optimal universal search for a solution to Q, given p, will generally find w first (to compute q and solve Q), and thus solve Q much faster than search for q from scratch."
	"Let both C and M be RNNs or similar general parallel-sequential computers. M’s vector of learnable real-valued parameters wM is trained by any SL or UL or RL algorithm to perform a certain well-defined task in some environment. Then wM is frozen. Now the goal is to train C’s parameters wC by some learning algorithm to perform another well-defined task whose solution may share mutual algorithmic information with the solution to M’s task. To facilitate this, we simply allow C to learn to actively inspect and reuse (in essentially arbitrary computable fashion) the algorithmic information conveyed by M and wM."
	"It means that now C’s relatively small candidate programs are given time to “think” by feeding sequences of activations into M, and reading activations out of M, before and while interacting with the environment. Since C and M are general computers, C’s programs may query, edit or invoke subprograms of M in arbitrary, computable ways through the new connections. Given some RL problem, according to the AIT argument, this can greatly accelerate C’s search for a problem-solving weight vector wˆ, provided the (time-bounded) mutual algorithmic information between wˆ and M’s program is high, as is to be expected in many cases since M’s environment-modeling program should reflect many regularities useful not only for prediction and coding, but also for decision making."
	"This simple but novel approach is much more general than previous computable, but restricted, ways of letting a feedforward C use a model M, by simulating entire possible futures step by step, then propagating error signals or temporal difference errors backwards. Instead, we give C’s program search an opportunity to discover sophisticated computable ways of exploiting M’s code, such as abstract hierarchical planning and analogy-based reasoning. For example, to represent previous observations, an M implemented as an LSTM network will develop high-level, abstract, spatio-temporal feature detectors that may be active for thousands of time steps, as long as those memories are useful to predict (and thus compress) future observations. However, C may learn to directly invoke the corresponding “abstract” units in M by inserting appropriate pattern sequences into M. C might then short-cut from there to typical subsequent abstract representations, ignoring the long input sequences normally required to invoke them in M, thus quickly anticipating a few possible positive outcomes to be pursued (plus computable ways of achieving them), or negative outcomes to be avoided."
	"Note that M (and by extension M) does not at all have to be a perfect predictor. For example, it won’t be able to predict noise. Instead M will have learned to approximate conditional expectations of future inputs, given the history so far. A naive way of exploiting M’s probabilistic knowledge would be to plan ahead through naive step-by-step Monte-Carlo simulations of possible M-predicted futures, to find and execute action sequences that maximize expected reward predicted by those simulations. However, we won’t limit the system to this naive approach. Instead it will be the task of C to learn to address useful problem-specific parts of the current M, and reuse them for problem solving. Sure, C will have to intelligently exploit M, which will cost bits of information (and thus search time for appropriate weight changes of C), but this is often still much cheaper in the AIT sense than learning a good C program from scratch."
	"While M’s weights are frozen, the weights of C can learn when to make C attend to history information represented by M’s state, and when to ignore such information, and instead use M’s innards in other computable ways. This can be further facilitated by introducing a special unit, uˆ, to C, where uˆ(t)all(t) instead of all(t) is fed into M at time t, such that C can easily (by setting uˆ(t) = 0) force M to completely ignore environmental inputs, to use M for “thinking” in other ways."
	"Given a new task and a C trained on several previous tasks, such hierarchical/incremental methods may freeze the current weights of C, then enlarge C by adding new units and connections which are trained on the new task. This process reduces the size of the search space for the new task, giving the new weights the opportunity to learn to use the frozen parts of C as subprograms."
	--
	"What you describe is my other old RNN-based CM system from 1990: a recurrent controller C and a recurrent world model M, where C can use M to simulate the environment step by step and plan ahead. But the new stuff is different and much less limited - now C can learn to ask all kinds of computable questions to M (e.g., about abstract long-term consequences of certain subprograms), and get computable answers back. No need to simulate the world millisecond by millisecond (humans apparently don’t do that either, but learn to jump ahead to important abstract subgoals)."

Bacon, Harb, Precup - "The Option-Critic Architecture" [http://arxiv.org/abs/1609.05140]
	"Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework."
	"We developed a general, gradient-based approach for learning simultaneously the intra-option policies and termination conditions, as well as the policy over options, in order to optimize a performance objective for the task at hand. Our ALE experiments demonstrate successful end-to-end training of the options in the presence of nonlinear function approximation. As noted, our approach only requires specifying the number of options. However, if one wanted to use additional pseudo-rewards, the option-critic framework would easily accommodate it. The internal policies and termination function gradients would simply need to be taken with respect to the pseudo-rewards instead of the task reward. A simple instance of this idea, which we used in some of the experiments, is to use additional rewards to encourage options that are indeed temporally extended, by adding a penalty whenever a switching event occurs."
	"Our approach can work seamlessly with any other heuristic for biasing the set of options towards some desirable property (e.g. compositionality or sparsity), as long as it can be expressed as an additive reward structure. However, as seen in the results, such biasing is not necessary to produce good results. The option-critic architecture relies on the policy gradient theorem, and as discussed in (Thomas 2014), the gradient estimators can be biased in the Qt discounted case. By introducing factors of the form γ^t Π i=1 [1 − βi] in our updates (Thomas 2014, eq (3)), it would be possible to obtain unbiased estimates. However, we do not recommend this approach, since the sample complexity of the unbiased estimators is generally too high and the biased estimators performed well in our experiments."
	"Perhaps the biggest remaining limitation of our work is the assumption that all options apply everywhere. In the case of function approximation, a natural extension to initiation sets is to use a classifier over features, or some other form of function approximation. As a result, determining which options are allowed may have similar cost to evaluating a policy over options (unlike in the tabular setting, where options with sparse initiation sets lead to faster decisions). This is akin to eligibility traces, which are more expensive than using no trace in the tabular case, but have the same complexity with function approximation. If initiation sets are to be learned, the main constraint that needs to be added is that the options and the policy over them lead to an ergodic chain in the augmented state-option space. This can be expressed as a flow condition that links initiation sets with terminations. The precise description of this condition, as well as sparsity regularization for initiation sets, is left for future work."
	-- http://videolectures.net/deeplearning2016_precup_advanced_lr/ (Precup)
	-- http://youtube.com/watch?v=8r_EoYnPjGk (Bacon)
	-- http://blog.shakirm.com/2016/07/learning-in-brains-and-machines-3-synergistic-and-modular-action/

Tamar, Wu, Thomas, Levine, Abbeel - "Value Iteration Networks" [http://arxiv.org/abs/1602.02867]
	"We introduce the value iteration network (VIN): a fully differentiable neural network with a ‘planning module’ embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains."
	"The introduction of powerful and scalable RL methods has opened up a range of new problems for deep learning. However, few recent works investigate policy architectures that are specifically tailored for planning under uncertainty, and current RL theory and benchmarks rarely investigate the generalization properties of a trained policy. This work takes a step in this direction, by exploring better generalizing policy representations. Our VIN policies learn an approximate planning computation relevant for solving the task, and we have shown that such a computation leads to better generalization in a diverse set of tasks, ranging from simple gridworlds that are amenable to value iteration, to continuous control, and even to navigation of Wikipedia links. In future work we intend to learn different planning computations, based on simulation, or optimal linear control, and combine them with reactive policies, to potentially develop RL solutions for task and motion planning"
	--
	"Its contribution is to offer a new way to think about value iteration in the context of deep networks. It shows how the CNN architecture can be hijacked to implement the Bellman optimality operator, and how the backprop signal can be used to learn a deterministic model of the underlying MDP."
	"Value iteration is similar enough to a sequence of convolutions and max-pooling layers that you can emulate an (unrolled) planning computation with a deep network. This allows neural nets to do planning, e.g. moving from start to goal in grid-world, or navigating a website to find query."
	-- https://youtube.com/watch?v=tXBHfbHHlKc (Tamar) + http://technion.ac.il/~danielm/icml_slides/Talk7.pdf
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Value-Iteration-Networks (Tamar)
	-- https://github.com/karpathy/paper-notes/blob/master/vin.md
	-- https://github.com/avivt/VIN
	-- https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks

Schaul, Horgan, Gregor, Silver - "Universal Value Function Approximators" [http://jmlr.org/proceedings/papers/v37/schaul15.pdf]
	"Value functions are a core component of reinforcement learning systems. The main idea is to construct a single function approximator V(s; θ) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators V(s, g; θ) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals."
	"Value functions may be used to represent knowledge beyond the agent’s overall goal. General value functions Vg(s) represent the utility of any state s in achieving a given goal g (e.g. a waypoint), represented by a pseudo-reward function that takes the place of the real rewards in the problem. Each such value function represents a chunk of knowledge about the environment: how to evaluate or control a specific aspect of the environment (e.g. progress toward a waypoint). A collection of general value functions provides a powerful form of knowledge representation that can be utilised in several ways. For example, the Horde architecture consists of a discrete set of value functions (‘demons’), all of which may be learnt simultaneously from a single stream of experience, by bootstrapping off-policy from successive value estimates. Each value function may also be used to generate a policy or option, for example by acting greedily with respect to the values, and terminating at goal states. Such a collection of options can be used to provide a temporally abstract action-space for learning or planning. Finally, a collection of value functions can be used as a predictive representation of state, where the predicted values themselves are used as a feature vector. In large problems, the value function is typically represented by a function approximator V(s, θ), such as a linear combination of features or a neural network with parameters θ. The function approximator exploits the structure in the state space to efficiently learn the value of observed states and generalise to the value of similar, unseen states. However, the goal space often contains just as much structure as the state space. Consider for example the case where the agent’s goal is described by a single desired state: it is clear that there is just as much similarity between the value of nearby goals as there is between the value of nearby states. Our main idea is to extend the idea of value function approximation to both states s and goals g, using a universal value function approximator V(s, g, θ). A sufficiently expressive function approximator can in principle identify and exploit structure across both s and g. By universal, we mean that the value function can generalise to any goal g in a set G of possible goals: for example a discrete set of goal states; their power set; a set of continuous goal regions; or a vector representation of arbitrary pseudo-reward functions. This UVFA effectively represents an infinite Horde of demons that summarizes a whole class of predictions in a single object. Any system that enumerates separate value functions and learns each individually (like the Horde) is hampered in its scalability, as it cannot take advantage of any shared structure (unless the demons share parameters). In contrast, UVFAs can exploit two kinds of structure between goals: similarity encoded a priori in the goal representations g, and the structure in the induced value functions discovered bottom-up. Also, the complexity of UVFA learning does not depend on the number of demons but on the inherent domain complexity. This complexity is larger than standard value function approximation, and representing a UVFA may require a rich function approximator such as a deep neural network. Learning a UVFA poses special challenges. In general, the agent will only see a small subset of possible combinations of states and goals (s, g), but we would like to generalise in several ways. Even in a supervised learning context, when the true value Vg(s) is provided, this is a challenging regression problem."
	"On the Atari game of Ms Pacman, we then demonstrate that UVFAs can scale to larger visual input spaces and different types of goals, and show they generalize across policies for obtaining possible pellets."
	"This paper has developed a universal approximator for goal-directed knowledge. We have demonstrated that our UVFA model is learnable either from supervised targets, or directly from real experience; and that it generalises effectively to unseen goals. We conclude by discussing several ways in which UVFAs may be used. First, UVFAs can be used for transfer learning to new tasks with the same dynamics but different goals. Specifically, the values V(s, g; θ) in a UVFA can be used to initialise a new, single value function Vg(s) for a new task with unseen goal g. We demonstrate that an agent which starts from transferred values in this fashion can learn to solve the new task g considerably faster than random value initialization. Second, generalized value functions can be used as features to represent state; this is a form of predictive representation. A UVFA compresses a large or infinite number of predictions into a short feature vector. Specifically, the state embedding φ(s) can be used as a feature vector that represents state s. Furthermore, the goal embedding φ(g) can be used as a separate feature vector that represents state g. These features can capture non-trivial structure in the domain. Third, a UVFA could be used to generate temporally abstract options. For any goal g a corresponding option may be constructed that acts (soft-)greedily with respect to V(s, g; θ) and terminates e.g. upon reaching g. The UVFA then effectively provides a universal option that represents (approximately) optimal behaviour towards any goal g∈ G. This in turn allows a hierarchical policy to choose any goal g∈ G as a temporally abstract action. Finally, a UVFA could also be used as a universal option model. Specifically, if pseudorewards are defined by goal achievement, then V(s, g; θ) approximates the (discounted) probability of reaching g from s, under a policy that tries to reach it."
	-- http://videolectures.net/icml2015_schaul_universal_value/
	-- http://schaul.site44.com/publications/uvfa-slides.pdf

Guo, Singh, Lee, Lewis, Wang - "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning" [https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning]
	"The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best realtime agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN."
	-- https://youtube.com/watch?v=B3b6NLUxN3U (Singh)
	-- https://youtube.com/watch?v=igm38BakyAg (Lee)

Oh, Guo, Lee, Lewis, Singh - "Action-Conditional Video Prediction using Deep Networks in Atari Games" [http://arxiv.org/abs/1507.08750]
	"Motivated by vision-based reinforcement learning problems, in particular Atari games from the recent benchmark Aracade Learning Environment, we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs."
	"Modeling videos (i.e., building a generative model) is still a very challenging problem because it usually involves high-dimensional natural-scene data with complex temporal dynamics. Thus, recent studies have mostly focused on modeling simple video data, such as bouncing balls or small video patches, where the next frame is highly predictable based on the previous frames. In many applications, however, future frames are not only dependent on previous frames but also on additional control or action variables. For example, the first-person-view in a vehicle is affected by wheel-steering and acceleration actions. The camera observation of a robot is similarly dependent on its movement and changes of its camera angle. More generally, in vision-based reinforcement learning problems, learning to predict future images conditioned on future actions amounts to learning a model of the dynamics of the agent-environment interaction; such transition-models are an essential component of model-based learning approaches to RL."
	"The encoding part computes high-level abstractions of input frames, the action-conditional transformation part predicts the abstraction of the next frame conditioned on the action, and finally the decoding part maps the predicted high-level abstraction to a detailed frame. The feedforward architecture takes the last 4 frames as input while the recurrent architecture takes just the last frame but has recurrent connections. Our experimental results on predicting images in Atari games show that our architectures are able to generate realistic frames over 100-step action-conditional future frames without diverging. We show that the representations learned by our architectures 1) approximately capture natural similarity among actions, and 2) discover which objects are directly controlled by the agent’s actions and which are only indirectly influenced or not controlled at all. We evaluated the usefulness of our architectures for control in two ways: 1) by replacing emulator frames with predicted frames in a previously-learned model-free controller (DQN; DeepMind’s state of the art Deep-Q-Network for Atari Games), and 2) by using the predicted frames to drive a more informed than random exploration strategy to improve a model-free controller (also DQN)."
	"This paper introduced two different novel deep architectures that predict future frames that are dependent on actions and showed qualitatively and quantitatively that they are able to predict visually realistic and useful-for-control frames over 100-step futures on several Atari game domains. To our knowledge, this is the first paper to show good deep predictions in Atari games. Since our architectures were domain independent we expect that they will generalize to many vison-based RL problems. In future work we will learn models that predict future reward in addition to predicing future frames and evaluate the performance of our architectures in model-based RL."
	-- https://sites.google.com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction (demos)
	-- https://youtu.be/igm38BakyAg?t=15m26s (Lee)
	-- http://research.microsoft.com/apps/video/default.aspx?id=259646 (17:30)
	-- https://github.com/junhyukoh/nips2015-action-conditional-video-prediction




[interesting papers - value-based methods]

Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, Hassabis - "Human-Level Control Through Deep Reinforcement Learning" [http://home.uchicago.edu/~arij/journalclub/papers/2015_Mnih_et_al.pdf]
	"The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks."
	-- http://nature.com/nature/journal/v518/n7540/full/nature14236.html
	-- http://nature.com/nature/journal/v518/n7540/extref/nature14236-s1.pdf
	-- http://youtube.com/watch?v=EfGD2qveGdQ (demo)
	-- http://videolectures.net/rldm2015_silver_reinforcement_learning/ (demos, 57:56, 1:06:04, 1:15:15)
	-- http://youtu.be/XAbLn66iHcQ?t=1h41m21s + http://youtube.com/watch?v=0xo1Ldx3L5Q (3D racing demo)
	-- http://youtube.com/watch?v=nMR5mjCFZCw (3D labyrinth demo)
	-- http://youtube.com/watch?v=re6hkcTWVUY (Doom gameplay demo)
	-- http://youtube.com/watch?v=6jlaBD9LCnM + https://youtube.com/watch?v=6JT6_dRcKA (blockworld demo)
	-- http://youtube.com/user/eldubro/videos (demos)
	-- http://youtube.com/watch?v=iqXKQf2BOSE (demo)
	-- http://youtube.com/watch?v=lge-dl2JUAM + https://youtube.com/watch?v=xN1d3qHMIEQ (interviews and demos)
	-- http://sodeepdude.blogspot.ru/2015/03/deepminds-atari-paper-replicated.html (demos)
	-- http://videolectures.net/nipsworkshops2013_mnih_atari/ (Volodymyr Mnih)
	-- http://youtube.com/watch?v=xzM7eI7caRk (Volodymyr Mnih)
	-- http://youtube.com/watch?v=dV80NAlEins (Nando de Freitas)
	-- http://youtube.com/watch?v=HUmEbUkeQHg (Nando de Freitas)
	-- http://youtube.com/watch?v=mrgJ53TIcQc (Pavlov, in russian)
	-- https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner + https://sites.google.com/a/deepmind.com/dqn/
	-- https://github.com/carpedm20/deep-rl-tensorflow
	-- https://github.com/Kaixhin/Atari (persistent advantage learning, dueling network architecture, Double Q-learning)
	-- https://github.com/tambetm/simple_dqn + https://youtu.be/KkIf0Ok5GCE + https://youtu.be/0ZlgrQS3krg
	-- https://github.com/tambetm/gymexperiments
	-- https://github.com/spragunr/deep_q_rl + http://sodeepdude.blogspot.ru/2015/03/deepminds-atari-paper-replicated.html
	-- https://github.com/ugo-nama-kun/DQN-chainer + http://youtube.com/watch?v=N813o-Xb6S8
	-- https://github.com/muupan/dqn-in-the-caffe
	-- https://github.com/brian473/neural_rl
	-- https://github.com/kristjankorjus/Replicating-DeepMind
	-- https://github.com/asrivat1/DeepLearningVideoGames
	-- https://github.com/Jabberwockyll/deep_rl_ale
	-- https://github.com/VinF/deer
	-- https://github.com/sherjilozair/dqn
	-- https://github.com/gliese581gg/DQN_tensorflow
	-- https://github.com/devsisters/DQN-tensorflow
	-- https://github.com/osh/kerlym
	-- https://github.com/ludc/rltorch/blob/master/torch/policies/DeepQPolicy.lua
	-- http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html

Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas - "Dueling Network Architectures for Deep Reinforcement Learning" [http://arxiv.org/abs/1511.06581]
	"In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain."
	"The advantage of the dueling architecture lies partly in its ability to learn the state-value function efficiently. With every update of the Q values in the dueling architecture, the value stream V is updated – this contrasts with the updates in a single-stream architecture where only the value for one of the actions is updated, the values for all other actions remain untouched. This more frequent updating of the value stream in our approach allocates more resources to V, and thus allows for better approximation of the state values, which in turn need to be accurate for temporal difference-based methods like Q-learning to work. This phenomenon is reflected in the experiments, where the advantage of the dueling architecture over single-stream Q networks grows when the number of actions is large. Furthermore, the differences between Q-values for a given state are often very small relative to the magnitude of Q. For example, after training with DDQN on the game of Seaquest, the average action gap (the gap between the Q values of the best and the second best action in a given state) across visited states is roughly 0.04, whereas the average state value across those states is about 15. This difference in scales can lead to small amounts of noise in the updates which can lead to reorderings of the actions, and thus make the nearly greedy policy switch abruptly. The dueling architecture with its separate advantage stream is robust to such effects."
	--
	"In advantage learning one throws away information that is not needed for coming up with a good policy. The argument is that throwing away information allows you to focus your resources on learning what is important. As an example consider Tetris when you gain a unit reward for every time step you survive. Arguably the optimal value function takes on large values when the screen is near empty, while it takes on small values when the screen is near full. The range of differences can be enormous (from millions to zero). However, for optimal decision making how long you survive does not matter. What matters is the small differences in how the screen is filled up because this is what determines where to put the individual pieces. If you learn an action value function and your algorithm focuses on something like the mean square error, i.e., getting the magnitudes right, it is very plausible that most resources of the learning algorithm will be spent on capturing how big the values are, while little resource will be spent on capturing the value differences between the actions. This is what advantage learning can fix. The fix comes because advantage learning does not need to wait until the value magnitudes are properly captured before it can start learning the value differences. As can be seen from this example, advantage learning is expected to make a bigger difference where the span of optimal values is orders of magnitudes larger than action-value differences."
	"Many recent developments blur the distinction between model and algorithm. This is profound - at least for someone with training in statistics. Ziyu Wang replaced the convnet of DQN and re-run exactly the same algorithm but with a different net (a slight modification of the old net with two streams which he calls the dueling architecture). That is, everything is the same, but only the representation (neural net) changed slightly to allow for computation of not only the Q function, but also the value and advantage functions. The simple modification resulted in a massive performance boost. For example, for the Seaquest game, DQN of the Nature paper scored 4,216 points, while the modified net of Ziyu leads to a score of 37,361 points. For comparison, the best human we have found scores 40,425 points. Importantly, many modifications of DQN only improve on the 4,216 score by a few hundred points, while the Ziyu's network change using the old vanilla DQN code and gradient clipping increases the score by nearly a factor of 10. I emphasize that what Ziyu did was he changed the network. He did not change the algorithm. However, the computations performed by the agent changed remarkably. Moreover, the modified net could be used by any other Q learning algorithm. RL people typically try to change equations and write new algorithms, instead here the thing that changed was the net. The equations are implicit in the network. One can either construct networks or play with equations to achieve similar goals."
	-- https://youtube.com/watch?v=TpGuQaswaHs + https://youtube.com/watch?v=oNLITLfrvQY (demos)
	-- http://techtalks.tv/talks/dueling-network-architectures-for-deep-reinforcement-learning/62381/ (Wang)
	-- https://youtu.be/mrgJ53TIcQc?t=35m4s (Pavlov, in russian)
	-- http://torch.ch/blog/2016/04/30/dueling_dqn.html
	-- https://github.com/carpedm20/deep-rl-tensorflow
	-- https://github.com/Kaixhin/Atari
	-- https://github.com/tambetm/gymexperiments

van Hasselt, Guez, Silver - "Deep Reinforcement Learning with Double Q-Learning" [http://arxiv.org/abs/1509.06461]
	"The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games."
	"This paper has five contributions. First, we have shown why Q-learning can be overoptimistic in large-scale problems, even if these are deterministic, due to the inherent estimation errors of learning. Second, by analyzing the value estimates on Atari games we have shown that these overestimations are more common and severe in practice than previously acknowledged. Third, we have shown that Double Q-learning can be used at scale to successfully reduce this overoptimism, resulting in more stable and reliable learning. Fourth, we have proposed a specific implementation called Double DQN, that uses the existing architecture and deep neural network of the DQN algorithm without requiring additional networks or parameters. Finally, we have shown that Double DQN finds better policies, obtaining new state-of-the-art results on the Atari 2600 domain."
	-- https://youtu.be/qLaDWKd61Ig?t=32m52s (Silver)
	-- https://youtu.be/mrgJ53TIcQc?t=17m31s (Pavlov, in russian)
	-- https://github.com/carpedm20/deep-rl-tensorflow
	-- https://github.com/Kaixhin/Atari

Schaul, Quan, Antonoglou, Silver - "Prioritized Experience Replay" [http://arxiv.org/abs/1511.05952]
	"Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in the Deep Q-Network algorithm, which achieved human-level performance in Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 42 out of 57 games."
	"Online reinforcement learning agents incrementally update their parameters (of the policy, value function or model) while they observe a stream of experience. In their simplest form, they discard incoming data immediately, after a single update. Two issues with this are (a) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms, and (b) the rapid forgetting of possibly rare experiences that would be useful later on. Experience replay a ddresses both of these issues: with experience stored in a replay memory, it becomes possible to break the temporal correlations by mixing more and less recent experience for the updates, and rare experience will be used for more than just a single update. DQN used a large sliding-window replay memory, sampled from it uniformly at random, and effectively revisited each transition eight times. In general, experience replay can reduce the amount of experience required to learn, and replace it with more computation and more memory – which are often cheaper resources than the RL agent’s interactions with its environment."
	"In this paper, we investigate how prioritizing which transitions are replayed can make experience replay more efficient and effective than if all transitions are replayed uniformly. The key idea is that an RL agent can learn more effectively from some transitions than from others. Transitions may be more or less surprising, redundant, or task-relevant. Some transitions may not be immediately useful to the agent, but might become so when the agent competence increases (Schmidhuber, 1991). We propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling."
	"Using a replay memory leads to design choices at two levels: which experience to store and which to forget, and which experience to replay (and how to do so). This paper addresses only the latter: making the most effective use of the replay memory for learning, assuming that its contents are outside of our control."
	"We find that adding prioritized replay to DQN leads to a substantial improvement in final score on 42 out of 57 games, with the median normalized performance score across 57 games increased from 69% to 97%. Furthermore, we find that the boost from prioritized experience replay is complementary to the one from introducing Double Q-learning into DQN: performance increases another notch, leading to the current state-of-the-art on the Atari benchmark. Compared to Double DQN, the mean performance increased from 389% to 551%, and the median performance from 110% to 128% bringing additional games such as River Raid, Seaquest and Surround to a human level for the first time, and making large jumps on others (e.g., Atlantis, Gopher, James Bond 007 or Space Invaders)."
	"We stumbled upon another phenomenon (obvious in retrospect), namely that some fraction of the visited transitions are never replayed before they drop out of the sliding window memory, and many more are first replayed only a long time after they are encountered. Also, uniform sampling is implicitly biased toward out-of-date transitions that were generated by a policy that has typically seen hundreds of thousands of updates since. Prioritized replay with its bonus for unseen transitions directly corrects the first of these issues, and also tends to help with the second one, as more recent transitions tend to have larger error – this is because old transitions will have had more opportunities to have them corrected, and because novel data tends to be less well predicted by the value function."
	"We hypothesize that deep neural networks interact with prioritized replay in another interesting way. When we distinguish learning the value given a representation (i.e., the top layers) from learning an improved representation (i.e., the bottom layers), then transitions for which the representation is good will quickly reduce their error and then be replayed much less, increasing the learning focus on others where the representation is poor, thus putting more resources into distinguishing aliased states – if the observations and network capacity allow for it."
	"Feedback for Exploration: An interesting side-effect of prioritized replay is that the total number Mi that a transition will end up being replayed varies widely, and this gives a rough indication of how useful it was to the agent. This potentially valuable signal can be fed back to the exploration strategy that generates the transitions. For example, we could sample exploration hyper-parameters (such as the fraction of random actions, the Boltzmann temperature, or the amount of intrinsic reward to mix in) from a parameterized distribution at the beginning of each episode, monitor the usefulness of the experience via Mi, and update the distribution toward generating more useful experience. Or, in a parallel system like the Gorila agent, it could guide resource allocation between a collection of concurrent but heterogeneous “actors”, each with different exploration hyper-parameters."
	"Prioritized Memories: Considerations that help determine which transitions to replay are likely to be relevant for determining which memories to store and when to erase them (i.e., when it becomes unlikely that we would ever want to replay them anymore). An explicit control over which memories to keep or erase can help reduce the required total memory size, because it reduces redundancy (frequently visited transitions will have low error, so many of them will be dropped), while automatically adjusting for what has been learned already (dropping many of the ‘easy’ transitions) and biasing the contents of the memory to where the errors remain high. This is a non-trivial aspect, because memory requirements for DQN are currently dominated by the size of the replay memory, no longer by the size of the neural network. Erasing is a more final decision than reducing the replay probability, thus an even stronger emphasis of diversity may be necessary, for example by tracking the age of each transitions and using it to modulate the priority in such a way as to preserve sufficient old experience to prevent cycles (related to ‘hall of fame’ ideas in multi-agent literature) or collapsing value functions. The priority mechanism is also flexible enough to permit integrating experience from other sources, such as from a planner or from human expert trajectories, since knowing the source can be used to modulate each transition’s priority, for example in such a way as to preserve a sufficient fraction of external experience in memory."
	"Numerous neuroscience studies have identified mechanisms of experience replay in the hippocampus of rodents, where sequences of prior experience are replayed, either during awake resting or sleep, and in particular that this happens more for rewarded paths. Furthermore, there is a likely link between increased replay of an experience, and how much can be learned from it, or its TD-error."
	-- https://youtu.be/mrgJ53TIcQc?t=25m43s (Pavlov, in russian)
	-- https://github.com/Kaixhin/Atari
	-- https://github.com/carpedm20/deep-rl-tensorflow

Hausknecht, Stone - "Deep Recurrent Q-Learning for Partially Observable MDPs" [http://arxiv.org/abs/1507.06527]
	"Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network exhibits similar performance on standard Atari 2600 MDPs but better performance on equivalent partially observed domains featuring flickering game screens. Results indicate that given the same length of history, recurrency allows partial information to be integrated through time and is superior to alternatives such as stacking a history of frames in the network's input layer. We additionally show that when trained with partial observations, DRQN's performance at evaluation time scales as a function of observability. Similarly, when trained with full observations and evaluated with partial observations, DRQN's performance degrades more gracefully than that of DQN. We therefore conclude that when dealing with partially observed domains, the use of recurrency confers tangible benefits."
	"Real-world tasks often feature incomplete and noisy state information, resulting from partial observability. We modify DQN to better deal with the noisy observations characteristic of POMDPs by leveraging advances in Recurrent Neural Networks. More specifically we combined a Long Short Term Memory with a Deep Q-Network and show the resulting Deep Recurrent Q-Network, despite the lack of convolutional velocity detection, is better equipped than a standard Deep Q-Network to handle the type of partial observability induced by flickering game screens. Further analysis shows that DRQN, when trained with partial observations, can generalize its policies to the case of complete observations. On the Flickering Pong domain, performance scales with the observability of the domain, reaching near-perfect performance when every game screen is observed. This result indicates that the recurrent network learns policies that are both robust enough to handle to missing game screens, and scalable enough to improve performance. Generalization also occurs in the opposite direction: when trained on unobscured Atari games and evaluated against obscured games, DRQN’s performance generalizes better than DQN’s at all levels of partial information. Our results indicate that given access to the same amount of history, processing observations in a recurrent layer (like DRQN) rather than stacking frames in the input layer (like DQN) yields better performance on POMDPs and better generalization for both POMDPs and MDPs."
	--
	"Demonstrated that recurrent Q learning can perform the required information integration to resolve short-term partial observability (e.g. to estimate velocities) that is achieved via stacks of frames in the original DQN architecture."
	-- https://youtube.com/watch?v=bE5DIJvZexc (Fritzler, in russian)

Li, Gao, He, Chen, Deng, He - "Recurrent Reinforcement Learning: A Hybrid Approach" [http://arxiv.org/abs/1509.03044]
	"Successful applications of reinforcement learning in real-world problems often require dealing with partially observable states. It is in general very challenging to construct and infer hidden states as they often depend on the agent’s entire interaction history and may require substantial domain knowledge. In this work, we investigate a deep-learning approach to learning the representation of states in partially observable tasks, with minimal prior knowledge of the domain. In particular, we study reinforcement learning with deep neural networks, including RNN and LSTM, which are equipped with the desired property of being able to capture long-term dependency on history, and thus providing an effective way of learning the representation of hidden states. We further develop a hybrid approach that combines the strength of both supervised learning (for representing hidden states) and reinforcement learning (for optimizing control) with joint training. Extensive experiments based on a KDD Cup 1998 direct mailing campaign problem demonstrate the effectiveness and advantages of the proposed approach, which performs the best across the board."
	"In this work, we have studied how to use deep learning models, for both supervised learning and reinforcement learning, to solve a CRM task, which is typical of real-world non-Markovian problems. In particular, we investigated how to utilize supervised signals in training data to learn hidden-state representations, and then jointly train a deep Q-network (using reinforcement learning) to optimize the control for maximizing long-term rewards. Through a large-scale experimental analysis under different settings, we showed that: (1) Deep RL is more effective than SL for optimizing lifetime values; (2) RL with RNN/LSTM models is a promising approach to solving non-Markovian tasks with long-term dependencies; (3) It is promising to use memory networks models to learn hidden-state representations in a supervised learning manner, with DQN jointly trained for non-Markovian tasks. Beyond the analysis, our experimental results demonstrate the promise for deep reinforcement learning on specific CRM tasks. The promising results suggest multiple interesting directions for future work. One idea is to explore the use of RNN/LSTM in model-based or policy-based RL, as opposed to the value-function-based approaches this work focuses on. Those alternatives may be more appropriate solutions in certain application domains. Another important direction is to capture latent structures of actions, in order to facilitate generalization across different actions. Doing so also allows handling newly emerged actions that are common in many applications."
	"Our work tries to close the decision-making loop: we aim to develop machine-learned models that directly take actions to maximize LTV of customers. We take a reinforcement-learning approach, instead of a data-mining or supervised-learning one, to learn a decision-making policy from data."
	"We use the much more flexible function approximator of deep neural networks to fit the Q function. It allows us to learn an RL model that outperforms a strong algorithm based on linear function approximation. More importantly, it provides an effective way to deal with hidden states that are not considered in these previous works."
	"RL models are trained to maximize long-term rewards. In contrast, SL models can be optimized to predict observations and immediate rewards, thus having the potential to better represent and infer hidden states. With such complementary strengths, it is beneficial to take a hybrid approach, which uses SL for hidden-state representation learning and RL for policy learning. Moreover, these two components should not be optimized separately: ideally, the SL component should learn an internal state representation that allows the RL component to maximize long-term reward."
	"For training the hybrid model, we use a joint supervised-reinforced approach. First, we train an RNN (or LSTM), which learns hidden states from signals including next observations and immediate rewards. Then, the learned hidden states are the input to DQN, which learns Q-function of a near-optimal policy. These two training steps are interleaved in each SGD iteration."
	"The difference between these models and RL-RNN (or RL-LSTM) is that, during training, the supervised signals are used to learn the state information, are back-propagated to the head of RNN/LSTM, while the RL signals are only back-propagated to the hidden layers of RNN for DQN training, do not involve in the RNN training."




[interesting papers - policy-based methods]

Duan, Chen, Houthooft, Schulman, Abbeel - "Benchmarking Deep Reinforcement Learning for Continuous Control" [http://arxiv.org/abs/1604.06778]
	"Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released open-source in order to facilitate experimental reproducibility and to encourage adoption by other researchers."
	"In this work, a benchmark of continuous control problems for reinforcement learning is presented, covering a wide variety of challenging tasks. We implemented several reinforcement learning algorithms, and presented them in the context of general policy parameterizations. Results show that among the implemented algorithms, TNPG, TRPO, and DDPG are effective methods for training deep neural network policies. Still, the poor performance on the proposed hierarchical tasks calls for new algorithms to be developed. Implementing and evaluating existing and newly proposed algorithms will be our continued effort. By providing an open-source release of the benchmark, we encourage other researchers to evaluate their algorithms on the proposed tasks."
	"This paper benchmarks performance over a wide range of tasks of Reinforce, (Truncated) Natural Policy Gradient, RWR, REPS, TRPO, CEM, CMA-ES, DDPG."
	-- http://techtalks.tv/talks/benchmarking-deep-reinforcement-learning-for-continuous-control/62380/
	-- https://github.com/rllab/rllab

Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu - "Asynchronous Methods for Deep Reinforcement Learning" [https://arxiv.org/abs/1602.01783]
	"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
	"We have presented asynchronous versions of four standard reinforcement learning algorithms and showed that they are able to train neural network controllers on a variety of domains in a stable manner. Our results show that in our proposed framework stable training of neural networks through reinforcement learning is possible with both valuebased and policy-based methods, off-policy as well as onpolicy methods, and in discrete as well as continuous domains. When trained on the Atari domain using 16 CPU cores, the proposed asynchronous algorithms train faster than DQN trained on an Nvidia K40 GPU, with A3C surpassing the current state-of-the-art in half the training time. One of our main findings is that using parallel actorlearners to update a shared model had a stabilizing effect on the learning process of the three value-based methods we considered. While this shows that stable online Q-learning is possible without experience replay, which was used for this purpose in DQN, it does not mean that experience replay is not useful. Incorporating experience replay into the asynchronous reinforcement learning framework could substantially improve the data efficiency of these methods by reusing old data. This could in turn lead to much faster training times in domains like TORCS where interacting with the environment is more expensive than updating the model for the architecture we used."
	-- http://youtube.com/watch?v=0xo1Ldx3L5Q (TORCS demo)
	-- http://youtube.com/watch?v=nMR5mjCFZCw (3D Labyrinth demo)
	-- http://youtube.com/watch?v=Ajjc08-iPx8 (MuJoCo demo)
	-- http://youtube.com/watch?v=9sx1_u2qVhQ (Mnih)
	-- http://techtalks.tv/talks/asynchronous-methods-for-deep-reinforcement-learning/62475/ (Mnih)
	-- https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2
	-- https://github.com/Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning
	-- https://github.com/miyosuda/async_deep_reinforce
	-- https://github.com/muupan/async-rl
	-- https://github.com/yandexdataschool/AgentNet/blob/master/agentnet/learning/a2c_n_step.py
	-- https://github.com/coreylynch/async-rl
	-- https://github.com/carpedm20/deep-rl-tensorflow/blob/master/agents/async.py
	-- https://github.com/danijar/mindpark/blob/master/mindpark/algorithm/a3c.py

Schulman, Levine, Moritz, Jordan, Abbeel - "Trust Region Policy Optimization" [http://arxiv.org/abs/1502.05477]
	"In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization. This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters."
	"We proposed and analyzed trust region methods for optimizing stochastic control policies. We proved monotonic improvement for an algorithm that repeatedly optimizes a local approximation to the expected cost of the policy with a KL divergence penalty, and we showed that an approximation to this method that incorporates a KL divergence constraint achieves good empirical results on a range of challenging policy learning tasks, outperforming prior methods. Our analysis also provides a perspective that unifies policy gradient and policy iteration methods, and shows them to be special limiting cases of an algorithm that optimizes a certain objective subject to a trust region constraint. In the domain of robotic locomotion, we successfully learned controllers for swimming, walking and hopping in a physics simulator, using general purpose neural networks and minimally informative costs. To our knowledge, no prior work has learned controllers from scratch for all of these tasks, using a generic policy search method and non-engineered, general-purpose policy representations. In the game-playing domain, we learned convolutional neural network policies that used raw images as inputs. This requires optimizing extremely high-dimensional policies, and only two prior methods report successful results on this task. Since the method we proposed is scalable and has strong theoretical foundations, we hope that it will serve as a jumping-off point for future work on training large, rich function approximators for a range of challenging problems. At the intersection of the two experimental domains we explored, there is the possibility of learning robotic control policies that use vision and raw sensory data as input, providing a unified scheme for training robotic controllers that perform both perception and control. The use of more sophisticated policies, including recurrent policies with hidden state, could further make it possible to roll state estimation and control into the same policy in the partially-observed setting. By combining our method with model learning, it would also be possible to substantially reduce its sample complexity, making it applicable to real-world settings where samples are expensive."
	--
	"Combines theoretical ideas from conservative policy gradient algorithm to prove that monotonic improvement can be guaranteed when one solves a series of subproblems of optimizing a bound on the policy performance. The conclusion is that one should use KL-divergence constraint."
	"As you iteratively improve your policy, it’s important to constrain the KL divergence between the old and new policy to be less than some constant δ. This δ (in the unit of nats) is better than a fixed step size, since the meaning of the step size changes depending on what the rewards and problem structure look like at different points in training. This is called Trust Region Policy Optimization (or, in a first-order variant, Proximal Policy Optimization) and it matters more as we do more experience replay."
	-- https://youtube.com/watch?v=jeid0wIrSn4 + https://vimeo.com/113957342 (demo)
	-- https://youtu.be/xe-z4i3l-iQ?t=30m35s (Abbeel)
	-- http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, 0:27:10)
	-- https://youtube.com/watch?v=gb5Q2XL5c8A (Schulman)
	-- https://github.com/joschu/modular_rl
	-- https://github.com/rll/deeprlhw2/blob/master/ppo.py + https://github.com/rll/deeprlhw2
	-- https://github.com/wojzaremba/trpo + https://github.com/wojzaremba/trpo_rnn
	-- https://github.com/rllab/rllab/blob/master/rllab/algos/trpo.py
	-- https://github.com/kvfrans/parallel-trpo

Schulman, Moritz, Levine, Jordan, Abbeel - "High-Dimensional Continuous Control Using Generalized Advantage Estimation" [http://arxiv.org/abs/1506.02438]
	"Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time."
	"Policy gradient methods provide a way to reduce reinforcement learning to stochastic gradient descent, by providing unbiased gradient estimates. However, so far their success at solving difficult control problems has been limited, largely due to their high sample complexity. We have argued that the key to variance reduction is to obtain good estimates of the advantage function. We have provided an intuitive but informal analysis of the problem of advantage function estimation, and justified the generalized advantage estimator, which has two parameters which adjust the bias-variance tradeoff. We described how to combine this idea with trust region policy optimization and a trust region algorithm that optimizes a value function, both represented by neural networks. Combining these techniques, we are able to learn to solve difficult control tasks that have previously been out of reach for generic reinforcement learning methods. One question that merits future investigation is the relationship between value function estimation error and policy gradient estimation error. If this relationship were known, we could choose an error metric for value function fitting that is well-matched to the quantity of interest, which is typically the accuracy of the policy gradient estimation. Some candidates for such an error metric might include the Bellman error or projected Bellman error, as described in Bhatnagar et al. (2009). Another enticing possibility is to use a shared function approximation architecture for the policy and the value function, while optimizing the policy using generalized advantage estimation. While formulating this problem in a way that is suitable for numerical optimization and provides convergence guarantees remains an open question, such an approach could allow the value function and policy representations to share useful features of the input, resulting in even faster learning. In concurrent work, researchers have been developing policy gradient methods that involve differentiation with respect to the continuous-valued action (Lillicrap et al., 2015; Heess et al., 2015). While we found empirically that the one-step return (lambda = 0) leads to excessive bias and poor performance, these papers show that such methods can work when tuned appropriately. However, note that those papers consider control problems with substantially lower-dimensional state and action spaces than the ones considered here. A comparison between both classes of approach would be useful for future work."
	-- https://youtu.be/gb5Q2XL5c8A?t=21m2s + https://youtube.com/watch?v=ATvp0Hp7RUI + https://youtube.com/watch?v=Pvw28wPEWEo (demo)
	-- https://youtu.be/xe-z4i3l-iQ?t=30m35s (Abbeel)
	-- https://youtu.be/rO7Dx8pSJQw?t=40m20s (Schulman)
	-- https://github.com/joschu/modular_rl
	-- https://github.com/rll/deeprlhw2/blob/master/ppo.py + https://github.com/rll/deeprlhw2

Silver, Lever, Heess, Degris, Wierstra, Riedmiller - "Deterministic Policy Gradient Algorithms" [http://jmlr.org/proceedings/papers/v32/silver14.html]
	"In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counter-parts in high-dimensional action spaces."
	"Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces. The basic idea is to represent the policy by a parametric probability distribution πθ(a|s) = P [a|s; θ] that stochastically selects action a in state s according to parameter vector θ. Policy gradient algorithms typically proceed by sampling this stochastic policy and adjusting the policy parameters in the direction of greater cumulative reward. In this paper we instead consider deterministic policies a=μθ(s). It is natural to wonder whether the same approach can be followed as for stochastic policies: adjusting the policy parameters in the direction of the policy gradient. It was previously believed that the deterministic policy gradient did not exist, or could only be obtained when using a model. However, we show that the deterministic policy gradient does indeed exist, and furthermore it has a simple model-free form that simply follows the gradient of the action-value function. In addition, we show that the deterministic policy gradient is the limiting case, as policy variance tends to zero, of the stochastic policy gradient."
	"From a practical viewpoint, there is a crucial difference between the stochastic and deterministic policy gradients. In the stochastic case, the policy gradient integrates over both state and action spaces, whereas in the deterministic case it only integrates over the state space. As a result, computing the stochastic policy gradient may require more samples, especially if the action space has many dimensions. In order to explore the full state and action space, a stochastic policy is often necessary. To ensure that our deterministic policy gradient algorithms continue to explore satisfactorily, we introduce an off-policy learning algorithm. The basic idea is to choose actions according to a stochastic behaviour policy (to ensure adequate exploration), but to learn about a deterministic target policy (exploiting the efficiency of the deterministic policy gradient). We use the deterministic policy gradient to derive an off-policy actor-critic algorithm that estimates the action-value function using a differentiable function approximator, and then updates the policy parameters in the direction of the approximate action-value gradient. We also introduce a notion of compatible function approximation for deterministic policy gradients, to ensure that the approximation does not bias the policy gradient."
	"We apply our deterministic actor-critic algorithms to several benchmark problems: a high-dimensional bandit; several standard benchmark reinforcement learning tasks with low dimensional action spaces; and a high-dimensional task for controlling an octopus arm. Our results demonstrate a significant performance advantage to using deterministic policy gradients over stochastic policy gradients, particularly in high dimensional tasks. In practice, the deterministic actor-critic significantly outperformed its stochastic counterpart by several orders of magnitude in a bandit with 50 continuous action dimensions, and solved a challenging reinforcement learning problem with 20 continuous action dimensions and 50 state dimensions. Furthermore, our algorithms require no more computation than prior methods: the computational cost of each update is linear in the action dimensionality and the number of policy parameters."
	--
	"DPG provides a continuous analogue to DQN, exploiting the differentiability of the Q-network to solve a wide variety of continuous control tasks."
	-- http://videolectures.net/rldm2015_silver_reinforcement_learning/ (Silver, 1:07:23)
	-- http://youtube.com/watch?v=qLaDWKd61Ig&t=38m58s (Silver)
	-- https://youtu.be/mrgJ53TIcQc?t=1h3m2s (Seleznev, in russian)
	-- https://youtu.be/rO7Dx8pSJQw?t=50m (Schulman)
	-- http://pemami4911.github.io/blog_posts/2016/08/21/ddpg-rl.html
	-- https://github.com/rllab/rllab/blob/master/rllab/algos/ddpg.py
	-- https://github.com/yandexdataschool/AgentNet/blob/master/agentnet/learning/dpg_n_step.py
	-- https://github.com/iassael/torch-policy-gradient

Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, Wierstra - "Continuous Control with Deep Reinforcement Learning" [http://arxiv.org/abs/1509.02971]
	"We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs."
	"The work presented here combines insights from recent advances in deep learning and reinforcement learning, resulting in an algorithm that robustly solves challenging problems across a variety of domains with continuous action spaces, even when using raw pixels for observations. As with most reinforcement learning algorithms, the use of non-linear function approximators nullifies any convergence guarantees; however, our experimental results demonstrate that stable learning without the need for any modifications between environments. Interestingly, all of our experiments used substantially fewer steps of experience than was used by DQN learning to find solutions in the Atari domain. Nearly all of the problems we looked at were solved within 2.5 million steps of experience (and usually far fewer), a factor of 20 fewer steps than DQN requires for good Atari solutions. This suggests that, given more simulation time, DDPG may solve even more difficult problems than those considered here. A few limitations to our approach remain. Most notably, as with most model-free reinforcement approaches, DDPG requires a large number training episodes to find solutions. However, we believe that a robust model-free approach may be an important component of larger systems which may attack these limitations."
	"While DQN solves problems with high-dimensional observation spaces, it can only handle discrete and low-dimensional action spaces. Many tasks of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. DQN cannot be straightforwardly applied to continuous domains since it relies on a finding the action that maximises the action-value function, which in the continuous valued case requires an iterative optimization process at every step."
	"In this work we present a model-free, off-policy actor-critic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action spaces. Our work is based on the deterministic policy gradient algorithm. However, as we show below, a naive application of this actor-critic method with neural function approximators is unstable for challenging problems. Here we combine the actor-critic approach with insights from the recent success of Deep Q Network. Prior to DQN, it was generally believed that learning value functions using large, non-linear function approximators was difficult and unstable. DQN is able to learn value functions using such function approximators in a stable and robust way due to two innovations: 1. the network is trained off-policy with samples from a replay buffer to minimize correlations between samples; 2. the network is trained with a target Q network to give consistent targets during temporal difference backups. In this work we make use of the same ideas, along with batch normalization, a recent advance in deep learning."
	"A key feature of the approach is its simplicity: it requires only a straightforward actor-critic architecture and learning algorithm with very few “moving parts”, making it easy to implement and scale to more difficult problems and larger networks. For the physical control problems we compare our results to a baseline computed by a planner that has full access to the underlying simulated dynamics and its derivatives. Interestingly, DDPG can sometimes find policies that exceed the performance of the planner, in some cases even when learning from pixels (the planner always plans over the true, low-dimensional state space)."
	"Surprisingly, in some simpler tasks, learning policies from pixels is just as fast as learning using the low-dimensional state descriptor. This may be due to the action repeats making the problem simpler. It may also be that the convolutional layers provide an easily separable representation of state space, which is straightforward for the higher layers to learn on quickly."
	-- http://youtube.com/watch?v=tJBIqkC1wWM (demo)
	-- http://youtube.com/watch?v=Tb5gASEJIRM (demo) + https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
	-- http://videolectures.net/rldm2015_silver_reinforcement_learning/ (Silver, 1:07:23)
	-- http://youtu.be/qLaDWKd61Ig?t=39m (Silver)
	-- http://youtu.be/KHZVXao4qXs?t=52m58s (Silver)
	-- https://youtu.be/mrgJ53TIcQc?t=1h3m2s (Seleznev, in russian)
	-- https://youtu.be/rO7Dx8pSJQw?t=50m (Schulman)
	-- http://pemami4911.github.io/blog_posts/2016/08/21/ddpg-rl.html
	-- https://github.com/rllab/rllab/blob/master/rllab/algos/ddpg.py
	-- https://github.com/iassael/torch-policy-gradient
	-- https://github.com/yandexdataschool/AgentNet/blob/master/agentnet/learning/dpg_n_step.py

Heess, Wayne, Silver, Lillicrap, Tassa, Erez - "Learning Continuous Control Policies by Stochastic Value Gradients" [http://arxiv.org/abs/1510.09142]
	"We present a unified framework for learning continuous control policies using backpropagation. It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise. The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions. We use learned models but only require observations from the environment instead of observations from model-predicted trajectories, minimizing the impact of compounded model errors. We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation. One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains."
	"We have shown that two potential problems with value gradient methods, their reliance on planning and restriction to deterministic models, can be exorcised, broadening their relevance to reinforcement learning. We have shown experimentally that the SVG framework can train neural network policies in a robust manner to solve interesting continuous control problems. Furthermore, we did not harness sophisticated generative models of stochastic dynamics, but one could readily do so, presenting great room for growth."
	"Policy gradient algorithms maximize the expectation of cumulative reward by following the gradient of this expectation with respect to the policy parameters. Most existing algorithms estimate this gradient in a model-free manner by sampling returns from the real environment and rely on a likelihood ratio estimator. Such estimates tend to have high variance and require large numbers of samples or, conversely, low-dimensional policy parameterizations. A second approach to estimate a policy gradient relies on backpropagation instead of likelihood ratio methods. If a differentiable environment model is available, one can link together the policy, model, and reward function to compute an analytic policy gradient by backpropagation of reward along a trajectory. Instead of using entire trajectories, one can estimate future rewards using a learned value function (a critic) and compute policy gradients from subsequences of trajectories. It is also possible to backpropagate analytic action derivatives from a Q-function to compute the policy gradient without a model. Following Fairbank, we refer to methods that compute the policy gradient through backpropagation as value gradient methods. In this paper, we address two limitations of prior value gradient algorithms. The first is that, in contrast to likelihood ratio methods, value gradient algorithms are only suitable for training deterministic policies. Stochastic policies have several advantages: for example, they can be beneficial for partially observed problems; they permit on-policy exploration; and because stochastic policies can assign probability mass to off-policy trajectories, we can train a stochastic policy on samples from an experience database in a principled manner. When an environment model is used, value gradient algorithms have also been critically limited to operation in deterministic environments. By exploiting a mathematical tool known as “re-parameterization” that has found recent use for generative models, we extend the scope of value gradient algorithms to include the optimization of stochastic policies in stochastic environments. We thus describe our framework as Stochastic Value Gradient methods. Secondly, we show that an environment dynamics model, value function, and policy can be learned jointly with neural networks based only on environment interaction. Learned dynamics models are often inaccurate, which we mitigate by computing value gradients along real system trajectories instead of planned ones, a feature shared by model-free methods. This substantially reduces the impact of model error because we only use models to compute policy gradients, not for prediction, combining advantages of model-based and model-free methods with fewer of their drawbacks. We present several algorithms that range from model-based to model-free methods, flexibly combining models of environment dynamics with value functions to optimize policies in stochastic or deterministic environments. Experimentally, we demonstrate that SVG methods can be applied using generic neural networks with tens of thousands of parameters while making minimal assumptions about plans or environments. By examining a simple stochastic control problem, we show that SVG algorithms can optimize policies where model-based planning and likelihood ratio methods cannot. We provide evidence that value function approximation can compensate for degraded models, demonstrating the increased robustness of SVG methods over model-based planning. Finally, we use SVG algorithms to solve a variety of challenging, under-actuated, physical control problems, including swimming of snakes, reaching, tracking, and grabbing with a robot arm, fall-recovery for a monoped, and locomotion for a planar cheetah and biped."
	--
	"In policy-based and actor-critic methods, stochastic policy is usually defined as a fixed distribution over action domain with parameters whose values are adapted when training. SVG suggests a synthesis of model-based with model-free approaches that allows optimizing the distribution as a function by means of the standard gradient descent."
	"Stochastic value gradients generalize DPG to stochastic policies in a number of ways, giving a spectrum from model-based to model-free algorithms. While SVG(0) is a direct stochastic generalization of DPG, SVG(1) combines an actor, critic and model f. The actor is trained through a combination of gradients from the critic, model and reward simultaneously."
	-- https://youtu.be/PYdL7bcn_cM (demo)
	-- https://youtu.be/mrgJ53TIcQc?t=1h10m31s (Seleznev, in russian)
	-- https://youtu.be/rO7Dx8pSJQw?t=50m (Schulman)

Schulman, Heess, Weber, Abbeel - "Gradient Estimation Using Stochastic Computation Graphs" [http://arxiv.org/abs/1506.05254]
	"In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions."
	"We have developed a framework for describing a computation with stochastic and deterministic operations, called a stochastic computation graph. Given a stochastic computation graph, we can automatically obtain a gradient estimator, given that the graph satisfies the appropriate conditions on differentiability of the functions at its nodes. The gradient can be computed efficiently in a backwards traversal through the graph: one approach is to apply the standard backpropagation algorithm to one of the surrogate loss functions; another approach (which is roughly equivalent) is to apply a modified backpropagation procedure. The results we have presented are sufficiently general to automatically reproduce a variety of gradient estimators that have been derived in prior work in reinforcement learning and probabilistic modeling. We hope that this work will facilitate further development of interesting and expressive models."
	-- http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, 1:02:04)

Weber, Heess, Eslami, Schulman, Wingate, Silver - "Reinforced Variational Inference" [http://approximateinference.org/accepted/WeberEtAl2015.pdf]
	"Recent years have seen an increase in the complexity and scale of probabilistic models used to understand and analyze data, with a corresponding increase in the difficulty of performing inference. An important enabling factor in this context has been the development of stochastic gradient algorithms for learning variational approximations to posterior distributions. In a separate line of work researchers have been investigating how to use probabilistic inference for the problem of optimal control. By viewing control as an inference problem, they showed that they could ‘borrow’ algorithms from the inference literature (e.g. belief propagation) and turn them into control algorithms. In this work, we do just the opposite: we formally map the problem of learning approximate posterior distributions in variational inference onto the policy optimization problem in reinforcement learning, explaining this connection at two levels. We first provide a high level connection, where draws from the approximate posterior correspond to trajectory samples, free energies to expected returns, and where the core computation involves computing gradients of expectations. We follow by a more detailed, sequential mapping where Markov Decision Processes concepts (state, action, rewards and transitions) are clearly defined in the inference context. We then illustrate how this allows us to leverage ideas from RL for inference network learning, for instance by introducing the concept of value functions in sequential variational inference. For concreteness and simplicity, in the main text we focus on inference for a particular model class and derive the general case in the appendix."




[interesting papers - guided policy search]

Levine, Koltun - "Guided Policy Search" [http://vladlen.info/papers/guided-policy-search.pdf]
	"Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running."
	"In this paper, we show how trajectory optimization can guide the policy search away from poor local optima. Our guided policy search algorithm uses differential dynamic programming to generate “guiding samples”, which assist the policy search by exploring high-reward regions. An importance sampled variant of the likelihood ratio estimator is used to incorporate these guiding samples directly into the policy search. We show that DDP can be modified to sample from a distribution over high reward trajectories, making it particularly suitable for guiding policy search. Furthermore, by initializing DDP with example demonstrations, our method can perform learning from demonstration. The use of importance sampled policy search also allows us to optimize the policy with second order quasi-Newton methods for many gradient steps without requiring new on-policy samples, which can be crucial for complex, nonlinear policies. Our main contribution is a guided policy search algorithm that uses trajectory optimization to assist policy learning. We show how to obtain suitable guiding samples, and we present a regularized importance sampled policy optimization method that can utilize guiding samples and does not require a learning rate or new samples at every gradient step. We evaluate our method on planar swimming, hopping, and walking, as well as 3D humanoid running, using general-purpose neural network policies. We also show that both the proposed sampling scheme and regularizer are essential for good performance, and that the learned policies can generalize successfully to new environments."
	"Standard likelihood ratio methods require new samples from the current policy at each gradient step, do not admit off-policy samples, and require the learning rate to be chosen carefully to ensure convergence. We discuss how importance sampling can be used to lift these constraints."
	"Prior methods employed importance sampling to reuse samples from previous policies. However, when learning policies with hundreds of parameters, local optima make it very difficult to find a good solution. In this section, we show how differential dynamic programming can be used to supplement the sample set with off-policy guiding samples that guide the policy search to regions of high reward."
	"We incorporate guiding samples into the policy search by building one or more initial DDP solutions and supplying the resulting samples to the importance sampled policy search algorithm. These solutions can be initialized with human demonstrations or with an offline planning algorithm. When learning from demonstrations, we can perform just one step of DDP starting from the example demonstration, thus constructing a Gaussian distribution around the example. If adaptive guiding distributions are used, they are constructed at each iteration of the policy search starting from the previous DDP solution. Although our policy search component is model-free, DDP requires a model of the system dynamics. Numerous recent methods have proposed to learn the model, and if we use initial examples, only local models are required. One might also wonder why the DDP policy is not itself a suitable controller. The issue is that this policy is time-varying and only valid around a single trajectory, while the final policy can be learned from many DDP solutions in many situations. Guided policy search can be viewed as transforming a collection of trajectories into a controller. This controller can adhere to any parameterization, reflecting constraints on computation or available sensors in partially observed domains. In our evaluation, we show that such a policy generalizes to situations where the DDP policy fail."
	"Policy gradient methods often require on-policy samples at each gradient step, do not admit off-policy samples, and cannot use line searches or higher order optimization methods such as LBFGS, which makes them difficult to use with complex policy classes. Our approach follows prior methods that use importance sampling to address these challenges. While these methods recycle samples from previous policies, we also introduce guiding samples, which dramatically speed up learning and help avoid poor local optima. We also regularize the importance sampling estimator, which prevents the optimization from assigning low probabilities to all samples. The regularizer controls how far the policy deviates from the samples, serving a similar function to the natural gradient, which bounds the information loss at each iteration. Unlike Tang and Abbeel’s ESS constraint, our regularizer does not penalize reliance on a few samples, but does avoid policies that assign a low probability to all samples. Our evaluation shows that the regularizer can be crucial for learning effective policies."
	"We presented a guided policy search algorithm that can learn complex policies with hundreds of parameters by incorporating guiding samples into the policy search. These samples are drawn from a distribution built around a DDP solution, which can be initialized from demonstrations. We evaluated our method using general-purpose neural networks on a range of challenging locomotion tasks, and showed that the learned policies generalize to new environments. While our policy search is model-free, it is guided by a model-based DDP algorithm. A promising avenue for future work is to build the guiding distributions with model-free methods that either build trajectory following policies or perform stochastic trajectory optimization. Our rough terrain results suggest that GPS can generalize by learning basic locomotion principles such as balance. Further investigation of generalization is an exciting avenue for future work. Generalization could be improved by training on multiple environments, or by using larger neural networks with multiple layers or recurrent connections. It would be interesting to see whether such extensions could learn more general and portable concepts, such as obstacle avoidance, perturbation recoveries, or even higher-level navigation skills."
	--
	"Use (modification of) importance sampling to get policy gradient, where samples are obtained via trajectory optimization."
	-- https://graphics.stanford.edu/projects/gpspaper/index.htm
	-- http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, part 2)
	-- http://youtube.com/watch?v=EtMyH_--vnU (Levine)
	-- https://video.seas.harvard.edu/media/ME+Sergey+Levine+2015+-04-01/1_gqqp9r3o/23375211 (Levine)
	-- http://youtube.com/watch?v=xMHjkZBvnfU (Abbeel)
	-- http://rll.berkeley.edu/gps/ + http://rll.berkeley.edu/gps/faq.html
	-- https://github.com/nivwusquorum/guided-policy-search/

Levine, Abbeel - "Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics" [http://rll.berkeley.edu/nips2014gps/mfcgps.pdf]
	"We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation."
	-- http://rll.berkeley.edu/nips2014gps/ (demos)
	-- http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, part 2)
	-- https://github.com/nivwusquorum/guided-policy-search/

Levine, Wagener, Abbeel - "Learning Contact-Rich Manipulation Skills with Guided Policy Search" [http://arxiv.org/abs/1501.05611]
	"Autonomous learning of object manipulation skills can enable robots to acquire rich behavioral repertoires that scale to the variety of objects found in the real world. However, current motion skill learning methods typically restrict the behavior to a compact, low-dimensional representation, limiting its expressiveness and generality. In this paper, we extend a recently developed policy search method and use it to learn a range of dynamic manipulation behaviors with highly general policy representations, without using known models or example demonstrations. Our approach learns a set of trajectories for the desired motion skill by using iteratively refitted time-varying linear models, and then unifies these trajectories into a single control policy that can generalize to new situations. To enable this method to run on a real robot, we introduce several improvements that reduce the sample count and automate parameter selection. We show that our method can acquire fast, fluent behaviors after only minutes of interaction time, and can learn robust controllers for complex tasks, including stacking large lego blocks, putting together a plastic toy, placing wooden rings onto tight-fitting pegs, and screwing bottle caps onto bottles."
	"The central idea behind guided policy search is to decompose the policy search problem into alternating trajectory optimization and supervised learning phases, where trajectory optimization is used to find a solution to the control problem and produce training data that is then used in the supervised learning phase to train a nonlinear, high-dimensional policy. By training a single policy from multiple trajectories, guided policy search can produce complex policies that generalize effectively to a range of initial states."
	-- http://rll.berkeley.edu/icra2015gps/
	-- http://youtube.com/watch?t=35&v=JeVppkoloXs + http://youtube.com/watch?v=oQasCj1X0e8 (demo)
	-- http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, part 2)
	-- http://youtube.com/watch?v=EtMyH_--vnU (Levine)
	-- https://video.seas.harvard.edu/media/ME+Sergey+Levine+2015+-04-01/1_gqqp9r3o/23375211 (Levine)
	-- http://youtube.com/watch?v=xMHjkZBvnfU (Abbeel)
	-- http://reddit.com/r/MachineLearning/comments/36yx00/2_papers_in_deep_sensorimotor_learning/ + http://reddit.com/r/MachineLearning/comments/374e8h/uc_berkley_labs_develops_a_deep_learning/

Levine, Finn, Darrell, Abbeel - "End-to-End Training of Deep Visuomotor Policies" [http://arxiv.org/abs/1504.00702]
	"Policy search methods based on reinforcement learning and optimal control can allow robots to automatically learn a wide range of tasks. However, practical applications of policy search tend to require the policy to be supported by hand-engineered components for perception, state estimation, and low-level control. We propose a method for learning policies that map raw, low-level observations, consisting of joint angles and camera images, directly to the torques at the robot's joints. The policies are represented as deep convolutional neural networks with 92,000 parameters. The high dimensionality of such policies poses a tremendous challenge for policy search. To address this challenge, we develop a sensorimotor guided policy search method that can handle high-dimensional policies and partially observed tasks. We use BADMM to decompose policy search into an optimal control phase and supervised learning phase, allowing CNN policies to be trained with standard supervised learning techniques. This method can learn a number of manipulation tasks that require close coordination between vision and control, including inserting a block into a shape sorting cube, screwing on a bottle cap, fitting the claw of a toy hammer under a nail with various grasps, and placing a coat hanger on a clothes rack."
	-- https://sites.google.com/site/visuomotorpolicy/home
	-- http://youtube.com/watch?v=EtMyH_--vnU (Levine)
	-- https://video.seas.harvard.edu/media/ME+Sergey+Levine+2015+-04-01/1_gqqp9r3o/23375211 (Levine)
	-- http://youtube.com/watch?v=xMHjkZBvnfU (Abbeel)
	-- http://reddit.com/r/MachineLearning/comments/36yx00/2_papers_in_deep_sensorimotor_learning/ + http://reddit.com/r/MachineLearning/comments/374e8h/uc_berkley_labs_develops_a_deep_learning/
	-- http://rll.berkeley.edu/gps/ (code) + http://rll.berkeley.edu/gps/faq.html

Zhang, Levine, McCarthy, Finn, Abbeel - "Learning Deep Neural Network Policies with Continuous Memory States" [http://arxiv.org/abs/1507.01273]
	"Policy learning for partially observed control tasks requires policies that can remember salient information from past observations. In this paper, we present a method for learning policies with internal memory for high-dimensional, continuous systems, such as robotic manipulators. Our approach consists of augmenting the state and action space of the system with continuous-valued memory states that the policy can read from and write to. Learning general-purpose policies with this type of memory representation directly is difficult, because the policy must automatically figure out the most salient information to memorize at each time step. We show that, by decomposing this policy search problem into a trajectory optimization phase and a supervised learning phase through a method called guided policy search, we can acquire policies with effective memorization and recall strategies. Intuitively, the trajectory optimization phase chooses the values of the memory states that will make it easier for the policy to produce the right action in future states, while the supervised learning phase encourages the policy to use memorization actions to produce those memory states. We evaluate our method on tasks involving continuous control in manipulation and navigation settings, and show that our method can learn complex policies that successfully complete a range of tasks that require memory."
	"Our experimental results show that our method can be used to learn a variety of tasks involving continuous control in manipulation and navigation settings. In direct comparisons, we find that our approach outperforms a method where the neural network in guided policy search is na¨ıvely replaced with a recurrent network using backpropagation through time, as well as a purely feedforward policy with no memory."
	"While specialized RNN representations such as LSTMs or GRUs can mitigate these issues, our experiments show that our guided policy search algorithm with memory states can produce more effective policies than backpropagation through time with LSTMs."
	"Previous work has only applied guided policy search to training reactive feedforward policies, since the algorithm assumes that the policy is Markovian. We modify the BADMM-based guided policy search method to handle continuous memory states. The memory states are added to the state of the system, and the policy is tasked both with choosing the action and modifying the memory states. Although the resulting policy can be viewed as an RNN, we do not need to perform backpropagation through time to train the recurrent connections inside the policy. Instead, the memory states are optimized by the trajectory optimization algorithm, which intuitively seeks to set the memory states to values that will allow the policy to take the appropriate action at each time step, and the policy then attempts to mimic this behavior in the supervised learning phase."
	"We presented a method for training policies for continuous control tasks that require memory. Our method consists of augmenting the state space with memory states, which the policy can choose to read and write as necessary. The resulting augmented control problem is solved using guided policy search, which uses simple trajectory-centric reinforcement learning algorithms to optimize trajectories from several initial states, and then uses these trajectories to generate a training set that can be used to optimize the policy with supervised learning. In the augmented state space, the policy is purely reactive, which means that policy training does not require backpropagating the gradient through time. However, when viewed together with the memory states, the policy is endowed with memory, and can be regarded as a recurrent neural network. Our experimental results show that our method can be used to learn policies for a variety of simulated robotic tasks that require maintaining internal memory to succeed. Part of the motivation for our approach came from the observation that even fully feed-forward neural network policies could often complete tricky tasks that seemed to require memory by using the physical state of the robot to “store” information, similarly to how a person might “remember” a number while counting by using their fingers. In our approach, we exploit this capability of reactive feedforward policies by providing extra state variables that do not have a physical analog, and exist only for the sake of memory."
	"One interesting direction for follow-up work is to apply our approach for training recurrent networks for general supervised learning tasks, rather than just robotic control. In this case, the memory state comprises the entire state of the system, and the cost function is simply the supervised learning loss. Since the hidden memory state activations are optimized separately from the network weights, such an approach could in principle be more effective at training networks that perform complex reasoning over temporally extended intervals. Furthermore, since our method trains stochastic policies, it would also be able to train stochastic recurrent neural networks, where the transition dynamics are non-deterministic. These types of networks are typically quite challenging to train, and exploring this further is an exciting direction for future work."
	-- http://rll.berkeley.edu/gpsrnn/ (demo)
	-- http://thespermwhale.com/jaseweston/ram/slides/session4/ram_talk_zhang_marvin.pdf

Zhang, Kahn, Levine, Abbeel - "Learning Deep Control Policies for Autonomous Aerial Vehicles with MPC-Guided Policy Search" [http://arxiv.org/abs/1509.06791]
	"Model predictive control is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters. However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments. Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to underactuated systems that are liable to fail catastrophically during training, before an effective policy has been found. We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment. This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle’s onboard sensors. After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC. We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time."
	"We presented an algorithm for training deep neural network control policies for autonomous aerial vehicles, by using model predictive control to generate guiding samples for guided policy search. Our MPC-guided policy search uses a modified MPC algorithm that trades off minimizing the cost against matching the current neural network policy, so as to generate good training data that can be used to train a better policy with standard supervised learning. Since the partially trained neural network policy is never used to choose actions at training time, the more robust and reliable MPC method provides a substantial improvement in safety over traditional reinforcement learning methods. Our results show that this algorithm is able to learn complex policies, such as high speed obstacle avoidance, using raw sensor inputs and lowlevel motor command outputs. One of the key ideas behind our method is the notion of an instrumented training setup, which allows MPC to be performed at training time with true state observations, which could be provided, for example, by using motion capture. At the same time, the vehicle gathers observations from its own onboard sensors, and trains the policy to mimic the action chosen by MPC using only the raw sensor readings, without relying on the full state. Acquiring the true sensor readings is important, because accurately modeling complex sensors, such as laser range finders and cameras, is very difficult, while obtaining a model of the vehicle that is accurate enough to perform MPC is comparatively easier. While our approach can train very complex, highdimensional policies, it shares many of the limitations of prior guided policy search methods. In particular, full state observations are required at training time, in order to perform MPC, even though the final neural network policy can perform the task using only onboard sensors. In the real world, this kind of state information could be obtained using an instrumented training environment (with, for example, motion capture). Since the instrumentation is only required during training, the final neural network is still able to act in the real world, so this approach is practical for a wide range of robotic tasks. However, not all aerial maneuvers can be learned in such an instrumented training setup, and combining explicit state estimation with guided policy search in future work could lead to a much more broadly applicable algorithm. Another direction that can be explored in future work is to combine guided policy search with more sophisticated MPC and planning algorithms. In principle, a wide variety of methods can be used to generate guiding samples, and more sophisticated methods might afford superior robustness and obstacle avoidance."
	"We propose to use an off-policy guided policy search algorithm in combination with a model predictive control scheme to train policies for autonomous aerial vehicles in a way that avoids catastrophic failure at training time. Guided policy search transforms RL into supervised learning, where the final control policy is trained with supervised learning, and the supervision is provided by an optimal control algorithm. Typically, this optimal control algorithm is an offline trajectory optimization procedure, which either assumes a known model of the dynamics or uses an iteratively learned model. Both approaches are prone to failure during training, since the known model may be inaccurate, and the learned model is always inaccurate during the early stages of learning. By substituting MPC for offline trajectory optimization, we can obtain variant of guided policy search that is robust to moderate model errors, and thus avoid catastrophic failures during training. Furthermore, since the final policy is trained with supervised learning, we can obtain complex, high-dimensional, and highly nonlinear policies, such as deep neural networks, which can represent a wide range of complex behaviors. One might wonder why the guided policy search method is necessary if we already have access to an effective MPC procedure. In the case of autonomous aerial vehicles, training deep neural network policies with guided policy search affords us two main advantages. First, the neural network policy does not need to use the same inputs as MPC. In fact, we can restrict its inputs to only those observations that are directly available from the vehicle’s onboard sensors, such as IMU readings and data from laser range finders, while the MPC training phase uses the true state of the system. Since the policy is represented by a deep neural network, it can even process complex, raw sensor information. For example, prior work has shown that guided policy search can learn policies that directly use camera images. Since MPC is only used at training time, we can employ an instrumented training setup, where the full state is known at training time (e.g. using motion capture), but unavailable at test time. This instrumented training setup is one of the key benefits of our approach, since it allows for safe training with full state information, but still produces a policy that uses raw sensor readings and does not require explicit state estimation. The second benefit of this approach is that the final neural network policy is computationally much less expensive than MPC, and can be easily parallelized on specialized hardware. This advantage combines elegantly with the instrumented training setup, since the MPC solution can be computed offboard during training, while all policy computations may be performed onboard at test time."
	"Our main contribution is an MPC-guided policy search algorithm that can be used for learning control policies for autonomous aerial vehicles. This algorithm replaces the offline trajectory optimization that is typically used in guided policy search with online MPC, which continuously replans paths to the goal from the vehicle’s current state using an approximate model of the dynamics. Our modified MPC procedure also takes into account the actions that would be taken at each state by the current neural network in order to avoid actions that the network is unlikely to take. This ensures that, at convergence, the neural network achieves good long-horizon performance, despite being trained only with supervised learning. Our approach allows us to learn neural network policies that directly process raw observations from the vehicle’s onboard sensors, and are substantially faster to evaluate at test time than full MPC solutions."
	"Model predictive control is an effictive and popular technique for control of robotic systems, and is frequently used to control autonomous aerial vehicles such as quadrotors. MPC is straightforward to apply when the state of the system is known (e.g. via a motion capture system), or when it can be measured accurately through sensors with well-understood observation models. However, vehicles navigating complex, unstructured environments must use more complex sensors, such as cameras and laser range finders. Incorporating such sensors into optimal control directly is challenging, since the sensor reading depends on a complex and often unknown environment. This challenge is conventionally addressed by using localization and mapping algorithms to map out the environment, and then optimizing trajectories under the resulting map. However, this kind of model-based approach is quite challenging when the vehicle is moving at high speed, or when onboard computation is limited. On the other end of the spectrum from such model-based methods, reinforcement learning aims to directly learn control policies that map observations to controls. This approach in principle removes the need for explicit state estimation and extensive computation at test time, by using a number of training episodes to iteratively improve the policy from real-world experience. However, model-free RL is difficult to apply to underactuated systems such as quadrotors, due to the possibility of catastrophic failure during training. Model-based RL can mitigate this problem by training a model from real-world experience, and then optimizing the policy under this model. While such methods have been successfully applied to aerial vehicles, the requirement to be able to acquire an accurate model means that these methods share many of the challenges of MPC methods."
	"In this work, we use an off-policy reinforcement learning method called guided policy search, which incorporates the advantages of model-based methods at training time, while still training the policy to use only the onboard sensors of the robot, without explicit state estimation and using only real-world data. Guided policy search has been applied to locomotion, robotic manipulation, and vision-based robotic control, but all prior applications rely on an offline trajectory optimization phase to generate the controller that is then executed on the real system. While this offline optimization might use a learned model of the system dynamics, the resulting trajectory-centric controller is only adapted between episodes. This makes these methods liable to fail catastrophically when the model is inaccurate. We replace the offline trajectory optimization in guided policy search with MPC, which prevents catastrophic failures even during training, making the method suitable for learning policies for autonomous aerial vehicles."
	-- http://rll.berkeley.edu/icra2016mpcgps (demo)

Kahn, Zhang, Levine, Abbeel - "PLATO: Policy Learning using Adaptive Trajectory Optimization" [http://arxiv.org/abs/1603.00622]
	"Policy search can in principle acquire complex strategies for control of robots, self-driving vehicles, and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when training must be done for safety-critical systems. We propose PLATO, an algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy, in order to generate training samples at states that are likely to be visited by the policy while avoiding highly undesirable on-policy actions. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy, and empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Compared to prior methods, our empirical results demonstrate that PLATO learns faster and often converges to a better solution on a set of challenging simulated experiments involving autonomous aerial vehicles."
	"In this paper, we presented PLATO, an algorithm for learning complex, high-dimensional policies that combine perception and control into a single expressive function approximator, such as a deep neural network. PLATO uses a trajectory optimization teacher to provide supervision to a standard supervised learning algorithm, allowing for a simple and data-efficient learning method. The teacher adapts to the behavior of the neural network policy to ensure that the distribution over states and observations is sufficiently close to the learned policy, allowing for a bound on the long-term performance of the learned policy. Our empirical evaluation, conducted on a set of challenging simulated quadrotor domains, demonstrates that PLATO outperforms a number of previous methods, both in terms of the robustness and effectiveness of the final policy, and in terms of the safety of the training procedure. PLATO has two key advantages that make it well-suited for learning control policies for real-world robotic systems. First, since the learned neural network policy does not need to be executed at training time, the method benefits from the robustness of model-predictive control (MPC), minimizing catastrophic failures at training time. This is particularly important when the distribution over training states and observations is non-stationary, as in the canyon/forest switching scenario. Here, methods that execute the learned policy, such as DAgger, can suffer a catastrophic failure when the agent encounters observations that are too different from those seen previously. Mitigating these issues typically requires hand-designed safety mechanisms, while PLATO automatically switches to a more off-policy behavior. The second advantage of PLATO is that the learned policy can use a different set of observations than MPC. Effective use of MPC requires observing or inferring the full state of the system, which might be accomplished, for instance, by instrumenting the environment with motion capture, or using a known map with relocalization (Williams et al., 2007). The policy, however, can be trained directly on raw input from onboard sensors, forcing it to perform both perception and control. Once trained, such a policy can be used in uninstrumented natural environments. PLATO shares these benefits with recently developed guided policy search algorithms (Zhang et al., 2016). However, in contrast with guided policy search, PLATO does not require a carefully designed episodic environment. In fact, the policy can be learned equally well for infinite horizon tasks without any episodic structure or reset mechanism, making it practical, for example, for learning navigation policies in complex environments. One of the most appealing prospects of learning expressive neural network policies with an automated MPC teacher is the possibility of acquiring real-world policies that directly use rich sensory inputs, such as camera images, depth sensors, and other inputs that are difficult to process with standard model-based techniques. Because of this, one very interesting avenue for future work is to apply PLATO on real physical platforms, especially ones equipped with novel and unusual sensors."
	-- http://youtube.com/watch?v=clHp6QgVyAU (demo)

Mordatch, Mishra, Eppner, Abbeel - "Combining Model-Based Policy Search with Online Model Learning for Control of Physical Humanoids" [http://www.eecs.berkeley.edu/~igor.mordatch/darwin/paper.pdf]
	"We present an automatic method for interactive control of physical humanoid robots based on high-level tasks that does not require manual specification of motion trajectories or specially-designed control policies. The method is based on the combination of a model-based policy that is trained off-line in simulation and sends high-level commands to a model-free controller that executes these commands on the physical robot. This low-level controller simultaneously learns and adapts a local model of dynamics on-line and computes optimal controls under the learned model. The high-level policy is trained using a combination of trajectory optimization and neural network learning, while considering physical limitations such as limited sensors and communication delays. The entire system runs in real-time on the robot’s computer and uses only on-board sensors. We demonstrate successful policy execution on a range of tasks such as leaning, hand reaching, and robust balancing behaviors atop a tilting base on the physical robot and in simulation."
	"In this paper, we presented an automatic method for interactive control of physical humanoid robots and demonstrated its effectiveness on a range of tasks. In the future, we wish to further increase the range of tasks to those that have been previously demonstrated in simulation, such as walking, getting up, and object manipulation. Our method is able to operate using only the lowperformance computer and sensors available on board the robot. This allows the possibility of the robot being deployed in unstructured outdoor environments in the future. However, the sensors we currently use are quite minimal and noisy, precluding the execution of more sophisticated policies, such as titling base balancing or walking. We are looking to augment on-board sensors with commercially available foot sensors and incorporate the robot’s camera information into our policies. While linear dynamics models have worked successfully as local models in the experiments we attempted, it would be interesting to explore other model classes, such as neural network dynamics models trained on-line."
	-- http://technologyreview.com/news/542921/robot-toddler-learns-to-stand-by-imagining-how-to-do-it/ (demo)

Mordatch, Lowrey, Andrew, Popovic, Todorov - "Interactive Control of Diverse Complex Characters with Neural Networks" [http://www.eecs.berkeley.edu/~igor.mordatch/policy/paper.pdf]
	"We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks – swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions."
	"Interactive real-time controllers that are capable of generating complex, stable and realistic movements have many potential applications including robotic control, animation and gaming. They can also serve as computational models in biomechanics and neuroscience. Traditional methods for designing such controllers are time-consuming and largely manual, relying on motion capture datasets or task-specific state machines. Our goal is to automate this process, by developing universal synthesis methods applicable to arbitrary behaviors, body morphologies, online changes in task objectives, perturbations due to noise and modeling errors. This is also the ambitious goal of much work in Reinforcement Learning and stochastic optimal control, however the goal has rarely been achieved in continuous high-dimensional spaces involving complex dynamics."
	"Specifically, we combine supervised learning with trajectory optimization, namely Contact-Invariant Optimization, which has given rise to some of the most elaborate motor behaviors synthesized automatically. Trajectory optimization however is an offline method, so the rationale here is to use a neural network to learn from the optimizer, and eventually generate similar behaviors online. There is closely related recent work along these lines, but the method presented here solves substantially harder problems – in particular it yields stable and realistic locomotion in three-dimensional space, where previous work was applied to only two-dimensional characters. The data needed to learn neural network controllers is much harder to obtain, and in the case of imaginary characters and novel robots we have to synthesize the training data ourselves (via trajectory optimization). At the same time the learning task for the network is harder. This is because we need precise real-valued outputs as opposed to categorical outputs, and also because our network must operate not on i.i.d. samples, but in a closed loop, where errors can amplify over time and cause instabilities. This necessitates specialized training procedures where the dataset of trajectories and the network parameters are optimized together. Another challenge caused by limited datasets is the potential for over-fitting and poor generalization. Our solution is to inject different forms of noise during training. The scale of our problem requires cloud computing and a GPU implementation, and training that takes on the order of hours. Interestingly, we invest more computing resources in generating the data than in learning from it. Thus the heavy lifting is done by the trajectory optimizer, and yet the neural network complements it in a way that yields interactive real-time control. Neural network controllers can also be trained with more traditional methods which do not involve trajectory optimization. A systematic comparison of these more direct methods with the present trajectory-optimization-based methods remains to be done. Nevertheless our impression is that networks trained with direct methods give rise to successful yet somewhat chaotic behaviors, while the present class of methods yield more realistic and purposeful behaviors. Using physics based controllers allows for interaction, but these controllers need specially designed architectures for each range of tasks or characters. For example, for biped location common approaches include state machines and use of simplified models (such as the inverted pendulum) and concepts (such as zero moment or capture points). For quadrupedal characters, a different set of state machines, contact schedules and simplified models is used. For flying and swimming yet another set of control architectures, commonly making use of explicit cyclic encodings, have been used. It is our aim to unify these disparate approaches."
	-- http://youtube.com/watch?&v=IxrnT0JOs4o (demo)
	-- http://research.microsoft.com/apps/video/default.aspx?id=259609 (Mordatch)




[interesting papers - inverse reinforcement learning]

Bogdanovic, Markovikj, Denil, Freitas - "Deep Apprenticeship Learning for Playing Video Games" [https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10159]
	"Recently it has been shown that deep neural networks can learn to play Atari games by directly observing raw pixels of the playing area. We show how apprenticeship learning can be applied in this setting so that an agent can learn to perform a task (i.e. play a game) by observing the expert, without any explicitly provided knowledge of the game’s internal state or objectives."

Wulfmeier, Ondruska, Posner - "Maximum Entropy Deep Inverse Reinforcement Learning" [http://arxiv.org/abs/1507.04888]
	"This paper presents a general framework for employing deep architectures - in particular neural networks - to solve the inverse reinforcement learning (IRL) problem. Specifically, we propose to exploit the representational capacity and favourable computational complexity of deep networks to approximate complex, nonlinear reward functions. We show that the Maximum Entropy paradigm for IRL lends itself naturally to the efficient training of deep architectures. At test time, the approach leads to a computational complexity independent of the number of demonstrations. This makes it especially well-suited for applications in life-long learning scenarios commonly encountered in robotics. We demonstrate that our approach achieves performance commensurate to the state-of-the-art on existing benchmarks already with simple, comparatively shallow network architectures while significantly outperforming the state-of-the-art on an alternative benchmark based on more complex, highly varying reward structures representing strong interactions between features. Furthermore, we extend the approach to include convolutional layers in order to eliminate the dependency on precomputed features of current algorithms and to underline the substantial gain in flexibility in framing IRL in the context of deep learning."

Finn, Levine, Abbeel - "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization" [https://arxiv.org/abs/1603.00448]
	"Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency."
	--
	"technique that lets one apply Maximum Entropy Inverse Optimal Control without the double-loop procedure and using policy gradient techniques"
	-- https://youtube.com/watch?v=hXxaepw0zAw (demo)
	-- http://techtalks.tv/talks/guided-cost-learning-deep-inverse-optimal-control-via-policy-optimization/62472/

Ho, Gupta, Ermon - "Model-Free Imitation Learning with Policy Optimization" [http://arxiv.org/abs/1605.08478]
	"In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima."
	"We showed that carefully blending state-of-the-art policy gradient algorithms for reinforcement learning with local cost function fitting lets us successfully train neural network policies for imitation in high-dimensional, continuous environments. Our method is able to identify a locally optimal solution, even in settings where optimal planning is out of reach. This is a significant advantage over competing algorithms that require repeatedly solving planning problems in an inner loop. In fact, when the inner planning problem is only approximately solved, competing algorithms do not even provide local optimality guarantees (Ermon et al., 2015). Our approach does not use expert interaction or reinforcement signal, fitting in a family of such approaches that includes apprenticeship learning and inverse reinforcement learning. When either of these additional resources is provided, alternative approaches (Kim et al., 2013; Daume III et al., 2009; Ross & Bagnell, 2010; Ross et al., 2011) may be more sample efficient, and investigating ways to combine these resources with our framework is an interesting research direction. We focused on the policy optimization component of apprenticeship learning, rather than the design of appropriate cost function classes. We believe this is an important area for future work. Nonlinear cost function classes have been successful in IRL (Ratliff et al., 2009; Levine et al., 2011) as well as in other machine learning problems reminiscent of ours, in particular that of training generative image models. In the language of generative adversarial networks (Goodfellow et al., 2014), the policy parameterizes a generative model of state-action pairs, and the cost function serves as an adversary. Apprenticeship learning with large cost function classes capable of distinguishing between arbitrary state-action visitation distributions would, enticingly, open up the possibility of exact imitation."
	-- http://techtalks.tv/talks/model-free-imitation-learning-with-policy-optimization/62471/

Ho, Ermon - "Generative Adversarial Imitation Learning" [http://arxiv.org/abs/1606.03476]
	"Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert’s cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments."
	"As we demonstrated, our method is generally quite sample efficient in terms of expert data. However, it is not particularly sample efficient in terms of environment interaction during training. The number of such samples required to estimate the imitation objective gradient was comparable to the number needed for TRPO to train the expert policies from reinforcement signals. We believe that we could significantly improve learning speed for our algorithm by initializing policy parameters with behavioral cloning, which requires no environment interaction at all. Fundamentally, our method is model free, so it will generally need more environment interaction than model-based methods. Guided cost learning, for instance, builds upon guided policy search and inherits its sample efficiency, but also inherits its requirement that the model is well-approximated by iteratively fitted time-varying linear dynamics. Interestingly, both our Algorithm 1 and guided cost learning alternate between policy optimization steps and cost fitting (which we called discriminator fitting), even though the two algorithms are derived completely differently. Our approach builds upon a vast line of work on IRL, and hence, just like IRL, our approach does not interact with the expert during training. Our method explores randomly to determine which actions bring a policy’s occupancy measure closer to the expert’s, whereas methods that do interact with the expert, like DAgger, can simply ask the expert for such actions. Ultimately, we believe that a method that combines well-chosen environment models with expert interaction will win in terms of sample complexity of both expert data and environment interaction."
	""Popular imitation approaches involve a two-stage pipeline: first learning a reward function, then running RL on that reward. Such a pipeline can be slow, and because it’s indirect, it is hard to guarantee that the resulting policy works well. This work shows how one can directly extract policies from data via a connection to GANs. As a result, this approach can be used to learn policies from expert demonstrations (without rewards)."
	-- https://github.com/openai/imitation

Hadfield-Menell, Dragan, Abbeel, Russell - "Cooperative Inverse Reinforcement Learning" [http://arxiv.org/abs/1606.03137]
	"For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning. A CIRL problem is a cooperative, partial information game with two agents, human and robot; both are rewarded according to the human’s reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm."
	"In this work, we presented a game-theoretic model for cooperative learning, CIRL. Key to this model is that the robot knows that it is in a shared environment and is attempting to maximize the human’s reward (as opposed to estimating the human’s reward function and adopting it as its own). This leads to cooperative learning behavior and provides a framework in which to design HRI algorithms and analyze the incentives of both actors in a learning environment. We reduced the problem of computing an optimal policy pair to solving a POMDP. This is a useful theoretical tool and can be used to design new algorithms, but it is clear that optimal policy pairs are only part of the story. In particular, when it performs a centralized computation, the reduction assumes that we can effectively program both actors to follow a set coordination policy. This may not be feasible in reality, although it may nonetheless be helpful in training humans to be better teachers. An important avenue for future research will be to consider the problem of equilibrium acquisition: the process by which two independent actors arrive at an equilibrium pair of policies. Returning to Wiener’s warning, we believe that the best solution is not to put a specific purpose into the machine at all, but instead to design machines that provably converge to the right purpose as they go along."




<brylevkirill (at) gmail.com>
