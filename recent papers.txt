  recent interesting papers:
  * theory
  * architectures
  * meta-learning
  * unsupervised learning
  * generative models
    - generative adversarial networks
    - variational autoencoders
    - autoregressive models
  * probabilistic inference
  * reasoning
  * program induction
  * reinforcement learning
    - algorithms
    - exploration
    - abstractions for states and actions
    - planning
    - transfer
    - imitation
    - memory
    - multi-agent
    - applications
  * dialog systems
  * natural language processing


  selected papers - https://github.com/brylevkirill/notes




[theory]

http://arxiv.org/abs/1611.09913 - Capacity and Trainability in Recurrent Neural Networks (Google Brain)
http://arxiv.org/abs/1611.03530 - Understanding Deep Learning Requires Rethinking Generalization (Google Brain)
	"1. The effective capacity of neural networks is large enough for a brute-force memorization of the entire data set.
	 2. Even optimization on random labels remains easy. In fact, training time increases only by a small constant factor compared with training on the true labels.
	 3. Randomizing labels is solely a data transformation, leaving all other properties of the learning problem unchanged."
	"It is likely that learning in the traditional sense still occurs in part, but it appears to be deeply intertwined with massive memorization. Classical approaches are therefore poorly suited for reasoning about why these models generalize well."
	"Deep Learning networks are just massive associative memory stores! Deep Learning networks are capable of good generalization even when fitting random data. This is indeed strange in that many arguments for the validity of Deep Learning is on the conjecture that ‘natural’ data tends to exists in a very narrow manifold in multi-dimensional space. Random data however does not have that sort of tendency."




[architectures]

http://www.nature.com.sci-hub.cc/nature/journal/vaop/ncurrent/full/nature20101.html - Hybrid Computing using a Neural Network with Dynamic External Memory (DeepMind)
	-- https://deepmind.com/blog/differentiable-neural-computers/
	-- https://youtube.com/watch?v=PQrlOjj8gAc (Wayne)
	-- https://youtu.be/otRoAQtc5Dk?t=59m56s (Polykovskiy)
	-- https://github.com/yos1up/DNC
	-- https://github.com/Mostafa-Samir/DNC-tensorflow
	-- https://github.com/frownyface/dnc
	-- https://github.com/khaotik/dnc-theano
http://arxiv.org/abs/1610.09027 - Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes (DeepMind)  # improved differentiable neural computer

http://arxiv.org/abs/1602.03032 - Associative Long Short-Term Memory (Graves)
	-- http://techtalks.tv/talks/associative-long-short-term-memory/62525/ (Danihelka)
	-- http://www.cogsci.ucsd.edu/~sereno/170/readings/06-Holographic.pdf
	-- https://github.com/mohammadpz/Associative_LSTM
http://arxiv.org/abs/1605.07427 - Hierarchical Memory Networks (Bengio)
http://arxiv.org/abs/1607.00036 - Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes (Bengio)
http://arxiv.org/abs/1610.06258 - Using Fast Weights to Attend to the Recent Past (Hinton)  # alternative to LSTM
	https://drive.google.com/file/d/0B8i61jl8OE3XdHRCSkV1VFNqTWc (Hinton) : "It's a different approach to a Neural Turing Machine. It does not require any decisions about where to write stuff or where to read from. Anything that happened recently can automatically be retrieved associatively. Fast associative memory should allow neural network models of sequential human reasoning."
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Using-Fast-Weights-to-Attend-to-the-Recent-Past (Ba)
	-- http://www.fields.utoronto.ca/talks/title-tba-337 (Hinton)
	-- https://youtube.com/watch?v=mrj_hyH974o (Novikov, in russian)
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1610.06258
	-- https://reddit.com/r/MachineLearning/comments/58qjiw/research161006258_using_fast_weights_to_attend_to/d92kctk/
	-- https://theneuralperspective.com/2016/12/04/implementation-of-using-fast-weights-to-attend-to-the-recent-past/
	-- https://github.com/ajarai/fast-weights
	-- https://github.com/jxwufan/AssociativeRetrieval

http://openreview.net/forum?id=B1ckMDqlg - Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Google Brain)
	-- https://github.com/dennybritz/deeplearning-papernotes/blob/3718d181a0fed5ed806582822ed0dbde530122bf/notes/mixture-experts.md
http://arxiv.org/abs/1612.00796 - Overcoming Catastrophic Forgetting in Neural Networks (DeepMind)

http://arxiv.org/abs/1608.05343 - Decoupled Neural Interfaces using Synthetic Gradients (DeepMind)
	"At the very least it can allow individual modules to do gradient updates before waiting for the backward pass to reach them. So you could get better GPGPU utilization when the ordinary 'locked' mode of forward-then-backward doesn't always saturate the available compute units.
	Put differently, if you consider the dependency DAG of tensor operations, using these DNI things reduces the depth of the parameter gradient nodes (which is the whole point of training) in the DAG. So for example, the gradient update for the layer at the beginning of a n-layer chain goes from depth ~2n to depth ~1, the layer at the end has depth n, which doesn't change. On average, the depth of the gradient computation nodes is about 40% of what it would be normally, for deep networks. So there is a lot more flexibility for scheduling nodes in time and space.
	And for coarser-grained parallelism it could allow modules running on different devices to do updates before a final loss gradient is available to be distributed to all the devices. Synchronization still has to happen to update the gradient predictions, but that can happen later, and could even be opportunistic (asynchronous or stochastic)."
	"I guess that the synthetic gradients conditioned on the labels and the synthetic layer inputs conditioned on the data work for the same reason why stochastic depth works: during training, at any given layer the networks before and after it can be approximated by simpler, shallower versions. In stochastic depth the approximation is performed by skipping layers, so the whole network is approximated by a shallower version of itself, which changes at each step. In this work, instead, the approximation is performed by separate networks.
	-- https://deepmind.com/blog/decoupled-neural-networks-using-synthetic-gradients/
	-- http://cnichkawde.github.io/SyntheticGradients.html
http://arxiv.org/abs/1701.08734 - PathNet: Evolution Channels Gradient Descent in Super Neural Networks (DeepMind)

http://arxiv.org/abs/1609.01704 - Hierarchical Multiscale Recurrent Neural Networks (Bengio)
	-- https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/hm-rnn.md
	-- https://medium.com/@jimfleming/notes-on-hierarchical-multiscale-recurrent-neural-networks-7362532f3b64
http://arxiv.org/abs/1610.09513 - Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences
	"If you take an LSTM and add a “time gate” that controls at what frequency to be open to new input and how long to be open each time, you can have different neurons that learn to look at a sequence with different frequencies, create a “wormhole” for gradients, save compute, and do better on long sequences and when you need to process inputs from multiple sensors that are sampled at different rates."
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Phased-LSTM-Accelerating-Recurrent-Network-Training-for-Long-or-Event-based-Sequences (Neil)
	-- https://github.com/dannyneil/public_plstm

http://arxiv.org/abs/1607.06450 - Layer Normalization
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-2 (Ba, 23:27)
http://arxiv.org/abs/1602.07868 - Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks (Kingma)
	"Batch normalization adds noise to gradient updates. While noise is probably good when one is training images with CNNs because it adds a bit of regularization (e.g., it can take care of invariances and stuff that doesn’t matter), it’s not so useful when one wants to do reinforcement learning. Instead of normalizing batches, he does normalization directly onto the weights, the contributions being: weight normalization + data dependent initialization. Scores on reinforcement learning with DQN gets better, sometimes 30% more."
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Weight-Normalization-A-Simple-Reparameterization-to-Accelerate-Training-of-Deep-Neural-Networks
	-- https://youtu.be/mrj_hyH974o?t=1h46m43s (Lobacheva, in russian)
	-- https://github.com/openai/weightnorm

http://arxiv.org/abs/1603.08983 - Adaptive Computation Time for Recurrent Neural Networks (Graves)
	-- http://distill.pub/2016/augmented-rnns/
	-- https://www.evernote.com/shard/s189/sh/fd165646-b630-48b7-844c-86ad2f07fcda/c9ab960af967ef847097f21d94b0bff7
	-- https://github.com/DeNeutoy/act-tensorflow
http://arxiv.org/abs/1606.03401 - Memory-Efficient Backpropagation Through Time (Graves)

http://arxiv.org/abs/1612.02879 - Crossprop: Learning Representations through Stochastic Gradient Descent in Cross-Validation Error (Sutton)




[meta-learning]

http://arxiv.org/abs/1606.04474 - Learning to Learn by Gradient Descent by Gradient Descent
	"Take some computation where you usually wouldn’t keep around intermediate states, such as a planning computation (say value iteration, where you only keep your most recent estimate of the value function) or stochastic gradient descent (where you only keep around your current best estimate of the parameters). Now keep around those intermediate states as well, perhaps reifying the unrolled computation in a neural net, and take gradients to optimize the entire computation with respect to some loss function. Instances: Value Iteration Networks, Learning to learn by gradient descent by gradient descent."
	-- https://youtu.be/tPWGGwmgwG0?t=10m50s (de Freitas)
	-- https://youtu.be/x1kf4Zojtb0?t=1h4m53s (de Freitas)
	-- https://blog.acolyer.org/2017/01/04/learning-to-learn-by-gradient-descent-by-gradient-descent/
	-- https://github.com/deepmind/learning-to-learn
http://arxiv.org/abs/1611.03824 - Learning to Learn for Global Optimization of Black Box Functions (de Freitas)
https://openreview.net/forum?id=rJY0-Kcll - Optimization as a Model for Few-Shot Learning (Larochelle)

http://arxiv.org/abs/1611.02779 - RL^2: Fast Reinforcement Learning via Slow Reinforcement Learning (OpenAI, Abbeel)
	"future directions:
	- better outer-loop algorithms
	- scaling RL^2 to 1M games
	- model-based RL^2
	- curriculum learning / universal RL^2
	- RL^2 + one-shot imitation learning
	- RL^2 for simulation -> real world transfer"
	-- https://youtu.be/19eNQ1CLt5A?t=7m52s (Sutskever)
http://arxiv.org/abs/1611.05763 - Learning to Reinforcement Learn (DeepMind)
	-- https://hackernoon.com/learning-policies-for-learning-policies-meta-reinforcement-learning-rl²-in-tensorflow-b15b592a2ddf (Juliani)
	-- https://github.com/awjuliani/Meta-RL

http://arxiv.org/abs/1603.05106 - One-Shot Generalization in Deep Generative Models
	"move over DRAW: deepmind's latest has spatial-transform attention and 1-shot generalization"
	-- http://techtalks.tv/talks/one-shot-generalization-in-deep-generative-models/62365/
	-- https://youtu.be/XpIDCzwNe78?t=43m (Bartunov)
http://arxiv.org/abs/1606.02185 - Towards a Neural Statistician
	-- http://techtalks.tv/talks/neural-statistician/63048/ (Edwards)
	-- https://youtu.be/XpIDCzwNe78?t=51m53s (Bartunov)
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1606.02185
http://arxiv.org/abs/1606.04080 - Matching Networks for One Shot Learning (Vinyals)
	"Given just a few, or even a single, examples of an unseen class, it is possible to attain high classification accuracy on ImageNet using Matching Networks.  The core architecture is simple and straightforward to train and performant across a range of image and text classification tasks. Matching Networks are trained in the same way as they are tested: by presenting a series of instantaneous one shot learning training tasks, where each instance of the training set is fed into the network in parallel. Matching Networks are then trained to classify correctly over many different input training sets. The effect is to train a network that can classify on a novel data set without the need for a single step of gradient descent."
	-- https://pbs.twimg.com/media/Cy7Eyh5WgAAZIw2.jpg:large
	-- https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/
http://arxiv.org/abs/1612.02192 - Fast Adaptation in Generative Models with Generative Matching Networks (Bartunov)
	-- https://youtu.be/XpIDCzwNe78 (Bartunov)
	-- http://github.com/sbos/gmn

http://arxiv.org/abs/1609.09106 - HyperNetworks (Google Brain)
	-- http://blog.otoro.net/2016/09/28/hyper-networks/
	"Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks."
	"Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters."
http://arxiv.org/abs/1611.01578 - Neural Architecture Search with Reinforcement Learning (Google Brain)
http://arxiv.org/abs/1611.02167 - Designing Neural Network Architectures using Reinforcement Learning




[unsupervised learning]

http://openreview.net/forum?id=Sy2fzU9gl - beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework (Mohamed)  # VAE which learns disentangled representations
http://arxiv.org/abs/1604.08772 - Towards Conceptual Compression (Gregor)
	-- https://pbs.twimg.com/media/Cy3pYfWWIAA_C9h.jpg:large
http://arxiv.org/abs/1603.08575 - Attend, Infer, Repeat: Fast Scene Understanding with Generative Models (Hinton)
	"The latent variables can be a list or set of vectors."
	"Consider the task of clearing a table after dinner. To plan your actions you will need to determine which objects are present, what classes they belong to and where each one is located on the table. In other words, for many interactions with the real world the perception problem goes far beyond just image classification. We would like to build intelligent systems that learn to parse the image of a scene into objects that are arranged in space, have visual and physical properties, and are in functional relationships with each other. And we would like to do so with as little supervision as possible. Starting from this notion our paper presents a framework for efficient inference in structured, generative image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network."
http://arxiv.org/abs/1606.05579 - Early Visual Concept Learning with Unsupervised Deep Learning
	-- http://tinyurl.com/jgbyzke
	-- https://github.com/loliverhennigh/Early-Visual-Concept-Learning-Recreation-of-Some-Results
	"This paper has quite a big flaws on the motivational part. They introduce a VAE with temperature. First, that exists from before, it has been tested and so on in other papers. Secondly, they frame it as though disentanglement somehow implies a metric with a prior. No actual evidence with this. Also they try to introduce this neuroscience idea, but I did not read (although I skimmed it only) any real neuroscience evidence that the brain does measures a KL. It seems very convenient to "decide" that this is the right constrained, because it gives you back a VAE, which we already know works 100 times. To me it seems they did it backwards, they new the VAE works and just tried to frame this visual ventral stream thing somehow to imply that the VAE is the correct thing to do, but only by hand waving. Why not for instance try to minimize the entropy of Q only, that makes a lot more sense for disentanglement. Also, we know since forever that sampling the manifold more densely gives you better result, that whole section is pretty much useless. Also, they don't compare to other models, which try to do disentanglement explicitly, one could wonder why. Additionally, a shortcoming of full disentanglement is multimodality, which they did not comment at all, and my guess is because actually VAE will never be able to do that. The only take away for me of this paper is the results, which show some nice features of VAE, however from more natural images we know that VAE does not work so well on that."
	-- https://youtube.com/watch?v=4tc84kKdpY4
	-- http://arkitus.com/attend-infer-repeat/
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/EslamiHWTKH16 (Larochelle)
http://arxiv.org/abs/1702.04649 - Generative Temporal Models with Memory (DeepMind)
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1702.04649

http://arxiv.org/abs/1611.07492 - Inducing Interpretable Representations with Variational Autoencoders (Goodman)
http://arxiv.org/abs/1611.02648 - Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders (Arulkumaran)
	-- http://ruishu.io/2016/12/25/gmvae/
http://arxiv.org/abs/1611.03383 - Disentangling Factors of Variation in Deep Representations using Adversarial Training (LeCun)
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1611.03383

http://arxiv.org/abs/1605.08803 - Density Estimation using Real NVP
	"Most interestingly, it is the only powerful generative model I know that combines A) a tractable likelihood, B) an efficient / one-pass sampling procedure and C) the explicit learning of a latent representation."
	-- http://www-etud.iro.umontreal.ca/~dinhlaur/real_nvp_visual/ (demo)
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Phased-LSTM-Accelerating-Recurrent-Network-Training-for-Long-or-Event-based-Sequences (08:19, Dinh) + https://docs.google.com/presentation/d/152NyIZYDRlYuml5DbBONchJYA7AAwlti5gTWW1eXlLM/
	-- https://periscope.tv/hugo_larochelle/1ypKdAVmbEpGW (Dinh)
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1605.08803
	-- https://github.com/tensorflow/models/tree/master/real_nvp
	-- https://github.com/taesung89/real-nvp




[generative models]

http://arxiv.org/abs/1511.05101 - How (not) to train your generative model: schedule sampling, likelihood, adversary (Huszar)
	-- http://inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/
http://arxiv.org/abs/1511.01844 - A Note of the Evaluation of Generative Models
	-- http://videolectures.net/iclr2016_theis_generative_models/ (Theis)
	-- https://pbs.twimg.com/media/CjA02jrWYAECWOZ.jpg:large ("The generative model on the left gets a better log-likelihood score.")
http://arxiv.org/abs/1611.04273 - On the Quantitative Analysis of Decoder-based Generative Models (Salakhutdinov)
	-- https://github.com/tonywu95/eval_gen
	"We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution."

http://arxiv.org/abs/1610.08123 - Socratic Learning: Empowering the Generative Model (Re)  # learning with minimal supervision
	-- https://youtube.com/watch?v=0gRNochbK9c




[generative models - generative adversarial networks]

http://arxiv.org/abs/1701.00160 - NIPS 2016 Tutorial: Generative Adversarial Networks (Goodfellow)
        -- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks (Goodfellow) + http://iangoodfellow.com/slides/2016-12-04-NIPS.pdf

http://sites.google.com/site/nips2016adversarial/WAT16_paper_21.pdf - A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models (Abbeel, Levine)
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-3 (33:17, Levine)
	-- https://youtu.be/RZOKRFBtSh4?t=10m48s (Finn)

http://arxiv.org/abs/1702.08235 - Variational Inference using Implicit Distributions (Huszar)
	"This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more."
	-- http://inference.vc/variational-inference-with-implicit-probabilistic-models-part-1-2/
	-- http://inference.vc/variational-inference-with-implicit-models-part-ii-amortised-inference-2/
	-- http://inference.vc/variational-inference-using-implicit-models-part-iii-joint-contrastive-inference-ali-and-bigan/
	-- http://inference.vc/variational-inference-using-implicit-models-part-iv-denoisers-instead-of-discriminators/
http://arxiv.org/abs/1610.03483 - Learning in Implicit Generative Models (Mohamed)
http://arxiv.org/abs/1610.06545 - Revisiting Classifier Two-Sample Tests for GAN Evaluation and Causal Discovery (Facebook)
http://arxiv.org/abs/1606.00709 - f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization
	"Shows how to optimize many different objectives using adversarial training."
	-- https://youtube.com/watch?v=I1M_jGWp5n0
	-- https://youtube.com/watch?v=kQ1eEXgGsCU (Nowozin)
http://arxiv.org/abs/1612.02780 - Improved Generator Objectives for GANs
	"We present a framework to understand GAN training as alternating density ratio estimation and approximate divergence minimization. This provides an interpretation for the mismatched GAN generator and discriminator objectives often used in practice, and explains the problem of poor sample diversity. We also derive a family of generator objectives that target arbitrary f-divergences without minimizing a lower bound, and use them to train generative image models that target either improved sample quality or greater sample diversity."

http://openreview.net/forum?id=Hk4_qw5xe - Towards Principled Methods for Training Generative Adversarial Networks (Bottou)
https://arxiv.org/abs/1701.07875 - Wasserstein GAN (Bottou)
	"GAN loss that corresponds with image quality
	GAN loss that converges (decreasing loss actually means something), so you can actually tune your hyperparameters
	Stable GAN training, where generator nets without batch norm, silly layer architectures and even straight up MLPs can generate decent images
	Way less mode collapse
	Theory about why it works and why the old methods had the problems we experienced."
	"Paper uses Wasserstein distance instead of Jensen-Shannon divergence to compare distributions."
	"Paper gets rid of a few unnecessary logarithms, and clips weights."
	"There are two fundamental problems in doing image generation using GANs: 1) model structure 2) optimization instability. This paper makes no claims of improving model structure nor does it have experiments in that direction. To improve on imagenet generation, we need some work in (1) as well."
	"Having loss curves that actually make sense and reflect sample quality? Perhaps showing how you can have meaningful and stable training process without having to intentionally cripple (or undertrain) the discriminator/critic? Perhaps the fact that the authors show why the original GAN formulations (using KL/JS divergence) are problematic AND provide a solution for those problems."
	"Authors are not claiming that this directly improves image quality, but offers a host of other benefits like stability, the ability to make drastic architecture changes without loss of functionality, and, most importantly, a loss metric that actually appears to correlate with sample quality. That last one is a pretty big deal."
	"Wasserstein distance fits so nicely in the GAN framework so that the WGAN critic provide a natural lower bound on the EMD."
	--
	"Using Wasserstein objective reduces instability, but we still lack proof of existence of an equilibrium. Game theory doesn’t help because we need a so-called pure equilibrium, and simple counter-examples such as rock/paper/scissors show that it doesn’t exist in general. Such counterexamples are easily turned into toy GAN scenarios with generator and discriminator having finite capacity, and the game lacks a pure equilibrium."
	-- http://www.alexirpan.com/2017/02/22/wasserstein-gan.html
	-- https://paper.dropbox.com/doc/Wasserstein-GAN-GvU0p2V9ThzdwY3BbhoP7
	-- https://github.com/martinarjovsky/WassersteinGAN
	-- https://github.com/shekkizh/WassersteinGAN.tensorflow
	-- https://github.com/kuleshov/tf-wgan
	-- https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/WassersteinGAN
http://arxiv.org/abs/1611.02163 - Unrolled Generative Adversarial Networks
	"We introduce a method to stabilize GANs by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator."
http://arxiv.org/abs/1606.03498 - Improved Techniques for Training GANs
	-- https://youtu.be/RZOKRFBtSh4?t=26m18s (Metz)
	-- https://github.com/aleju/papers/blob/master/neural-nets/Improved_Techniques_for_Training_GANs.md
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2FSalimansGZCRC16
	-- http://inference.vc/understanding-minibatch-discrimination-in-gans/
	-- https://github.com/openai/improved-gan
	"Our CIFAR-10 samples also look very sharp - Amazon Mechanical Turk workers can distinguish our samples from real data with an error rate of 21.3% (50% would be random guessing)"
	"In addition to generating pretty pictures, we introduce an approach for semi-supervised learning with GANs that involves the discriminator producing an additional output indicating the label of the input. This approach allows us to obtain state of the art results on MNIST, SVHN, and CIFAR-10 in settings with very few labeled examples. On MNIST, for example, we achieve 99.14% accuracy with only 10 labeled examples per class with a fully connected neural network — a result that’s very close to the best known results with fully supervised approaches using all 60,000 labeled examples."
	--
	"Idea is to give discriminator an entire minibatch of samples as input, rather than just one sample. Thus the discriminator can tell whether the generator just constantly produces a single image. With the collapse discovered, gradients will be sent to the generator to correct the problem."
	-- https://github.com/poolio/unrolled_gan

http://arxiv.org/abs/1611.04051 - GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution
	-- https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html
http://arxiv.org/abs/1702.07983 - Maximum-Likelihood Augmented Discrete Generative Adversarial Networks (Bengio)
http://arxiv.org/abs/1702.08431 - Boundary-Seeking Generative Adversarial Networks (Bengio)
	"This approach can be used to train a generator with discrete output when the generator outputs a parametric conditional distribution. We demonstrate the effectiveness of the proposed algorithm with discrete image data. In contrary to the proposed algorithm, we observe that the recently proposed Gumbel-Softmax technique for re-parametrizing the discrete variables does not work for training a GAN with discrete data."

http://arxiv.org/abs/1605.09782 - Adversarial Feature Learning
http://arxiv.org/abs/1609.08661 - Task Specific Adversarial Cost Function
	-- https://github.com/ToniCreswell/piGAN
http://arxiv.org/abs/1612.04357 - Stacked Generative Adversarial Networks
	-- https://github.com/xunhuang1995/SGAN
http://openreview.net/forum?id=Byk-VI9eg - Generative Multi-Adversarial Networks
http://arxiv.org/abs/1701.02386 - AdaGAN: Boosting Generative Models  # addressing missing modes
http://www.stat.ucla.edu/~ywu/ABP/doc/arXivABP.pdf - Alternating Back-Propagation for Generator Network

https://sites.google.com/site/nips2016adversarial/WAT16_paper_20.pdf - Generating Text via Adversarial Training
	-- http://machinedlearnings.com/2017/01/generating-text-via-adversarial-training.html

http://arxiv.org/abs/1609.07093 - Neural Photo Editing with Introspective Adversarial Networks
	-- https://youtube.com/watch?v=FDELBFSeqQs (demo)
	-- https://github.com/ajbrock/Neural-Photo-Editor
http://arxiv.org/abs/1605.05396 - Generative Adversarial Text to Image Synthesis (Reed)
http://arxiv.org/abs/1610.09585 - Conditional Image Synthesis With Auxiliary Classifier GANs (Olah)
	-- https://pbs.twimg.com/media/CwM0BzjVUAAWTn4.jpg:large
	-- https://youtu.be/RZOKRFBtSh4?t=21m47s (Odena)
	-- https://github.com/buriburisuri/ac-gan
http://www.evolvingai.org/files/nguyen2016ppgn__v1.pdf - Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space
	-- https://pbs.twimg.com/media/Czpn0VLVEAA_RpK.jpg:large
	-- https://github.com/Evolving-AI-Lab/ppgn
http://arxiv.org/abs/1612.07828 - Learning from Simulated and Unsupervised Images through Adversarial Training (Apple)
	-- https://github.com/carpedm20/simulated-unsupervised-tensorflow
http://arxiv.org/abs/1612.05424 - Unsupervised Pixel-Level Domain Adaptation with Generative Asversarial Networks (Google Brain)
http://arxiv.org/abs/1611.07004 - Image-to-Image Translation with Conditional Adversarial Networks
	-- https://phillipi.github.io/pix2pix/
http://arxiv.org/abs/1703.00848 - Unsupervised Image-to-Image Translation Networks (NVIDIA)
http://arxiv.org/abs/1609.04468 - Sampling Generative Networks: Notes on a Few Effective Techniques (smilevector)
	-- https://github.com/dribnet/plat
http://arxiv.org/abs/1610.06918 - Learning to Protect Communications with Adversarial Neural Cryptography
	-- https://nlml.github.io/neural-networks/adversarial-neural-cryptography/
	-- https://blog.acolyer.org/2017/02/10/learning-to-protect-communications-with-adversarial-neural-cryptography/




[generative models - variational autoencoders]

http://arxiv.org/abs/1702.08658 - Towards a Deeper Understanding of Variational Autoencoding Models
	"We provide a formal explanation for why VAEs generate blurry samples when trained on complex natural images. We show that under some conditions, blurry samples are not caused by the use of a maximum likelihood approach as previously thought, but rather they are caused by an inappropriate choice for the inference distribution. We specifically target this problem by proposing a sequential VAE model, where we gradually augment the the expressiveness of the inference distribution using a process inspired by the recent infusion training process. As a result, we are able to generate sharp samples on the LSUN bedroom dataset, even using 2-norm reconstruction loss in pixel space."
	"We propose a new explanation of the VAE tendency to ignore the latent code. We show that this problem is specific to the original VAE objective function and does not apply to the more general family of VAE models we propose. We show experimentally that using our more general framework, we achieve comparable sample quality as the original VAE, while at the same time learning meaningful features through the latent code, even when the decoder is a powerful PixelCNN that can by itself model data."
http://arxiv.org/abs/1611.02731 - Variational Lossy Autoencoder (OpenAI)

http://arxiv.org/abs/1701.04722 - Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks
	-- https://gist.github.com/poolio/b71eb943d6537d01f46e7b20e9225149
	-- http://inference.vc/variational-inference-with-implicit-models-part-ii-amortised-inference-2/

http://arxiv.org/abs/1509.00519 - Importance Weighted Autoencoders (Burda)
	-- http://dustintran.com/blog/importance-weighted-autoencoders/
	-- https://github.com/yburda/iwae
	-- https://github.com/arahuja/generative-tf
	-- https://github.com/blei-lab/edward/blob/master/examples/iwvi.py
http://arxiv.org/abs/1602.06725 - Variational Inference for Monte Carlo Objectives
	-- http://techtalks.tv/talks/variational-inference-for-monte-carlo-objectives/62507/
	-- https://evernote.com/shard/s189/sh/54a9fb88-1a71-4e8a-b0e3-f13480a68b8d/0663de49b93d397f519c7d7f73b6a441
http://arxiv.org/abs/1609.02200 - Discrete Variational Autoencoders
	-- https://youtube.com/watch?v=c6GukeAkyVs (Struminsky)
http://arxiv.org/abs/1611.00712 - The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables
	-- http://youtube.com/watch?v=JFgXEbgcT7g (Jang)
	-- https://laurent-dinh.github.io/2016/11/22/gumbel-max.html
	-- https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html
	-- https://gist.github.com/gngdb/ef1999ce3a8e0c5cc2ed35f488e19748
http://arxiv.org/abs/1611.01144 - Categorical Reparametrization with Gumbel-Softmax
	-- http://youtube.com/watch?v=JFgXEbgcT7g (Jang)
	-- http://blog.evjang.com/2016/11/tutorial-categorical-variational.html
	-- https://laurent-dinh.github.io/2016/11/22/gumbel-max.html
	-- https://casmls.github.io/general/2017/02/01/GumbelSoftmax.html
	-- https://github.com/EderSantana/gumbel
http://openreview.net/forum?id=ryMxXPFex - Discrete Variational Autoencoders (D-Wave)

http://arxiv.org/abs/1612.00377 - Multi-modal Variational Encoder-Decoders (Courville)
http://arxiv.org/abs/1607.05690 - Stochastic Backpropagation through Mixture Density Distributions
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1607.05690
http://arxiv.org/abs/1611.06585 - Variational Boosting: Iteratively Refining Posterior Approximations (Adams)
	-- http://andymiller.github.io/2016/11/23/vb.html
	-- https://youtu.be/Jh3D8Gi4N0I?t=1h9m52s (Nekludov, in russian)
http://arxiv.org/abs/1606.04934 - Improving Variational Inference with Inverse Autoregressive Flow
	"Most VAEs have so far been trained using crude approximate posteriors, where every latent variable is independent. Normalizing Flows have addressed this problem by conditioning each latent variable on the others before it in a chain, but this is computationally inefficient due to the introduced sequential dependencies. The core contribution of this work, termed inverse autoregressive flow (IAF), is a new approach that, unlike previous work, allows us to parallelize the computation of rich approximate posteriors, and make them almost arbitrarily flexible."
	-- https://github.com/openai/iaf
http://arxiv.org/abs/1611.02304 - Normalizing Flows on Riemannian Manifolds (Mohamed)
	-- https://pbs.twimg.com/media/CyntnWDXAAA97Ks.jpg

http://arxiv.org/abs/1602.05473 - Auxiliary Deep Generative Models
	-- http://techtalks.tv/talks/auxiliary-deep-generative-models/62509/
	-- https://github.com/larsmaaloee/auxiliary-deep-generative-models
http://arxiv.org/abs/1603.06277 - Composing graphical models with neural networks for structured representations and fast inference
	-- https://youtube.com/watch?v=btr1poCYIzw
	-- http://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/svae-slides.pdf
http://arxiv.org/pdf/1702.02390 - A Hybrid Convolutional Variational Autoencoder for Text Generation
	-- https://github.com/stas-semeniuta/textvae
http://arxiv.org/abs/1702.08139 - Improved Variational Autoencoders for Text Modeling using Dilated Convolutions (Salakhutdinov)

http://arxiv.org/abs/1610.05683 - Rejection Sampling Variational Inference
http://arxiv.org/abs/1610.02287 - The Generalized Reparameterization Gradient
	-- https://youtu.be/mrj_hyH974o?t=1h23m40s (Vetrov, in russian)
http://arxiv.org/abs/1511.00830 - The Variational Fair Autoencoder
	-- http://videolectures.net/iclr2016_louizos_fair_autoencoder/ (Louizos)
http://arxiv.org/abs/1511.06499 - The Variational Gaussian Process
	-- http://videolectures.net/iclr2016_tran_variational_gaussian/ (Tran)
	-- http://github.com/blei-lab/edward
http://arxiv.org/abs/1605.06197 - Stick-Breaking Variational Autoencoders  # latent representation with stochastic dimensionality




[generative models - autoregressive models]

http://arxiv.org/abs/1601.06759 - Pixel Recurrent Neural Networks
	-- http://techtalks.tv/talks/pixel-recurrent-neural-networks/62375/ (van den Oord)
	-- https://evernote.com/shard/s189/sh/fdf61a28-f4b6-491b-bef1-f3e148185b18/aba21367d1b3730d9334ed91d3250848 (Larochelle)
	-- https://github.com/tensorflow/magenta/blob/master/magenta/reviews/pixelrnn.md (Kastner)
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2FOordKK16#shagunsodhani
	-- https://github.com/zhirongw/pixel-rnn
	-- https://github.com/igul222/pixel_rnn
	-- https://github.com/carpedm20/pixel-rnn-tensorflow
	-- https://github.com/shiretzet/PixelRNN
http://arxiv.org/abs/1606.05328 - Conditional Image Generation with PixelCNN Decoders
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-1 (27:26, van den Oord)
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1606.05328#shagunsodhani
	-- https://github.com/openai/pixel-cnn
	-- https://github.com/kundan2510/pixelCNN
	-- https://github.com/anantzoid/Conditional-PixelCNN-decoder
https://openreview.net/forum?id=BJrFC6ceg - PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications (OpenAI)
	-- https://github.com/openai/pixel-cnn

ttp://arxiv.org/abs/1609.03499 - WaveNet: A Generative Model for Raw Audio (DeepMind)
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-1 (42:36, van den Oord)
	-- https://github.com/ibab/tensorflow-wavenet
	-- https://github.com/basveeling/wavenet/
	-- https://github.com/usernaamee/keras-wavenet
	-- https://github.com/tomlepaine/fast-wavenet
http://arxiv.org/abs/1610.10099 - Neural Machine Translation in Linear Time (ByteNet) (DeepMind)
	"Generalizes LSTM seq2seq by preserving the resolution. Dynamic unfolding instead of attention. Linear time computation."
http://arxiv.org/abs/1612.08083 - Language Modeling with Gated Convolutional Networks (outperforming LSTM on language modelling) (Facebook)
	-- https://github.com/DingKe/nn_playground/tree/master/gcnn
http://arxiv.org/abs/1603.06042 - Globally Normalized Transition-Based Neural Networks (SyntaxNet, “Parsey McParseface")
	"The parser uses a feed forward NN, which is much faster than the RNN usually used for parsing. Also the paper is using a global method to solve the label bias problem. This method can be used for many tasks and indeed in the paper it is used also to shorten sentences by throwing unnecessary words. The label bias problem arises when predicting each label in a sequence using a softmax over all possible label values in each step. This is a local approach but what we are really interested in is a global approach in which the sequence of all labels that appeared in a training example are normalized by all possible sequences. This is intractable so instead a beam search is performed to generate alternative sequences to the training sequence. The search is stopped when the training sequence drops from the beam or ends. The different beams with the training sequence are then used to compute the global loss."
	-- https://github.com/udibr/notes/raw/master/Talk%20by%20Sasha%20Rush%20-%20Interpreting%2C%20Training%2C%20and%20Distilling%20Seq2Seq%E2%80%A6.pdf
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1603.06042
	-- https://github.com/tensorflow/models/tree/master/syntaxnet

http://arxiv.org/abs/1607.07086 - An Actor-Critic Algorithm for Sequence Prediction (Bengio)
http://arxiv.org/abs/1606.02960 - Sequence-to-Sequence Learning as Beam-Search Optimization
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-2 (Wiseman, 0:44:02)
	-- http://shortscience.org/paper?bibtexKey=journals/corr/1606.02960
http://arxiv.org/abs/1701.06549 - Learning to Decode for Future Success (Stanford)

http://arxiv.org/abs/1606.03402 - Length Bias in Encoder Decoder Models and a Case for Global Conditioning (Google)
http://arxiv.org/abs/1611.02796 - Tuning Recurrent Neural Networks with Reinforcement Learning (Magenta)
	-- https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning/
	-- https://github.com/tensorflow/magenta/tree/master/magenta/models/rl_tuner
	"In contrast to relying solely on possibly biased data, our approach allows for encoding high-level domain knowledge into the RNN, providing a general, alternative tool for training sequence models."

http://arxiv.org/abs/1610.09038 - Professor Forcing: A New Algorithm for Training Recurrent Networks
	"In professor forcing, G is simply an RNN that is trained to predict the next element in a sequence and D a discriminative bi-directional RNN. G is trained to fool D into thinking that the hidden states of G occupy the same state space at training and inference time. D, in turn, is trained to tell apart the hidden states of G at training and inference time. At the Nash equilibrium, D cannot tell apart the state spaces any better and G cannot make them any more similar. This is motivated by the problem that RNNs typically diverge to regions of the state space that were never observed during training and which are hence difficult to generalize to."
	"Proposes providing the discriminator with the intermediate hidden vectors of the generator rather than its sequence outputs. Such a strategy makes the system differentiable and achieves promising results in tasks like character-level language modeling and handwriting generation."
	-- https://youtube.com/watch?v=I7UFPBDLDIk
	-- http://videolectures.net/deeplearning2016_goyal_new_algorithm/

http://arxiv.org/abs/1511.06391 - Order Matters: Sequence to Sequence for Sets (Vinyals)
	-- https://youtube.com/watch?v=uohtFXD_39c&t=56m51s (Bengio)




[probabilistic inference]

http://arxiv.org/abs/1701.03757 - Deep Probabilistic Programming (Edward)
	-- http://edwardlib.org/zoo

http://arxiv.org/abs/1612.01474 - Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles (DeepMind, non-bayesian)
	"So by training 5 times as many NNs/parameters, their ensemble can sometimes outperform just using Gal's dropout trick on a single model? What happens if you train with dropout a single model 5 times as large..."
http://arxiv.org/abs/1606.02556 - DISCO Nets: DISsimilarity COefficient Networks
	-- https://youtube.com/watch?v=OogNSKRkoes 

http://arxiv.org/abs/1605.07571 - Sequential Neural Models with Stochastic Layers
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Sequential-Neural-Models-with-Stochastic-Layers (Fraccaro)
	-- https://youtu.be/mrj_hyH974o?t=32m49s (in russian)
	-- https://github.com/marcofraccaro/srnn

http://arxiv.org/abs/1610.05735 - Deep Amortized Inference for Probabilistic Programs (Goodman)
http://arxiv.org/abs/1610.09900 - Inference Compilation and Universal Probabilistic Programming (Wood)

http://arxiv.org/abs/1606.07046 - Semantic Parsing to Probabilistic Programs for Situated Question Answering (AI2)




[reasoning]

http://arxiv.org/abs/1604.06361 - Row-less Universal Schema (McCallum)
http://arxiv.org/abs/1606.05804 - Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema (McCallum)
http://arxiv.org/abs/1607.01426 - Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks (McCallum)
	-- http://videolectures.net/deeplearning2016_das_neural_networks/ (Das)

http://arxiv.org/abs/1603.01547 - Text Understanding with the Attention Sum Reader Network
http://arxiv.org/abs/1606.03126 - Key-Value Memory Networks for Directly Reading Documents (Weston)
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1606.03126
	-- https://gist.github.com/shagunsodhani/a5e0baa075b4a917c0a69edc575772a8
	-- https://github.com/facebook/MemNN/blob/master/KVmemnn
https://openreview.net/forum?id=rJTKKKqeg - Tracking the World State with Recurrent Entity Networks (Facebook)
	-- https://github.com/jimfleming/recurrent-entity-networks
http://arxiv.org/abs/1606.01549 - Gated-Attention Readers for Text Comprehension (Salakhutdinov)
http://arxiv.org/abs/1606.04582 - Query-Regression Networks for Machine Comprehension (AI2)
	"We show the state-of-the-art results in the three datasets of story-based QA and dialog. We model a story or a dialog as a sequence of state-changing triggers and compute the final answer to the question or the system’s next utterance by recurrently updating (or reducing) the query. QRN is situated between the attention mechanism and RNN, effectively handling time dependency and long-term dependency problems of each technique, respectively. It addresses the long-term dependency problem of most RNNs by simplifying the recurrent update, in which the candidate hidden state (reduced query) does not depend on the previous state. Moreover, QRN can be parallelized and can address the well-known problem of RNN’s vanishing gradients."
	-- https://github.com/seominjoon/qrn

http://arxiv.org/abs/1603.01417 - Dynamic Memory Networks for Visual and Textual Question Answering (Socher)
	-- https://youtube.com/watch?v=FCtpHt6JEI8
	-- https://youtube.com/watch?v=DjPQRLMMAbw
	-- http://techtalks.tv/talks/dynamic-memory-networks-for-visual-and-textual-question-answering/62463/
	-- https://github.com/therne/dmn-tensorflow
http://arxiv.org/abs/1612.04844 - The More You Know: Using Knowledge Graphs for Image Classification (Salakhutdinov) # evolution of Gated Graph Sequence Neural Networks
http://arxiv.org/abs/1511.02799 - Deep Compositional Question Answering with Neural Module Networks (Darrell)
	-- https://youtube.com/watch?v=gDXD3hYfBW8 (Andreas)
	-- http://blog.jacobandreas.net/programming-with-nns.html
	-- https://github.com/abhshkdz/papers/blob/master/reviews/deep-compositional-question-answering-with-neural-module-networks.md
	-- http://github.com/jacobandreas/nmn2
http://arxiv.org/abs/1601.01705 - Learning to Compose Neural Networks for Question Answering (Darrell)
	-- https://youtube.com/watch?v=gDXD3hYfBW8 (Andreas)
	-- http://blog.jacobandreas.net/programming-with-nns.html
	-- http://github.com/jacobandreas/nmn2
http://arxiv.org/abs/1611.09978 - Modeling Relationships in Referential Expressions with Compositional Modular Networks (Darrell)

http://arxiv.org/abs/1512.00965 - Neural Enquirer: Learning to Query Tables with Natural Language
	"Authors propose a fully distributed neural enquirer, comprising several neuralized execution layers of field attention, row annotation, etc. While the model is not efficient in execution because of intensive matrix/vector operation during neural information processing and lacks explicit interpretation of execution, it can be trained in an end-to-end fashion because all components in the neural enquirer are differentiable."
http://arxiv.org/abs/1611.08945 - Learning a Natural Language Interface with Neural Programmer
	-- https://github.com/tensorflow/models/tree/master/neural_programmer
http://arxiv.org/abs/1611.00020 - Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision
	"We propose the Manager-Programmer-Computer framework, which integrates neural networks with non-differentiable memory to support abstract, scalable and precise operations through a friendly neural computer interface. Specifically, we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence neural "programmer", and a non-differentiable "computer" that is a Lisp interpreter with code assist."
http://akbc.ws/2016/papers/14_Paper.pdf - Learning Knowledge Base Inference with Neural Theorem Provers (Rocktäschel)
http://arxiv.org/abs/1605.06523 - TensorLog: A Differentiable Deductive Database (Cohen)
	-- https://github.com/TeamCohen/TensorLog

http://arxiv.org/abs/1602.02261 - WebNav: A New Large-Scale Task for Natural Language based Sequential Decision Making

http://arxiv.org/abs/1603.01312 - Learning Physical Intuition of Block Towers by Example (Fergus)
	-- https://youtu.be/oSAG57plHnI?t=19m48s (Tenenbaum)
http://arxiv.org/abs/1611.01843 - Learning to Perform Physics Experiments via Deep Reinforcement Learning (DeepMind)
	-- https://youtu.be/tPWGGwmgwG0?t=16m34s (de Freitas)
http://arxiv.org/abs/1612.00222 - Interaction Networks for Learning about Objects, Relations and Physics (DeepMind)
	-- https://blog.acolyer.org/2017/01/02/interaction-networks-for-learning-about-objects-relations-and-physics/




[program induction]

http://arxiv.org/abs/1511.06392 - Neural Random-Access Machines (Sutskever)
http://arxiv.org/abs/1602.03218 - Learning Efficient Algorithms with Hierarchical Attentive Memory (DeepMind) # "Our model may be seen as a special case of Gated Graph Neural Network" (REINFORCE)

http://openreview.net/forum?id=BkbY4psgg - Making Neural Programming Architectures Generalize via Recursion (Neural Programmer-Interpreter with recursion)

http://arxiv.org/abs/1605.07969 - Adaptive Neural Compilation
http://arxiv.org/abs/1605.06640 - Programming with a Differentiable Forth Interpreter (Riedel, Rocktäschel) (learning details of probabilistic program)
http://arxiv.org/abs/1608.04428 - TerpreT: A Probabilistic Programming Language for Program Induction
	"These works raise questions of (a) whether new models can be designed specifically to synthesize interpretable source code that may contain looping and branching structures, and (b) whether searching over program space using techniques developed for training deep neural networks is a useful alternative to the combinatorial search methods used in traditional IPS. In this work, we make several contributions in both of these directions."
	"Shows that differentiable interpreter-based program induction is inferior to discrete search-based techniques used by the programming languages community. We are then left with the question of how to make progress on program induction using machine learning techniques."




[reinforcement learning - algorithms]

http://arxiv.org/abs/1604.06778 - Benchmarking Deep Reinforcement Learning for Continuous Control (Abbeel)
	-- http://techtalks.tv/talks/benchmarking-deep-reinforcement-learning-for-continuous-control/62380/ (Duan)

http://arxiv.org/abs/1611.02779 - RL2: Fast Reinforcement Learning via Slow Reinforcement Learning (OpenAI, Abbeel)
http://arxiv.org/abs/1611.05763 - Learning to Reinforcement Learn (DeepMind)

http://arxiv.org/abs/1611.01224 - Sample Efficient Actor-Critic with Experience Replay (DeepMind) (ACER)

http://arxiv.org/abs/1702.08892 - Bridging the Gap Between Value and Policy Reinforcement Learning (Google Brain)
http://arxiv.org/abs/1611.01626 - PGQ: Combining policy gradient and Q-learning (DeepMind)
	"This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates."
	"We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms."
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2FODonoghueMKM16
	-- https://github.com/Fritz449/Asynchronous-RL-agent
http://arxiv.org/abs/1611.02247 - Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic (Levine)
	"We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods."
	"- unbiased gradient
	 - combine PG and AC gradients
	 - learns critic from off-policy data
	 - learns policy from on-policy data"
	"Q-Prop works with smaller batch size than TRPO-GAE
	Q-Prop is significantly more sample-efficient than TRPO-GAE"
	-- https://youtu.be/M6nfipCxQBc?t=16m11s (Lillicrap)

http://arxiv.org/abs/1602.04951 - Q(λ) with Off-Policy Corrections (DeepMind)
	-- https://youtube.com/watch?v=8hK0NnG_DhY&t=25m27s (Brunskill)
http://arxiv.org/abs/1606.02647 - Safe and Efficient Off-Policy Reinforcement Learning (DeepMind) (Retrace)
	"Retrace(λ) is a new strategy to weight a sample for off-policy learning, it provides low-variance, safe and efficient updates."
	"Our goal is to design a RL algorithm with two desired properties. Firstly, to use off-policy data, which is important for exploration, when we use memory replay, or observe log-data. Secondly, to use multi-steps returns in order to propagate rewards faster and avoid accumulation of approximation/estimation errors. Both properties are crucial in deep RL. We introduce the “Retrace” algorithm, which uses multi-steps returns and can safely and efficiently utilize any off-policy data."
	"open issue: off policy unbiased, low variance estimators for long horizon delayed reward problems"
	-- https://youtube.com/watch?v=8hK0NnG_DhY&t=25m27s (Brunskill)
http://arxiv.org/abs/1604.00923 - Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning (Brunskill)
	-- https://youtube.com/watch?v=8hK0NnG_DhY&t=15m44s (Brunskill)

http://arxiv.org/abs/1502.05477 - Trust Region Policy Optimization (Schulman, Levine, Jordan, Abbeel)
	-- https://youtube.com/watch?v=jeid0wIrSn4
http://arxiv.org/abs/1506.02438 - High-Dimensional Continuous Control Using Generalized Advantage Estimation (Schulman)
	-- https://youtube.com/watch?v=ATvp0Hp7RUI

http://arxiv.org/abs/1506.05254 - Gradient Estimation Using Stochastic Computation Graphs (Schulman)
	-- http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/ (Abbeel, 01:02:04)
	"Can mix and match likelihood ratio and path derivative. If black-box node: might need to place stochastic node in front of it and use likelihood ratio. This includes recurrent neural net policies."

http://openreview.net/forum?id=rJ8Je4clg - Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening (10x faster Q-learning)
	-- https://youtu.be/mrj_hyH974o?t=16m13s (in russian)

http://arxiv.org/abs/1611.05397 - Reinforcement Learning with Unsupervised Auxiliary Tasks (UNREAL, evolution of A3C)
	"This approach exploits the multithreading capabilities of standard CPUs. The idea is to execute many instances of our agent in parallel, but using a shared model. This provides a viable alternative to experience replay, since parallelisation also diversifies and decorrelates the data. Our asynchronous actor-critic algorithm, A3C, combines a deep Q-network with a deep policy network for selecting actions. It achieves state-of-the-art results, using a fraction of the training time of DQN and a fraction of the resource consumption of Gorila."
	-- https://youtube.com/watch?v=Uz-zGYrYEjA (demo)
	-- https://github.com/dennybritz/deeplearning-papernotes/blob/b097e313dc59c956575fb1bf23b64fa8d1d84057/notes/rl-auxiliary-tasks.md
http://arxiv.org/abs/1602.01783 - Asynchronous Methods for Deep Reinforcement Learning
	-- http://youtube.com/watch?v=0xo1Ldx3L5Q (TORCS demo)
	-- http://youtube.com/watch?v=nMR5mjCFZCw (3D Labyrinth demo)
	-- http://youtube.com/watch?v=Ajjc08-iPx8 (MuJoCo demo)
	-- http://youtube.com/watch?v=9sx1_u2qVhQ (Mnih)
	-- http://techtalks.tv/talks/asynchronous-methods-for-deep-reinforcement-learning/62475/ (Mnih)
	-- https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2
	-- https://github.com/Zeta36/Asynchronous-Methods-for-Deep-Reinforcement-Learning
	-- https://github.com/miyosuda/async_deep_reinforce
	-- https://github.com/muupan/async-rl
	-- https://github.com/yandexdataschool/AgentNet/blob/master/agentnet/learning/a2c_n_step.py
	-- https://github.com/coreylynch/async-rl
	-- https://github.com/carpedm20/deep-rl-tensorflow/blob/master/agents/async.py
	-- https://github.com/danijar/mindpark/blob/master/mindpark/algorithm/a3c.py

http://arxiv.org/abs/1612.07307 - Loss is Its Own Reward: Self-Supervision for Reinforcement Learning (Darrell)

http://arxiv.org/abs/1603.00748 - Continuous Deep Q-Learning with Model-based Acceleration (Sutskever)
	-- http://techtalks.tv/talks/continuous-deep-q-learning-with-model-based-acceleration/62474/
	-- https://youtu.be/M6nfipCxQBc?t=10m48s (Lillicrap)
	-- https://youtu.be/mrgJ53TIcQc?t=57m (Seleznev, in russian)
	-- http://www.bicv.org/?wpdmdl=1940
	-- https://github.com/carpedm20/NAF-tensorflow
	-- https://github.com/tambetm/gymexperiments
http://arxiv.org/abs/1511.04143 - Deep Reinforcement Learning In Parameterized Action Space
	-- https://github.com/mhauskn/dqn-hfo

http://arxiv.org/abs/1512.07679 - Reinforcement Learning in Large Discrete Action Spaces




[reinforcement learning - exploration]

https://openreview.net/forum?id=Bk8aOm9xl - Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning
	"Authors present two tractable approximations to their framework - one which ignores the stochasticity of the true environmental dynamics, and one which approximates the rate of information gain (somewhat similar to Schmidhuber's formal theory of creativity, fun and intrinsic motivation)."
	"Stadie et al. learn deterministic dynamics models by minimizing Euclidean loss - whereas in our work, we learn stochastic dynamics with cross entropy loss - and use L2 prediction errors for intrinsic motivation."
http://arxiv.org/abs/1611.04717 - #Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning (Abbeel)
	"The authors encourage exploration by adding a pseudo-reward of the form beta/sqrt(count(state)) for infrequently visited states. State visits are counted using Locality Sensitive Hashing (LSH) based on an environment-specific feature representation like raw pixels or autoencoder representations. The authors show that this simple technique achieves gains in various classic RL control tasks and several games in the ATARI domain. While the algorithm itself is simple there are now several more hyperaprameters to tune: The bonus coefficient beta, the LSH hashing granularity (how many bits to use for hashing) as well as the type of feature representation based on which the hash is computed, which itself may have more parameters. The experiments don't paint a consistent picture and different environments seem to need vastly different hyperparameter settings, which in my opinion will make this technique difficult to use in practice."
http://arxiv.org/abs/1611.07507 - Variational Intrinsic Control (DeepMind)
	"The second scenario is that in which the long-term goal of the agent is to get to a state with a maximal set of available intrinsic options – the objective of empowerment (Salge et al., 2014). This set of options consists of those that the agent knows how to use. Note that this is not the theoretical set of all options: it is of no use to the agent that it is possible to do something if it is unable to learn how to do it. Thus, to maximize empowerment, the agent needs to simultaneously learn how to control the environment as well – it needs to discover the options available to it. The agent should in fact not aim for states where it has the most control according to its current abilities, but for states where it expects it will achieve the most control after learning. Being able to learn available options is thus fundamental to becoming empowered."
	"Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016). The empowerment objective differs from this in a fundamental manner: the primary goal is not to understand or predict the observations but to control the environment. This is an important point – agents can often control an environment perfectly well without much understanding, as exemplified by canonical model-free reinforcement learning algorithms (Sutton & Barto, 1998), where agents only model action-conditioned expected returns. Focusing on such understanding might significantly distract and impair the agent, as such reducing the control it achieves."
http://arxiv.org/abs/1612.02605 - Towards Information-Seeking Agents (Maluuba)
	-- https://youtube.com/watch?v=3bSquT1zqj8 (demo)
http://arxiv.org/abs/1609.04994 - Exploration Potential
	"We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account."




[reinforcement learning - abstractions for states and actions]

http://openreview.net/forum?id=B1oK8aoxe - Stochastic Neural Networks for Hierarchical Reinforcement Learning (Abbeel)
	-- https://youtube.com/playlist?list=PLEbdzN4PXRGVB8NsPffxsBSOCcWFBMQx3 (demo)
	"Our SNN hierarchical approach outperforms state-of-the-art intrinsic motivation results like VIME (Houthooft et al., 2016)."
http://arxiv.org/abs/1610.05182 - Learning and Transfer of Modulated Locomotor Controllers (Silver)
	-- https://youtube.com/watch?v=sboPYvhpraQ (demo)

http://arxiv.org/abs/1611.07507 - Variational Intrinsic Control (DeepMind)
http://openreview.net/forum?id=B1oK8aoxe - Stochastic Neural Networks for Hierarchical Reinforcement Learning (Abbeel)
http://arxiv.org/abs/1606.04695 - Strategic Attentive Writer for Learning Macro-Actions (Vinyals)
	"Learning temporally extended actions and temporal abstraction in general is a long standing problem in reinforcement learning. They facilitate learning by enabling structured exploration and economic computation. In this paper we present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in a reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to – i.e. followed without replanning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information."
	-- https://youtube.com/watch?v=niMOdSu3yio (demo)
	-- https://blog.acolyer.org/2017/01/06/strategic-attentive-writer-for-learning-macro-actions/
	-- http://blog.shakirm.com/2016/07/learning-in-brains-and-machines-3-synergistic-and-modular-action/ (Mohamed)
http://arxiv.org/abs/1611.01796 - Modular Multitask Reinforcement Learning with Policy Sketches (Levine)
	-- https://youtube.com/watch?v=NRIcDEB64x8 (Andreas)
http://arxiv.org/abs/1609.05140 - The Option-Critic Architecture (Precup)
	-- https://youtube.com/watch?v=8r_EoYnPjGk (Bacon)
http://openreview.net/forum?id=H1eLE8qlx - Options Discovery with Budgeted Reinforcement Learning

http://arxiv.org/abs/1604.07255 - A Deep Hierarchical Approach to Lifelong Learning in Minecraft
	-- https://youtube.com/watch?v=RwjfE4kc6j8 (demo)
http://arxiv.org/abs/1604.06057 - Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation (Tenenbaum)
	-- https://drive.google.com/folderview?id=0B3yyTdZ1crn4enpGVmp5SFpnZms&usp=drive_web (demo)
	-- https://youtube.com/watch?v=ybDNvnVY1n8 (Kulkarni)
	-- http://mrkulk.github.io/notes/deephrl
	-- https://reddit.com/r/MachineLearning/comments/4frm32/160406057_hierarchical_deep_reinforcement/
	-- https://github.com/EthanMacdonald/h-DQN




[reinforcement learning - planning]

https://openreview.net/forum?id=BkJsCIcgl - The Predictron: End-to-End Learning and Planning (Silver)
	-- https://youtube.com/watch?v=BeaLdaN2C3Q
	-- https://github.com/zhongwen/predictron
http://arxiv.org/abs/1602.02867 - Value Iteration Networks
	-- https://youtube.com/watch?v=tXBHfbHHlKc (Tamar) + http://technion.ac.il/~danielm/icml_slides/Talk7.pdf
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Value-Iteration-Networks (Tamar)
	-- https://github.com/karpathy/paper-notes/blob/master/vin.md
	-- https://blog.acolyer.org/2017/02/09/value-iteration-networks/
	-- https://github.com/avivt/VIN
	-- https://github.com/TheAbhiKumar/tensorflow-value-iteration-networks
https://arxiv.org/abs/1701.02392 - Reinforcement Learning via Recurrent Convolutional Neural Networks
	"solving Markov Decision Processes and Reinforcement Learning problems using Recurrent Convolutional Neural Networks"
	"1. Solving Value / Policy Iteration in a standard MDP using Feedforward passes of a Value Iteration RCNN.
	2. Representing the Bayes Filter state belief update as feedforward passes of a Belief Propagation RCNN.
	3. Learning the State Transition models in a POMDP setting, using backpropagation on the Belief Propagation RCNN.
	4. Learning Reward Functions in an Inverse Reinforcement Learning framework from demonstrations, using a QMDP RCNN."
	-- https://youtube.com/watch?v=gpwA3QNTPOQ (Shankar)
	-- https://github.com/tanmayshankar/RCNN_MDP
https://openreview.net/forum?id=Bk8BvDqex - Metacontrol for Adaptive Imagination-Based Optimization (DeepMind)




[reinforcement learning - transfer]

http://arxiv.org/abs/1612.00429 - Generalizing Skills with Semi-Supervised Reinforcement Learning (Abbeel, Levine)
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-3 (39:26, Levine)
http://arxiv.org/abs/1609.07088 - Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer (Darrell, Abbeel, Levine)
	-- https://youtube.com/watch?v=n4EgRwzJE1o
http://arxiv.org/abs/1606.04671 - Progressive Neural Networks (DeepMind)
	-- https://youtube.com/watch?v=aWAP_CWEtSI (Hadsell)
	-- http://techtalks.tv/talks/progressive-nets-for-sim-to-real-transfer-learning/63043/ (Hadsell)
	-- https://blog.acolyer.org/2016/10/11/progressive-neural-networks/
	-- https://github.com/synpon/prog_nn
http://arxiv.org/abs/1511.06342 - Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning (Salakhutdinov)
	-- https://github.com/eparisotto/ActorMimic
http://arxiv.org/abs/1511.06295 - Policy Distillation (DeepMind)
	"Our new paper uses distillation to consolidate lots of policies into a single deep network. This works remarkably well, and can be applied online, during Q-learning, so that policies are compressed, distilled, and refined whilst being learned. Atari policies are actually improved through distillation and generalize better (with higher scores and lower variance) during novel starting state evaluation."
http://arxiv.org/abs/1606.05312 - Successor Features for Transfer in Reinforcement Learning (Silver)
http://arxiv.org/abs/1509.06841 - One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation and Neural Network Priors (Levine)




[reinforcement learning - imitation]

http://arxiv.org/abs/1603.00448 - Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization (Abbeel)
	-- https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Learning-Symposium-Session-3 (22:48, Levine)
http://arxiv.org/abs/1606.03476 - Generative Adversarial Imitation Learning
	-- https://github.com/openai/imitation
	"Uses a GAN framework to discriminate between teacher and student experience and force the student to behave close to the teacher."
http://openreview.net/forum?id=B16dGcqlx - Third Person Imitation Learning (Abbeel, Sutskever)
	"The authors propose a new framework for learning a policy from third-person experience. This is different from standard imitation learning which assumes the same "viewpoint" for teacher and student. The authors build upon Generative Adversarial Imitation Learning, which uses a GAN framework to discriminate between teacher and student experience and force the student to behave close to the teacher. However, when using third-person experience from a different viewpoint the discriminator would simply learn to discriminate between viewpoints instead of behavior and the framework isn't easily applicable. The authors' solution is to add a second discriminator to maximize a domain confusion loss based on the same feature representation. The objective is to learn the same (viewpoint-independent) feature representation for both teacher and student experience while also learning to discriminate between teacher and student observations. In other words, the objective is to maximize domain confusion while minimizing class loss. In practice, this is another discriminator term in the GAN objective. The authors also found that they need to feed observations at time t+n (n=4 in experiments) to signal the direction of movement in the environment."
http://arxiv.org/abs/1612.06699 - Unsupervised Perceptual Rewards for Imitation Learning (Levine)
	"To our knowledge, these are the first results showing that complex robotic manipulation skills can be learned directly and without supervised labels from a video of a human performing the task."
http://arxiv.org/abs/1605.06450 - Query-Efficient Imitation Learning for End-to-End Autonomous Driving (Cho)
http://arxiv.org/abs/1612.02179 - Model-based Adversarial Imitation Learning




[reinforcement learning - memory]

http://arxiv.org/abs/1606.04460 - Model-Free Episodic Control (DeepMind)
	"This might be achieved by a dual system (hippocampus vs neocortex http://wixtedlab.ucsd.edu/publications/Psych%20218/McClellandMcNaughtonOReilly95.pdf ) where information are stored in alternated way such that new nonstationary experience is rapidly encoded in the hippocampus (most flexible region of the brain with the highest amount of plasticity and neurogenesis); long term memory in the cortex is updated in a separate phase where what is updated (both in terms of samples and targets) can be controlled and does not put the system at risk of instabilities."
	-- https://sites.google.com/site/episodiccontrol/ (demo)
	-- http://blog.shakirm.com/2016/07/learning-in-brains-and-machines-4-episodic-and-interactive-memory/
	-- https://github.com/ShibiHe/Model-Free-Episodic-Control
	-- https://github.com/sudeepraja/Model-Free-Episodic-Control
http://arxiv.org/abs/1610.06402 - A Growing Long-term Episodic and Semantic Memory
http://arxiv.org/abs/1512.04455 - Memory-based Control with Recurrent Neural Networks (Silver)
	-- https://youtube.com/watch?v=V4_vb1D5NNQ (demo)




[reinforcement learning - multi-agent]

http://arxiv.org/abs/1611.00179 - Dual Learning for Machine Translation
	"In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the languagemodel likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods)."
	"The basic idea of dual learning is generally applicable: as long as two tasks are in dual form, we can apply the dual-learning mechanism to simultaneously learn both tasks from unlabeled data using reinforcement learning algorithms. Actually, many AI tasks are naturally in dual form, for example, speech recognition versus text to speech, image caption versus image generation, question answering versus question generation (e.g., Jeopardy!), search (matching queries to documents) versus keyword extraction (extracting keywords/queries for documents), so on and so forth."
http://arxiv.org/abs/1511.08779 - Multiagent Cooperation and Competition with Deep Reinforcement Learning
	-- https://youtube.com/playlist?list=PLfLv_F3r0TwyaZPe50OOUx8tRf0HwdR_u
	-- https://github.com/NeuroCSUT/DeepMind-Atari-Deep-Q-Learner-2Player
http://arxiv.org/abs/1602.02672 - Learning to Communicate to Solve Riddles with Deep Distributed Recurrent Q-Networks (de Freitas)
http://arxiv.org/abs/1605.06676 - Learning to Communicate with Deep Multi-Agent Reinforcement Learning (de Freitas)
	-- https://youtube.com/watch?v=cfsYBY4nd1c
	-- https://youtube.com/watch?v=xL-GKD49FXs (Foerster)
	-- http://videolectures.net/deeplearning2016_foerster_learning_communicate/ (Foerster)
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/1605.07133
	-- https://github.com/iassael/learning-to-communicate
http://arxiv.org/abs/1605.07736 - Learning Multiagent Communication with Backpropagation (Fergus)
	-- https://github.com/facebookresearch/CommNet




[reinforcement learning - applications]

http://arxiv.org/abs/1701.07274 - Deep Reinforcement Learning: An Overview
http://arxiv.org/abs/1701.01724 - DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker (Bowling)
	"In the past, perfect information games (chess, checkers, go) have been easier algorithmically than imperfect information games like poker. Powerful techniques like Alpha-Beta search, heuristic functions, depth-limited lookahead & Monte Carlo Tree Search work with perfect information games. They allow an AI to ignore the past and focus its computation on the tiny, immediate subgame most relevant for choosing actions. Until now, these techniques didn't really work in imperfect info games like poker, due to uncertainty about what each player knows. There is no single state to search from, but a set of states for each player. Past actions reveal info about what cards they hold. In imperfect info games like poker, local search hasn’t performed well. Had to solve the whole game at once, not as small fragments. But poker games are huge, and except for smallest versions, can't be solved exactly. Dominant technique was to solve a simplified game. This was called Abstraction-Solving-Translation. Simplify a game, solve it, use the simplified strategy in the real game. That simplification introduces mistakes. In some games, this still worked well enough to beat pros. AST didn't work well in No-Limit poker. Humans exploited simplified betting, and in huge pots, fine details of cards matter. This is the DeepStack breakthrough: it reintroduces powerful local search techniques to the imperfect info setting. No abstraction! It only considers a small local subgame to pick actions, given the "public state" and summary info of earlier actions in the hand. Early in the game, Deep Learning supplies a heuristic function to avoid searching to the end of the game. On the turn & river, it solves from the current decision until the end of the game and re-solves after every opponent action. On the preflop & flop, it solves to the end of the round then consults a deep neural net for value estimate of playing the turn/river. This NN is trained from randomly-generated hands (no human data needed) and must return value of every hand for each player. Deep Stack doesn't abstract cards or have to translate opponents bets. It always gets these details exactly right. This means there are no easy exploits, & we developed a new exploitability measurement program, Local Best Response, to show this. Also lets DeepStack play with any stacks/blinds. Can play freezeouts, cash games, etc. Earlier programs were specific to 1 stack size!"
	"Compared to Libratus, on the turn/river both programs are pretty similar: both use the same "continual resolving" trick (and counterfactual regret minimization). Biggest difference is preflop/flop. DeepStack uses continual resolving there too, so it can't get tricked by bet size attacks. Libratus used the old precomputed-strategy method for preflop/flop. It had holes they had to patch overnight, as pros found them. DeepStack can play any stack sizes, so can do freezeouts, cash games, etc. Libratus can only do 200bb stacks. Last big difference is resources. Libratus runs on a huge supercomputer, Cepheus only needs a laptop with a good GPU."
	"DeepStack does not compute and store a complete strategy prior to play. DeepStack computes a strategy based on the current state of the game for only the remainder of the hand, not maintaining one for the full game, which leads to lower overall exploitability."
	"Despite using ideas from abstraction, DeepStack is fundamentally different from abstraction-based approaches, which compute and store a strategy prior to play. While DeepStack restricts the number of actions in its lookahead trees, it has no need for explicit abstraction as each re-solve starts from the actual public state, meaning DeepStack always perfectly understands the current situation."
	"DeepStack is the first theoretically sound application of heuristic search methods—which have been famously successful in games like checkers, chess, and Go - to imperfect information games."
	"At a conceptual level, DeepStack’s continual re-solving, “intuitive” local search and sparse lookahead trees describe heuristic search, which is responsible for many AI successes in perfect information games. Until DeepStack, no theoretically sound application of heuristic search was known in imperfect information games."
	"During re-solving, DeepStack doesn’t need to reason about the entire remainder of the game because it substitutes computation beyond a certain depth with a fast approximate estimate, DeepStack’s "intuition" - a gut feeling of the value of holding any possible private cards in any possible poker situation. Finally, DeepStack’s intuition, much like human intuition, needs to be trained. We train it with deep learning using examples generated from random poker situations."
	-- http://deepstack.ai
http://arxiv.org/abs/1603.07954 - Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning (Barzilay)
	-- https://github.com/karthikncode/DeepRL-InformationExtraction
http://arxiv.org/abs/1611.09940 - Neural Combinatorial Optimization with Reinforcement Learning (Google Brain)
http://arxiv.org/abs/1611.03673 - Learning to Navigate in Complex Environments (DeepMind)
	-- http://youtube.com/watch?v=5Rflbx8y7HY (Mirowski)
http://arxiv.org/abs/1606.03152 - Policy Networks with Two-Stage Training for Dialogue Systems (Maluuba)
	-- http://www.maluuba.com/blog/2016/11/23/deep-reinforcement-learning-in-dialogue-systems
http://arxiv.org/abs/1610.09903 - Learning Runtime Parameters in Computer Systems with Delayed Experience Injection (optimizing cloud infrastructure with reinforcement learning)
http://arxiv.org/abs/1604.05085 - Mastering 2048 with Delayed Temporal Coherence Learning, Multi-State Weight Promotion, Redundant Encoding and Carousel Shaping
http://arxiv.org/abs/1609.05518 - Towards Deep Symbolic Reinforcement Learning
	-- https://youtube.com/watch?v=HOAVhPy6nrc (Shanahan)
http://arxiv.org/abs/1612.00563 - Self-critical Sequence Training for Image Captioning (REINFORCE with reward normalization but without baseline estimation)




[dialog systems]

http://arxiv.org/abs/1603.08023 - How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
	-- http://www.shortscience.org/paper?bibtexKey=journals/corr/LiuLSNCP16#shagunsodhani
http://arxiv.org/abs/1605.05414 - On the Evaluation of Dialogue Systems with Next Utterance Classification
http://openreview.net/forum?id=HJ5PIaseg - Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses
	-- https://youtube.com/watch?v=vTgwWobuoFw (Pineau)

http://arxiv.org/abs/1508.03386 - Learning from Real Users: Rating Dialogue Success with Neural Networks for Reinforcement Learning in Spoken Dialogue Systems (Young)
http://arxiv.org/abs/1605.07669 - On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems (Young)
http://arxiv.org/abs/1606.02689 - Continuously Learning Neural Dialogue Management (Young)
http://arxiv.org/abs/1612.03929 - Online Sequence-to-Sequence Reinforcement Learning for Open-domain Conversational Agents

http://arxiv.org/abs/1611.06216 - Generative Deep Neural Networks for Dialogue: A Short Review (Pineau)
http://arxiv.org/abs/1606.07056 - Emulating Human Conversations using Convolutional Neural Network-based IR
http://arxiv.org/abs/1610.07149 - Two are Better than One: An Ensemble of Retrieval- and Generation-Based Dialog Systems

http://arxiv.org/abs/1606.01541 - Deep Reinforcement Learning for Dialogue Generation (Jurafsky)
http://arxiv.org/abs/1701.06547 - Adversarial Learning for Neural Dialogue Generation (Jurafsky)
	-- https://github.com/jiweil/Neural-Dialogue-Generation
http://arxiv.org/abs/1606.03152 - Policy Networks with Two-Stage Training for Dialogue Systems (Maluuba)
	-- http://www.maluuba.com/blog/2016/11/23/deep-reinforcement-learning-in-dialogue-systems
http://arxiv.org/abs/1609.00777 - End-to-End Reinforcement Learning of Dialogue Agents for Information Access (Deng)
http://arxiv.org/abs/1608.05081 - Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks & Replay Buffer Spiking (Deng)
http://arxiv.org/abs/1606.01269 - End-to-End LSTM-based Dialog Control Optimized with Supervised and Reinforcement Learning (Zweig)

http://arxiv.org/abs/1604.04562 - A Network-based End-to-End Trainable Task-oriented Dialogue System (Young)
	-- http://videolectures.net/deeplearning2016_wen_network_based/ (Wen)
http://arxiv.org/abs/1606.02447 - Learning Language Games through Interaction (Manning)
http://arxiv.org/abs/1605.07683 - Learning End-to-End Goal-Oriented Dialog (Weston)
http://arxiv.org/abs/1606.03777 - Neural Belief Tracker: Data-Driven Dialogue State Tracking (Young)
http://arxiv.org/abs/1701.04024 - A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue (Manning)
http://arxiv.org/abs/1606.00776 - Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation (Bengio)
http://arxiv.org/abs/1605.06069 - A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues (Bengio)
http://arxiv.org/abs/1606.01292 - An Attentional Neural Conversation Model with Improved Specificity (Zweig)
http://kdd.org/kdd2016/subtopic/view/towards-conversational-recommender-systems - Towards Conversational Recommender Systems (Hoffman)
	-- https://periscope.tv/WiMLworkshop/1vAGRXDbvbkxl (Christakopoulou)
	-- https://youtube.com/watch?v=nLUfAJqXFUI (Christakopoulou)

http://arxiv.org/abs/1605.01652 - LSTM-based Mixture-of-Experts for Knowledge-Aware Dialogues
http://arxiv.org/abs/1603.01232 - Multi-domain Neural Network Language Generation for Spoken Dialogue Systems
http://arxiv.org/abs/1604.02038 - Sentence Level Recurrent Topic Model: Letting Topics Speak for Themselves
http://arxiv.org/abs/1611.09900 - Context-aware Natural Language Generation with Recurrent Neural Networks
http://arxiv.org/abs/1702.06703 - Data Distillation for Controlling Specificity in Dialogue Generation (Jurafsky)

http://arxiv.org/abs/1603.06155 - A Persona-Based Neural Conversation Model
	-- https://github.com/jiweil/Neural-Dialogue-Generation
http://arxiv.org/abs/1606.00372 - Conversational Contextual Cues: The Case of Personalization and History for Response Ranking (Kurzweil)
http://arxiv.org/abs/1607.00070 - A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems (Maluuba)

http://research.microsoft.com/apps/pubs/default.aspx?id=256085 - Deep Contextual Language Understanding in Spoken Dialogue Systems
http://arxiv.org/abs/1603.07954 - Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning (Barzilay)




[natural language processing]

http://arxiv.org/abs/1602.02410 - Exploring the Limits of Language Modeling (Vinyals)  # perplexity from 50 to 30
	-- http://deliprao.com/archives/201
	-- https://github.com/tensorflow/models/tree/master/lm_1b
http://arxiv.org/abs/1612.04426 - Improving Neural Language Models with a Continuous Cache (Facebook)  # adaptive softmax
http://arxiv.org/abs/1609.07843 - Pointer Sentinel Mixture Models (MetaMind)

http://arxiv.org/abs/1511.08198 - Towards Universal Paraphrastic Sentence Embeddings  # outperforming LSTM
	-- http://videolectures.net/iclr2016_wieting_universal_paraphrastic/ (Wieting)

http://arxiv.org/abs/1607.01759 - Bag of Tricks for Efficient Text Classification (Mikolov)  # fastText
	"Train the word-vectors so that they directly predict the target classes, almost as if those target classes were the nearby-context words (that would normally be predicted in classic word2vec). If you squint just right, it's also a little bit like Paragraph Vectors (often aka 'Doc2Vec'), backwards. Instead of an alternate input vector, which combines with all words to help predict words, it's alternate output nodes, to which all words contribute to the prediction."
	"As the paper says, people have been using low-rank classifiers for 10+ years (e.g. [http://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf]). Jason Weston of FAIR did a lot of work to popularize these embedding models in more recent times (e.g. [http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf]). This kind of approach is also essentially the same as many feature-based matrix factorization models (here's a survey [https://arxiv.org/abs/1109.2271]).
Use of hierarchical softmax for large label spaces also isn't new (e.g. [https://arxiv.org/abs/1412.7479] or [https://people.cs.umass.edu/~arvind/akbc-hierarchical.pdf]).
Other relevant recent work on simple embedding-baselines for text classification: Deep Unordered Composition Rivals Syntactic Methods for Text Classification [https://www.cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf]"
	-- http://www.shortscience.org/paper?bibtexKey=journals%2Fcorr%2F1607.01759#shagunsodhani
	-- https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py

http://arxiv.org/abs/1511.06361 - Order-Embeddings of Images and Language
	-- http://videolectures.net/iclr2016_vendrov_order_embeddings/ (Vendrov)
	-- https://github.com/ivendrov/order-embedding
	-- https://github.com/ivendrov/order-embeddings-wordnet
	-- https://github.com/LeavesBreathe/tensorflow_with_latest_papers/blob/master/partial_ordering_embedding.py

http://arxiv.org/abs/1609.09315 - Semantic Parsing with Semi-Supervised Sequential Autoencoders (DeepMind)  # discrete VAE
http://arxiv.org/abs/1607.03542 - Open-Vocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge (Gardner)

http://arxiv.org/abs/1603.08148 - Pointing the Unknown Words (Bengio)
http://arxiv.org/abs/1608.07905 - Machine Comprehension Using Match-LSTM And Answer Pointer

http://arxiv.org/abs/1511.06038 - Neural Variational Inference for Text Processing (Blunsom)
	-- http://dustintran.com/blog/neural-variational-inference-for-text-processing/
	-- https://github.com/carpedm20/variational-text-tensorflow
	-- https://github.com/cheng6076/NVDM
http://arxiv.org/abs/1511.06349 - Generating Sentences from a Continuous Space
	-- https://github.com/cheng6076/Variational-LSTM-Autoencoder




<brylevkirill (at) gmail.com>
